# Gemini æ ¼å¼è½¬ OpenAI æ ¼å¼è½¬æ¢é€»è¾‘æ–‡æ¡£

## ç›®å½•

1. [æ¦‚è¿°](#æ¦‚è¿°)
2. [æ•´ä½“æ¶æ„](#æ•´ä½“æ¶æ„)
3. [è¯·æ±‚è½¬æ¢æµç¨‹](#è¯·æ±‚è½¬æ¢æµç¨‹)
4. [å“åº”è½¬æ¢æµç¨‹](#å“åº”è½¬æ¢æµç¨‹)
5. [æµå¼å“åº”è½¬æ¢](#æµå¼å“åº”è½¬æ¢)
6. [å­—æ®µæ˜ å°„è¯¦è§£](#å­—æ®µæ˜ å°„è¯¦è§£)
7. [ç‰¹æ®Šå¤„ç†é€»è¾‘](#ç‰¹æ®Šå¤„ç†é€»è¾‘)
8. [é”™è¯¯å¤„ç†](#é”™è¯¯å¤„ç†)
9. [é…ç½®é¡¹è¯´æ˜](#é…ç½®é¡¹è¯´æ˜)
10. [å®Œæ•´ç¤ºä¾‹](#å®Œæ•´ç¤ºä¾‹)
11. [å¤šè½®å·¥å…·è°ƒç”¨çŠ¶æ€ä¿ç•™](#å¤šè½®å·¥å…·è°ƒç”¨çŠ¶æ€ä¿ç•™)
    - [thoughtSignature å¤„ç†é€»è¾‘](#thoughtsignature-å¤„ç†é€»è¾‘)
    - [reasoning_details å¤„ç†é€»è¾‘](#reasoning_details-å¤„ç†é€»è¾‘)
12. [æ€»ç»“](#æ€»ç»“)

---

## æ¦‚è¿°

æœ¬é¡¹ç›®å®ç°äº† Google Gemini API æ ¼å¼ä¸ OpenAI Chat Completions API æ ¼å¼ä¹‹é—´çš„åŒå‘è½¬æ¢ã€‚å½“å®¢æˆ·ç«¯ä½¿ç”¨ Gemini æ ¼å¼å‘é€è¯·æ±‚ï¼Œè€Œç›®æ ‡æ¸ é“æ˜¯ OpenAI æ—¶ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨è¿›è¡Œæ ¼å¼è½¬æ¢ã€‚

### æ”¯æŒçš„è½¬æ¢æ–¹å‘

- **è¯·æ±‚è½¬æ¢**: Gemini â†’ OpenAIï¼ˆå®¢æˆ·ç«¯å‘é€ Gemini æ ¼å¼ï¼Œè½¬å‘åˆ° OpenAI APIï¼‰
- **å“åº”è½¬æ¢**: OpenAI â†’ Geminiï¼ˆæ¥æ”¶ OpenAI å“åº”ï¼Œè¿”å› Gemini æ ¼å¼ç»™å®¢æˆ·ç«¯ï¼‰

### æ ¸å¿ƒæ–‡ä»¶

| æ–‡ä»¶ | è¯´æ˜ |
|------|------|
| `src/formats/gemini_converter.py` | Gemini æ ¼å¼è½¬æ¢å™¨ï¼Œè´Ÿè´£ Gemini â†’ OpenAI è¯·æ±‚è½¬æ¢å’Œ OpenAI â†’ Gemini å“åº”è½¬æ¢ |
| `src/formats/openai_converter.py` | OpenAI æ ¼å¼è½¬æ¢å™¨ |
| `src/formats/converter_factory.py` | è½¬æ¢å™¨å·¥å‚ï¼Œç»Ÿä¸€ç®¡ç†è½¬æ¢å™¨å®ä¾‹ |
| `src/api/unified_api.py` | ç»Ÿä¸€ API å±‚ï¼Œå¤„ç†è¯·æ±‚è·¯ç”±å’Œè½¬æ¢è°ƒç”¨ |

---

## æ•´ä½“æ¶æ„

### è¯·æ±‚æµç¨‹å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   å®¢æˆ·ç«¯    â”‚â”€â”€â”€â”€â–¶â”‚  unified_api.py  â”‚â”€â”€â”€â”€â–¶â”‚ GeminiConverter â”‚â”€â”€â”€â”€â–¶â”‚  OpenAI API  â”‚
â”‚ (Geminiæ ¼å¼) â”‚     â”‚   è¯·æ±‚æ¥æ”¶       â”‚     â”‚  è¯·æ±‚è½¬æ¢       â”‚     â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                            â”‚
                                                                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   å®¢æˆ·ç«¯    â”‚â—€â”€â”€â”€â”€â”‚  unified_api.py  â”‚â—€â”€â”€â”€â”€â”‚ GeminiConverter â”‚â—€â”€â”€â”€â”€â”‚  OpenAIå“åº”  â”‚
â”‚ (Geminiæ ¼å¼) â”‚     â”‚   å“åº”è¿”å›       â”‚     â”‚  å“åº”è½¬æ¢       â”‚     â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Gemini API ç«¯ç‚¹

ç³»ç»Ÿæ”¯æŒä»¥ä¸‹ Gemini æ ¼å¼çš„ API ç«¯ç‚¹ï¼š

| ç«¯ç‚¹ | è¯´æ˜ |
|------|------|
| `POST /v1beta/models/{model_id}:generateContent` | éæµå¼ç”Ÿæˆ |
| `POST /v1beta/models/{model_id}:streamGenerateContent` | æµå¼ç”Ÿæˆ |

### è½¬æ¢å™¨å·¥å‚æ¨¡å¼

```python
# converter_factory.py
class ConverterFactory:
    @classmethod
    def get_converter(cls, format_name: str) -> Optional[BaseConverter]:
        """è·å–æŒ‡å®šæ ¼å¼çš„è½¬æ¢å™¨"""
        converters = {
            "openai": OpenAIConverter,
            "anthropic": AnthropicConverter,
            "gemini": GeminiConverter
        }
        return converters.get(format_name)()
```

---

## è¯·æ±‚è½¬æ¢æµç¨‹

### å…¥å£å‡½æ•°

è¯·æ±‚è½¬æ¢çš„å…¥å£ä½äº `src/formats/gemini_converter.py` çš„ `_convert_to_openai_request` æ–¹æ³•ï¼ˆç¬¬ 136-371 è¡Œï¼‰ã€‚

### è½¬æ¢æ­¥éª¤

#### 1. æ¨¡å‹å­—æ®µå¤„ç†

```python
# å¿…é¡»æœ‰åŸå§‹æ¨¡å‹åç§°ï¼Œå¦åˆ™æŠ¥é”™
if not self.original_model:
    raise ValueError("Original model name is required for request conversion")

result_data["model"] = self.original_model  # ä½¿ç”¨åŸå§‹æ¨¡å‹åç§°
```

#### 2. å‡½æ•°è°ƒç”¨ ID æ˜ å°„é¢„æ‰«æ

åœ¨è½¬æ¢æ¶ˆæ¯ä¹‹å‰ï¼Œå…ˆæ‰«ææ•´ä¸ªå¯¹è¯å†å²ï¼Œä¸º `functionCall` å’Œ `functionResponse` å»ºç«‹ä¸€è‡´çš„ ID æ˜ å°„ï¼š

```python
# åˆå§‹åŒ–å‡½æ•°è°ƒç”¨IDæ˜ å°„è¡¨ï¼Œç”¨äºä¿æŒå·¥å…·è°ƒç”¨å’Œå·¥å…·ç»“æœçš„IDä¸€è‡´æ€§
# å…ˆæ‰«ææ•´ä¸ªå¯¹è¯å†å²ï¼Œä¸ºæ¯ä¸ªfunctionCallå’ŒfunctionResponseå»ºç«‹æ˜ å°„å…³ç³»
self._function_call_mapping = self._build_function_call_mapping(data.get("contents", []))
```

**æ˜ å°„æ„å»ºé€»è¾‘**ï¼ˆç¬¬ 1210-1237 è¡Œï¼‰ï¼š

```python
def _build_function_call_mapping(self, contents: List[Dict[str, Any]]) -> Dict[str, str]:
    """æ‰«ææ•´ä¸ªå¯¹è¯å†å²ï¼Œä¸ºfunctionCallå’ŒfunctionResponseå»ºç«‹IDæ˜ å°„"""
    mapping = {}
    function_call_sequence = {}  # {func_name: sequence_number}

    for content in contents:
        parts = content.get("parts", [])
        for part in parts:
            if "functionCall" in part:
                func_name = part["functionCall"].get("name", "")
                if func_name:
                    # ä¸ºæ¯ä¸ªå‡½æ•°è°ƒç”¨ç”Ÿæˆå”¯ä¸€çš„sequence number
                    sequence = function_call_sequence.get(func_name, 0) + 1
                    function_call_sequence[func_name] = sequence

                    # ç”Ÿæˆä¸€è‡´çš„ID: call_{func_name}_{åºå·}
                    tool_call_id = f"call_{func_name}_{sequence:04d}"
                    mapping[f"{func_name}_{sequence}"] = tool_call_id

            elif "functionResponse" in part:
                func_name = part["functionResponse"].get("name", "")
                if func_name:
                    # ä¸ºfunctionResponseåˆ†é…æœ€è¿‘çš„functionCallçš„ID
                    current_sequence = function_call_sequence.get(func_name, 0)
                    if current_sequence > 0:
                        mapping[f"response_{func_name}_{current_sequence}"] = mapping.get(f"{func_name}_{current_sequence}")

    return mapping
```

#### 3. ç³»ç»Ÿæ¶ˆæ¯è½¬æ¢

Gemini çš„ `systemInstruction` æˆ– `system_instruction` å­—æ®µè½¬æ¢ä¸º OpenAI çš„ `system` è§’è‰²æ¶ˆæ¯ï¼š

```python
# Gemini æ ¼å¼ï¼ˆæ”¯æŒä¸¤ç§å†™æ³•ï¼‰
{
    "systemInstruction": {
        "parts": [{"text": "You are a helpful assistant."}]
    }
}
# æˆ–
{
    "system_instruction": {
        "parts": [{"text": "You are a helpful assistant."}]
    }
}

# è½¬æ¢ä¸º OpenAI æ ¼å¼
{
    "messages": [
        {"role": "system", "content": "You are a helpful assistant."}
    ]
}
```

**ä»£ç å®ç°**ï¼ˆç¬¬ 152-162 è¡Œï¼‰ï¼š

```python
# æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯ - æ”¯æŒä¸¤ç§æ ¼å¼
system_instruction_data = data.get("systemInstruction") or data.get("system_instruction")
if system_instruction_data:
    system_parts = system_instruction_data.get("parts", [])
    system_text = ""
    for part in system_parts:
        if "text" in part:
            system_text += part["text"]
    if system_text:
        messages.append(self._create_system_message(system_text))
```

#### 4. æ¶ˆæ¯å†…å®¹è½¬æ¢

Gemini çš„ `contents` æ•°ç»„è½¬æ¢ä¸º OpenAI çš„ `messages` æ•°ç»„ï¼š

| Gemini è§’è‰² | OpenAI è§’è‰² | è¯´æ˜ |
|-------------|-------------|------|
| `user` | `user` | ç”¨æˆ·æ¶ˆæ¯ |
| `model` | `assistant` | åŠ©æ‰‹æ¶ˆæ¯ |
| `user` (å« functionResponse) | `tool` | å·¥å…·å“åº” |
| `tool` | `tool` | å·¥å…·å“åº”ï¼ˆè§’è‰²ä¸º toolï¼‰ |

**ç”¨æˆ·æ¶ˆæ¯å¤„ç†**ï¼ˆç¬¬ 171-214 è¡Œï¼‰ï¼š

```python
if gemini_role == "user":
    # æ£€æŸ¥æ˜¯å¦åŒ…å« functionResponseï¼ˆå·¥å…·ç»“æœï¼‰
    has_function_response = any("functionResponse" in part for part in parts)
    if has_function_response:
        # è½¬æ¢ä¸º OpenAI çš„ tool æ¶ˆæ¯
        for part in parts:
            if "functionResponse" in part:
                fr = part["functionResponse"]
                func_name = fr.get("name", "")
                response_content = fr.get("response", {})

                # ä»å“åº”å†…å®¹ä¸­æå–æ–‡æœ¬
                if isinstance(response_content, dict):
                    tool_result = response_content.get("content", json.dumps(response_content, ensure_ascii=False))
                else:
                    tool_result = str(response_content)

                # ä½¿ç”¨é¢„å…ˆå»ºç«‹çš„æ˜ å°„è·å–å¯¹åº”çš„tool_call_id
                tool_call_id = self._function_call_mapping.get(f"response_{func_name}_{sequence}")

                messages.append({
                    "role": "tool",
                    "tool_call_id": tool_call_id,
                    "content": tool_result
                })
    else:
        # æ™®é€šç”¨æˆ·æ¶ˆæ¯
        message_content = self._convert_content_from_gemini(parts)
        messages.append({
            "role": "user",
            "content": message_content
        })
```

**åŠ©æ‰‹æ¶ˆæ¯å¤„ç†ï¼ˆå«å·¥å…·è°ƒç”¨ï¼‰**ï¼ˆç¬¬ 216-233 è¡Œï¼‰ï¼š

```python
elif gemini_role == "model":
    # åŠ©æ‰‹æ¶ˆæ¯ï¼Œå¯èƒ½åŒ…å«å·¥å…·è°ƒç”¨
    message_content = self._convert_content_from_gemini(parts)

    if isinstance(message_content, dict) and message_content.get("type") == "tool_calls":
        # æœ‰å·¥å…·è°ƒç”¨çš„åŠ©æ‰‹æ¶ˆæ¯
        message = {
            "role": "assistant",
            "content": message_content.get("content"),
            "tool_calls": message_content["tool_calls"]
        }
        messages.append(message)
    else:
        # æ™®é€šåŠ©æ‰‹æ¶ˆæ¯
        messages.append({
            "role": "assistant",
            "content": message_content
        })
```

#### 5. ç”Ÿæˆé…ç½®è½¬æ¢

Gemini çš„ `generationConfig` è½¬æ¢ä¸º OpenAI çš„é¡¶å±‚å‚æ•°ï¼š

| Gemini å‚æ•° | OpenAI å‚æ•° | è¯´æ˜ |
|-------------|-------------|------|
| `temperature` | `temperature` | æ¸©åº¦å‚æ•° |
| `topP` | `top_p` | Top-P é‡‡æ · |
| `maxOutputTokens` | `max_tokens` | æœ€å¤§è¾“å‡º token æ•° |
| `stopSequences` | `stop` | åœæ­¢åºåˆ— |
| `response_mime_type` | `response_format.type` | å“åº”æ ¼å¼ |
| `response_schema` | `response_format.json_schema` | JSON Schema |

**ä»£ç å®ç°**ï¼ˆç¬¬ 279-302 è¡Œï¼‰ï¼š

```python
if "generationConfig" in data:
    config = data["generationConfig"]
    if "temperature" in config:
        result_data["temperature"] = config["temperature"]
    if "topP" in config:
        result_data["top_p"] = config["topP"]
    if "maxOutputTokens" in config:
        result_data["max_tokens"] = config["maxOutputTokens"]
    if "stopSequences" in config:
        result_data["stop"] = config["stopSequences"]

    # å¤„ç†ç»“æ„åŒ–è¾“å‡º
    if config.get("response_mime_type") == "application/json":
        result_data["response_format"] = {"type": "json_object"}
        if "response_schema" in config:
            result_data["response_format"] = {
                "type": "json_schema",
                "json_schema": {
                    "name": "response",
                    "strict": True,
                    "schema": config["response_schema"]
                }
            }
```

#### 6. å·¥å…·è°ƒç”¨è½¬æ¢

Gemini çš„ `tools[].function_declarations` è½¬æ¢ä¸º OpenAI çš„ `tools[].function`ï¼š

**Gemini æ ¼å¼**ï¼ˆæ”¯æŒ snake_case å’Œ camelCaseï¼‰ï¼š
```json
{
    "tools": [{
        "function_declarations": [{
            "name": "get_weather",
            "description": "Get weather info",
            "parameters": {
                "type": "OBJECT",
                "properties": {
                    "location": {"type": "STRING"}
                }
            }
        }]
    }]
}
```

**OpenAI æ ¼å¼**ï¼š
```json
{
    "tools": [{
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "Get weather info",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {"type": "string"}
                }
            }
        }
    }],
    "tool_choice": "auto"
}
```

**ä»£ç å®ç°**ï¼ˆç¬¬ 304-326 è¡Œï¼‰ï¼š

```python
if "tools" in data:
    openai_tools = []
    for tool in data["tools"]:
        # Geminiå®˜æ–¹ä½¿ç”¨ snake_case: function_declarations
        func_key = None
        if "function_declarations" in tool:
            func_key = "function_declarations"
        elif "functionDeclarations" in tool:  # å…¼å®¹æ—§å†™æ³•
            func_key = "functionDeclarations"
        if func_key:
            for func_decl in tool[func_key]:
                openai_tools.append({
                    "type": "function",
                    "function": {
                        "name": func_decl.get("name", ""),
                        "description": func_decl.get("description", ""),
                        "parameters": self._sanitize_schema_for_openai(func_decl.get("parameters", {}))
                    }
                })
    if openai_tools:
        result_data["tools"] = openai_tools
        result_data["tool_choice"] = "auto"
```

**JSON Schema ç±»å‹è½¬æ¢**ï¼ˆç¬¬ 1166-1208 è¡Œï¼‰ï¼š

Gemini ä½¿ç”¨å¤§å†™ç±»å‹åï¼ŒOpenAI ä½¿ç”¨å°å†™ï¼š

```python
def _sanitize_schema_for_openai(self, schema: Dict[str, Any]) -> Dict[str, Any]:
    """å°†Geminiæ ¼å¼çš„JSON Schemaè½¬æ¢ä¸ºOpenAIå…¼å®¹çš„æ ¼å¼"""
    # Geminiåˆ°OpenAIçš„ç±»å‹æ˜ å°„
    type_mapping = {
        "STRING": "string",
        "NUMBER": "number",
        "INTEGER": "integer",
        "BOOLEAN": "boolean",
        "ARRAY": "array",
        "OBJECT": "object"
    }

    def convert_types(obj):
        if isinstance(obj, dict):
            # è½¬æ¢typeå­—æ®µ
            if "type" in obj and isinstance(obj["type"], str):
                obj["type"] = type_mapping.get(obj["type"].upper(), obj["type"].lower())

            # è½¬æ¢éœ€è¦æ•´æ•°å€¼çš„å­—æ®µï¼ˆå°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ•´æ•°ï¼‰
            integer_fields = ["minItems", "maxItems", "minimum", "maximum", "minLength", "maxLength"]
            for field in integer_fields:
                if field in obj and isinstance(obj[field], str) and obj[field].isdigit():
                    obj[field] = int(obj[field])

            # é€’å½’å¤„ç†æ‰€æœ‰å­—æ®µ
            for key, value in obj.items():
                obj[key] = convert_types(value)

        elif isinstance(obj, list):
            return [convert_types(item) for item in obj]

        return obj

    return convert_types(copy.deepcopy(schema))
```

#### 6.1 éå‡½æ•°å·¥å…·è½¬æ¢ï¼ˆè™šæ‹Ÿå·¥å…·æ˜ å°„ï¼‰

Gemini ç‰¹æœ‰çš„éå‡½æ•°å·¥å…·ä¼šè¢«è½¬æ¢ä¸ºè™šæ‹Ÿå‡½æ•°å·¥å…·ï¼Œç”±è°ƒç”¨æ–¹å®ç°ï¼š

| Gemini å·¥å…· | OpenAI æ˜ å°„ | è¯´æ˜ |
|------------|-------------|------|
| `code_execution` | `function: code_execution` | ä»£ç æ‰§è¡Œå·¥å…·ï¼Œå‚æ•°: `{code: string}` |
| `google_search` | `function: google_search` | Google æœç´¢å·¥å…·ï¼Œå‚æ•°: `{query: string}` |
| `google_search_retrieval` | è­¦å‘Šå¹¶å¿½ç•¥ | ä¸æ”¯æŒ |
| `retrieval` | è­¦å‘Šå¹¶å¿½ç•¥ | ä¸æ”¯æŒ |

**ä»£ç å®ç°ç¤ºä¾‹**ï¼š
```python
# code_execution â†’ è™šæ‹Ÿå‡½æ•°å·¥å…·
if has_code_execution:
    openai_tools.append({
        "type": "function",
        "function": {
            "name": "code_execution",
            "description": "Execute Python code. Caller must implement the execution handler.",
            "parameters": {"type": "object", "properties": {"code": {"type": "string"}}, "required": ["code"]}
        }
    })
```

#### 6.2 ä»£ç æ‰§è¡Œå“åº”è½¬æ¢

Gemini çš„ `executableCode` å’Œ `codeExecutionResult` å“åº”éƒ¨åˆ†è½¬æ¢ä¸ºæ–‡æœ¬ï¼š

| Gemini éƒ¨åˆ† | è½¬æ¢ç»“æœ |
|------------|---------|
| `executableCode` | Markdown fenced code block: `` ```python\n{code}\n``` `` |
| `codeExecutionResult` | æ–‡æœ¬: `[code_execution_result]\noutcome: {outcome}\noutput:\n{output}` |

#### 6.3 å¤šæ¨¡æ€å†…å®¹å¤„ç†

| Gemini å†…å®¹ç±»å‹ | OpenAI è½¬æ¢ | è¯´æ˜ |
|----------------|-------------|------|
| `inlineData` (image/*) | `image_url` | Base64 å›¾åƒ |
| `inlineData` (audio/*) | `input_audio` | éŸ³é¢‘æ•°æ® |
| `inlineData` (video/*) | è­¦å‘Šå¹¶è·³è¿‡ | OpenAI ä¸æ”¯æŒ |
| `fileData` | æ–‡æœ¬å ä½ç¬¦ | `[fileData: mimeType=..., uri=...]` |

#### 6.4 ç”Ÿæˆé…ç½®å‚æ•°æ‰©å±•

æ–°å¢æ”¯æŒçš„ `generationConfig` å‚æ•°ï¼š

| Gemini å‚æ•° | OpenAI æ˜ å°„ |
|------------|------------|
| `presencePenalty` | `presence_penalty` |
| `frequencyPenalty` | `frequency_penalty` |
| `candidateCount` | `n` |

#### 6.5 å®‰å…¨è®¾ç½®é€ä¼ 

Gemini çš„ `safetySettings` ä¼šè¢«ä¿å­˜åˆ°è¯·æ±‚çš„ `metadata.gemini_safety_settings` ä¸­ï¼Œä¾›ä¸‹æ¸¸ä¸­é—´ä»¶æˆ–æ—¥å¿—ä½¿ç”¨ã€‚

#### 7. æ€è€ƒæ¨¡å¼è½¬æ¢ï¼ˆé‡ç‚¹ï¼‰

Gemini çš„ `thinkingConfig.thinkingBudget` è½¬æ¢ä¸º OpenAI çš„ `reasoning_effort` + `max_completion_tokens`ï¼š

**è½¬æ¢é€»è¾‘**ï¼ˆç¬¬ 328-365 è¡Œï¼‰ï¼š

```python
if "generationConfig" in data and "thinkingConfig" in data["generationConfig"]:
    thinking_config = data["generationConfig"]["thinkingConfig"]
    thinking_budget = thinking_config.get("thinkingBudget")

    if thinking_budget is not None and thinking_budget != 0:
        # æ ¹æ®thinking_budgetåˆ¤æ–­reasoning_effortç­‰çº§
        reasoning_effort = self._determine_reasoning_effort_from_budget(thinking_budget)
        result_data["reasoning_effort"] = reasoning_effort

        # å¤„ç†max_completion_tokensçš„ä¼˜å…ˆçº§é€»è¾‘
        max_completion_tokens = None

        # ä¼˜å…ˆçº§1ï¼šå®¢æˆ·ç«¯ä¼ å…¥çš„maxOutputTokens
        if "generationConfig" in data and "maxOutputTokens" in data["generationConfig"]:
            max_completion_tokens = data["generationConfig"]["maxOutputTokens"]
            # ç§»é™¤max_tokensï¼Œä½¿ç”¨max_completion_tokens
            result_data.pop("max_tokens", None)
        else:
            # ä¼˜å…ˆçº§2ï¼šç¯å¢ƒå˜é‡OPENAI_REASONING_MAX_TOKENS
            env_max_tokens = os.environ.get("OPENAI_REASONING_MAX_TOKENS")
            if env_max_tokens:
                max_completion_tokens = int(env_max_tokens)
            else:
                # ä¼˜å…ˆçº§3ï¼šéƒ½æ²¡æœ‰åˆ™æŠ¥é”™
                raise ConversionError("For OpenAI reasoning models, max_completion_tokens is required.")

        result_data["max_completion_tokens"] = max_completion_tokens
```

**reasoning_effort ç­‰çº§åˆ¤æ–­**ï¼ˆç¬¬ 23-68 è¡Œï¼‰ï¼š

```python
def _determine_reasoning_effort_from_budget(self, thinking_budget: Optional[int]) -> str:
    """æ ¹æ®thinkingBudgetåˆ¤æ–­OpenAI reasoning_effortç­‰çº§"""
    # å¦‚æœæ²¡æœ‰æä¾›thinking_budgetæˆ–ä¸º-1ï¼ˆåŠ¨æ€æ€è€ƒï¼‰ï¼Œé»˜è®¤ä¸ºhigh
    if thinking_budget is None or thinking_budget == -1:
        return "high"

    # ä»ç¯å¢ƒå˜é‡è·å–é˜ˆå€¼é…ç½®ï¼ˆå¿…éœ€ï¼‰
    low_threshold = int(os.environ.get("GEMINI_TO_OPENAI_LOW_REASONING_THRESHOLD"))
    high_threshold = int(os.environ.get("GEMINI_TO_OPENAI_HIGH_REASONING_THRESHOLD"))

    if thinking_budget <= low_threshold:
        return "low"
    elif thinking_budget <= high_threshold:
        return "medium"
    else:
        return "high"
```

**ç¯å¢ƒå˜é‡é…ç½®ç¤ºä¾‹**ï¼š
```bash
GEMINI_TO_OPENAI_LOW_REASONING_THRESHOLD=4096
GEMINI_TO_OPENAI_HIGH_REASONING_THRESHOLD=16384
```

| thinkingBudget | reasoning_effort |
|----------------|------------------|
| `-1`ï¼ˆåŠ¨æ€ï¼‰ | `high` |
| `<= 4096` | `low` |
| `4097 - 16384` | `medium` |
| `> 16384` | `high` |

---

## å“åº”è½¬æ¢æµç¨‹

### å…¥å£å‡½æ•°

å“åº”è½¬æ¢çš„å…¥å£ä½äº `src/formats/gemini_converter.py` çš„ `_convert_from_openai_response` æ–¹æ³•ï¼ˆç¬¬ 504-568 è¡Œï¼‰ã€‚

### è½¬æ¢æ­¥éª¤

#### 1. åŸºç¡€ç»“æ„æ„å»º

```python
result_data = {
    "candidates": [],
    "usageMetadata": {}
}
```

#### 2. å†…å®¹æå–ä¸è½¬æ¢

ä» OpenAI çš„ `choices[0].message` ä¸­æå–å†…å®¹ï¼Œè½¬æ¢ä¸º Gemini çš„ `candidates[0].content.parts`ï¼š

```python
if "choices" in data and data["choices"] and data["choices"][0]:
    choice = data["choices"][0]
    message = choice.get("message", {})
    content = message.get("content", "")
    tool_calls = message.get("tool_calls", [])

    # æ„å»º parts
    parts = []

    # æ·»åŠ æ–‡æœ¬å†…å®¹ï¼ˆå¦‚æœæœ‰ï¼‰
    if content:
        parts.append({"text": content})

    # æ·»åŠ å·¥å…·è°ƒç”¨ï¼ˆå¦‚æœæœ‰ï¼‰
    if tool_calls:
        for tool_call in tool_calls:
            if tool_call and tool_call.get("type") == "function" and "function" in tool_call:
                func = tool_call["function"]
                func_name = func.get("name", "")
                # OpenAI çš„ arguments æ˜¯ JSON å­—ç¬¦ä¸²ï¼Œéœ€è¦è§£æ
                args_str = func.get("arguments", "{}")
                func_args = json.loads(args_str) if args_str else {}

                parts.append({
                    "functionCall": {
                        "name": func_name,
                        "args": func_args
                    }
                })
```

#### 3. å€™é€‰é¡¹æ„å»º

```python
candidate = {
    "content": {
        "parts": parts if parts else [{"text": ""}],
        "role": "model"
    },
    "finishReason": self._map_finish_reason(choice.get("finish_reason", "stop"), "openai", "gemini"),
    "index": 0
}
result_data["candidates"] = [candidate]
```

#### 4. ä½¿ç”¨é‡ä¿¡æ¯è½¬æ¢

```python
if "usage" in data and data["usage"] is not None:
    usage = data["usage"]
    result_data["usageMetadata"] = {
        "promptTokenCount": usage.get("prompt_tokens", 0),
        "candidatesTokenCount": usage.get("completion_tokens", 0),
        "totalTokenCount": usage.get("total_tokens", 0)
    }
```

#### 5. ç»“æŸåŸå› æ˜ å°„

```python
reason_mappings = {
    "openai": {
        "gemini": {
            "stop": "STOP",
            "length": "MAX_TOKENS",
            "content_filter": "SAFETY",
            "tool_calls": "MODEL_REQUESTED_TOOL"
        }
    }
}
```

---

## æµå¼å“åº”è½¬æ¢

### æµå¼ chunk è½¬æ¢

æµå¼ chunk è½¬æ¢ä½äº `src/formats/gemini_converter.py` çš„ `_convert_from_openai_streaming_chunk` æ–¹æ³•ï¼ˆç¬¬ 634-782 è¡Œï¼‰ã€‚

### å…³é”®è®¾è®¡ï¼šå·¥å…·è°ƒç”¨çŠ¶æ€ç´¯ç§¯

OpenAI æµå¼å“åº”ä¸­ï¼Œå·¥å…·è°ƒç”¨çš„å‚æ•°æ˜¯é€æ­¥å‘é€çš„ï¼Œéœ€è¦ç´¯ç§¯ååœ¨ç»“æŸæ—¶ä¸€æ¬¡æ€§è¾“å‡ºï¼š

```python
def _convert_from_openai_streaming_chunk(self, data: Dict[str, Any]) -> ConversionResult:
    # ä¸ºæµå¼å·¥å…·è°ƒç”¨ç»´æŠ¤çŠ¶æ€
    if not hasattr(self, '_streaming_tool_calls'):
        self._streaming_tool_calls = {}

    # æ”¶é›†æµå¼å·¥å…·è°ƒç”¨ä¿¡æ¯
    if "choices" in data and data["choices"] and data["choices"][0]:
        choice = data["choices"][0]
        delta = choice.get("delta", {})

        if "tool_calls" in delta:
            for tool_call in delta["tool_calls"]:
                call_index = tool_call.get("index", 0)

                # åˆå§‹åŒ–å·¥å…·è°ƒç”¨çŠ¶æ€
                if call_index not in self._streaming_tool_calls:
                    self._streaming_tool_calls[call_index] = {
                        "id": tool_call.get("id", ""),
                        "type": "function",
                        "function": {
                            "name": "",
                            "arguments": ""
                        }
                    }

                # ç´¯ç§¯å‡½æ•°åå’Œå‚æ•°
                if "function" in tool_call:
                    func = tool_call["function"]
                    if "name" in func:
                        self._streaming_tool_calls[call_index]["function"]["name"] = func["name"]
                    if "arguments" in func:
                        self._streaming_tool_calls[call_index]["function"]["arguments"] += func["arguments"]
```

### ç»“æŸ chunk å¤„ç†

å½“ OpenAI å“åº”ä¸­åŒ…å« `finish_reason` æ—¶ï¼Œè¾“å‡ºå®Œæ•´çš„å·¥å…·è°ƒç”¨ï¼š

```python
if data["choices"][0].get("finish_reason"):
    parts = []

    # å¤„ç†æ–‡æœ¬å†…å®¹
    content = delta.get("content", "")
    if content:
        parts.append({"text": content})

    # å¤„ç†æ”¶é›†åˆ°çš„å·¥å…·è°ƒç”¨
    if self._streaming_tool_calls:
        for call_index, tool_call in self._streaming_tool_calls.items():
            func = tool_call.get("function", {})
            func_name = func.get("name", "")
            func_args = func.get("arguments", "{}")

            # è§£æJSONå‚æ•°
            func_args_json = json.loads(func_args) if func_args else {}

            parts.append({
                "functionCall": {
                    "name": func_name,
                    "args": func_args_json
                }
            })

        # æ¸…ç†å·¥å…·è°ƒç”¨çŠ¶æ€
        self._streaming_tool_calls = {}

    result_data = {
        "candidates": [{
            "content": {
                "parts": parts,
                "role": "model"
            },
            "finishReason": self._map_finish_reason(finish_reason, "openai", "gemini"),
            "index": 0
        }]
    }

    # æ·»åŠ usageä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
    if "usage" in data and data["usage"] is not None:
        result_data["usageMetadata"] = {...}

    return ConversionResult(success=True, data=result_data)
```

### æ™®é€šæ–‡æœ¬ chunk å¤„ç†

å¯¹äºçº¯æ–‡æœ¬çš„æµå¼å†…å®¹ï¼Œå®æ—¶è½¬å‘ï¼š

```python
elif "choices" in data and data["choices"] and data["choices"][0]:
    delta = choice.get("delta", {})
    content = delta.get("content", "")

    if content:
        result_data = {
            "candidates": [{
                "content": {
                    "parts": [{"text": content}],
                    "role": "model"
                },
                "index": 0
            }]
        }
        return ConversionResult(success=True, data=result_data)
```

### æµå¼å“åº”æ ¼å¼å¯¹æ¯”

**OpenAI æµå¼å“åº”æ ¼å¼**ï¼š
```
data: {"id":"chatcmpl-xxx","object":"chat.completion.chunk","choices":[{"index":0,"delta":{"content":"Hello"},"finish_reason":null}]}
data: {"id":"chatcmpl-xxx","object":"chat.completion.chunk","choices":[{"index":0,"delta":{"content":" world"},"finish_reason":"stop"}],"usage":{...}}
data: [DONE]
```

**è½¬æ¢åçš„ Gemini æµå¼å“åº”æ ¼å¼**ï¼š
```
data: {"candidates":[{"content":{"parts":[{"text":"Hello"}],"role":"model"},"index":0}]}
data: {"candidates":[{"content":{"parts":[{"text":" world"}],"role":"model"},"finishReason":"STOP","index":0}],"usageMetadata":{...}}
```

---

## å­—æ®µæ˜ å°„è¯¦è§£

### è¯·æ±‚å­—æ®µæ˜ å°„è¡¨

| Gemini å­—æ®µ | OpenAI å­—æ®µ | è½¬æ¢è¯´æ˜ |
|-------------|-------------|----------|
| `model` | `model` | ä½¿ç”¨åŸå§‹è¯·æ±‚çš„æ¨¡å‹å |
| `systemInstruction.parts[].text` | `messages[role="system"].content` | ç³»ç»Ÿæ¶ˆæ¯ |
| `system_instruction.parts[].text` | `messages[role="system"].content` | ç³»ç»Ÿæ¶ˆæ¯ï¼ˆsnake_case å†™æ³•ï¼‰ |
| `contents[role="user"]` | `messages[role="user"]` | ç”¨æˆ·æ¶ˆæ¯ |
| `contents[role="model"]` | `messages[role="assistant"]` | åŠ©æ‰‹æ¶ˆæ¯ |
| `contents[].parts[].functionCall` | `messages[].tool_calls[]` | å·¥å…·è°ƒç”¨ |
| `contents[].parts[].functionResponse` | `messages[role="tool"]` | å·¥å…·å“åº” |
| `generationConfig.temperature` | `temperature` | æ¸©åº¦ |
| `generationConfig.topP` | `top_p` | Top-P |
| `generationConfig.maxOutputTokens` | `max_tokens` | æœ€å¤§è¾“å‡ºï¼ˆæ™®é€šæ¨¡å¼ï¼‰ |
| `generationConfig.maxOutputTokens` | `max_completion_tokens` | æœ€å¤§è¾“å‡ºï¼ˆæ€è€ƒæ¨¡å¼ï¼‰ |
| `generationConfig.stopSequences` | `stop` | åœæ­¢åºåˆ— |
| `tools[].function_declarations[]` | `tools[].function` | å·¥å…·å®šä¹‰ |
| `tools[].functionDeclarations[]` | `tools[].function` | å·¥å…·å®šä¹‰ï¼ˆcamelCase å†™æ³•ï¼‰ |
| `generationConfig.thinkingConfig.thinkingBudget` | `reasoning_effort` | æ€è€ƒç­‰çº§æ˜ å°„ |
| `generationConfig.response_mime_type` | `response_format.type` | å“åº”æ ¼å¼ |
| `generationConfig.response_schema` | `response_format.json_schema` | JSON Schema |

### å“åº”å­—æ®µæ˜ å°„è¡¨

| OpenAI å­—æ®µ | Gemini å­—æ®µ | è½¬æ¢è¯´æ˜ |
|-------------|-------------|----------|
| `choices[0].message.content` | `candidates[0].content.parts[].text` | æ–‡æœ¬å†…å®¹ |
| `choices[0].message.tool_calls[]` | `candidates[0].content.parts[].functionCall` | å·¥å…·è°ƒç”¨ |
| `choices[0].finish_reason` | `candidates[0].finishReason` | ç»“æŸåŸå› æ˜ å°„ |
| `usage.prompt_tokens` | `usageMetadata.promptTokenCount` | è¾“å…¥ token |
| `usage.completion_tokens` | `usageMetadata.candidatesTokenCount` | è¾“å‡º token |
| `usage.total_tokens` | `usageMetadata.totalTokenCount` | æ€» token |

### ç»“æŸåŸå› æ˜ å°„è¡¨

| OpenAI finish_reason | Gemini finishReason |
|----------------------|---------------------|
| `stop` | `STOP` |
| `length` | `MAX_TOKENS` |
| `content_filter` | `SAFETY` |
| `tool_calls` | `MODEL_REQUESTED_TOOL` |

### ç±»å‹æ˜ å°„è¡¨ï¼ˆJSON Schemaï¼‰

| Gemini ç±»å‹ | OpenAI ç±»å‹ |
|-------------|-------------|
| `STRING` | `string` |
| `NUMBER` | `number` |
| `INTEGER` | `integer` |
| `BOOLEAN` | `boolean` |
| `ARRAY` | `array` |
| `OBJECT` | `object` |

---

## ç‰¹æ®Šå¤„ç†é€»è¾‘

### 1. å·¥å…·è°ƒç”¨ ID ä¸€è‡´æ€§

Gemini çš„ `functionCall` å’Œ `functionResponse` æ²¡æœ‰ ID å­—æ®µï¼Œä½† OpenAI éœ€è¦ `tool_call_id` æ¥å…³è”å·¥å…·è°ƒç”¨å’Œå“åº”ã€‚è§£å†³æ–¹æ¡ˆæ˜¯é¢„æ‰«æå¯¹è¯å†å²ï¼Œä¸ºæ¯ä¸ªè°ƒç”¨ç”Ÿæˆä¸€è‡´çš„ IDï¼š

```python
# ç”Ÿæˆè§„åˆ™ï¼šcall_{function_name}_{åºå·:04d}
# ä¾‹å¦‚ï¼šcall_get_weather_0001, call_search_0001, call_get_weather_0002
tool_call_id = f"call_{func_name}_{sequence:04d}"
```

### 2. å¤šæ¨¡æ€å†…å®¹è½¬æ¢

**Gemini æ ¼å¼**ï¼š
```python
{
    "parts": [{
        "inlineData": {
            "mimeType": "image/jpeg",
            "data": "base64_encoded_data"
        }
    }]
}
```

**è½¬æ¢ä¸º OpenAI æ ¼å¼**ï¼š
```python
{
    "type": "image_url",
    "image_url": {
        "url": "data:image/jpeg;base64,base64_encoded_data"
    }
}
```

**ä»£ç å®ç°**ï¼ˆç¬¬ 977-1005 è¡Œï¼‰ï¼š

```python
def _convert_content_from_gemini(self, parts: List[Dict[str, Any]]) -> Any:
    for part in parts:
        if "text" in part:
            text_content += part["text"]
        elif "inlineData" in part:
            inline_data = part["inlineData"]
            mime_type = inline_data.get("mimeType", "image/jpeg")
            data_part = inline_data.get("data", "")
            converted_content.append({
                "type": "image_url",
                "image_url": {
                    "url": f"data:{mime_type};base64,{data_part}"
                }
            })
```

### 3. å‡½æ•°å‚æ•°æ ¼å¼è½¬æ¢

Gemini çš„ `args` æ˜¯å¯¹è±¡ï¼ŒOpenAI çš„ `arguments` æ˜¯ JSON å­—ç¬¦ä¸²ï¼š

```python
# Gemini â†’ OpenAIï¼šå¯¹è±¡ â†’ JSONå­—ç¬¦ä¸²
"arguments": json.dumps(fc.get("args", {}), ensure_ascii=False)

# OpenAI â†’ Geminiï¼šJSONå­—ç¬¦ä¸² â†’ å¯¹è±¡
func_args = json.loads(args_str) if args_str else {}
```

### 4. æ€è€ƒé¢„ç®—åŠ¨æ€å€¼å¤„ç†

å½“ `thinkingBudget` ä¸º `-1` æ—¶ï¼Œè¡¨ç¤ºåŠ¨æ€æ€è€ƒæ¨¡å¼ï¼š

```python
if thinking_budget is None or thinking_budget == -1:
    # åŠ¨æ€æ€è€ƒï¼Œé»˜è®¤ä¸º high
    return "high"
```

### 5. ç©ºå†…å®¹å¤„ç†

Gemini ä¸å…è®¸ç©ºçš„ parts æ•°ç»„ï¼š

```python
if not parts:
    parts = [{"text": ""}]
```

### 6. æµå¼å·¥å…·è°ƒç”¨å‚æ•°ç´¯ç§¯

OpenAI æµå¼å“åº”ä¸­ï¼Œå·¥å…·è°ƒç”¨çš„å‚æ•°æ˜¯é€æ­¥å‘é€çš„ JSON ç‰‡æ®µï¼Œéœ€è¦ç´¯ç§¯ï¼š

```python
# ç´¯ç§¯ arguments
self._streaming_tool_calls[call_index]["function"]["arguments"] += func["arguments"]

# æœ€ç»ˆè§£æ
func_args_json = json.loads(func_args) if func_args else {}
```

### 7. max_completion_tokens ä¼˜å…ˆçº§

æ€è€ƒæ¨¡å¼ä¸‹çš„ token é™åˆ¶ä¼˜å…ˆçº§ï¼š

1. å®¢æˆ·ç«¯ä¼ å…¥çš„ `maxOutputTokens`ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
2. ç¯å¢ƒå˜é‡ `OPENAI_REASONING_MAX_TOKENS`
3. éƒ½æ²¡æœ‰åˆ™æŠ¥é”™

```python
if "maxOutputTokens" in data["generationConfig"]:
    max_completion_tokens = data["generationConfig"]["maxOutputTokens"]
    result_data.pop("max_tokens", None)  # ç§»é™¤æ™®é€šæ¨¡å¼çš„ max_tokens
else:
    env_max_tokens = os.environ.get("OPENAI_REASONING_MAX_TOKENS")
    if env_max_tokens:
        max_completion_tokens = int(env_max_tokens)
    else:
        raise ConversionError("max_completion_tokens is required for reasoning models")
```

---

## é”™è¯¯å¤„ç†

### è½¬æ¢é”™è¯¯

```python
# convert_request æ–¹æ³•
try:
    return self._convert_to_openai_request(data)
except Exception as e:
    self.logger.error(f"Failed to convert Gemini request to openai: {e}")
    return ConversionResult(success=False, error=str(e))
```

### å¿…éœ€ç¯å¢ƒå˜é‡ç¼ºå¤±

æ€è€ƒæ¨¡å¼è½¬æ¢éœ€è¦ç‰¹å®šç¯å¢ƒå˜é‡ï¼š

```python
if low_threshold_str is None:
    raise ConversionError("GEMINI_TO_OPENAI_LOW_REASONING_THRESHOLD environment variable is required")

if high_threshold_str is None:
    raise ConversionError("GEMINI_TO_OPENAI_HIGH_REASONING_THRESHOLD environment variable is required")
```

### JSON è§£æé”™è¯¯

å·¥å…·è°ƒç”¨å‚æ•°è§£æå¤±è´¥æ—¶çš„å¤„ç†ï¼š

```python
try:
    func_args = json.loads(args_str) if args_str else {}
except json.JSONDecodeError:
    func_args = {}
```

### æµå¼å“åº”ä¸­çš„ [DONE] æ ‡è®°

```python
if func_args.strip() == "[DONE]":
    self.logger.warning(f"Found [DONE] in tool call arguments, skipping")
    continue
```

---

## é…ç½®é¡¹è¯´æ˜

### ç¯å¢ƒå˜é‡

| ç¯å¢ƒå˜é‡ | å¿…éœ€ | è¯´æ˜ | ç¤ºä¾‹å€¼ |
|----------|------|------|--------|
| `GEMINI_TO_OPENAI_LOW_REASONING_THRESHOLD` | æ˜¯ï¼ˆæ€è€ƒæ¨¡å¼ï¼‰ | low ç­‰çº§çš„ thinking_budget ä¸Šé™ | `4096` |
| `GEMINI_TO_OPENAI_HIGH_REASONING_THRESHOLD` | æ˜¯ï¼ˆæ€è€ƒæ¨¡å¼ï¼‰ | medium ç­‰çº§çš„ thinking_budget ä¸Šé™ | `16384` |
| `OPENAI_REASONING_MAX_TOKENS` | å¦ | æ€è€ƒæ¨¡å¼çš„é»˜è®¤ max_completion_tokens | `32768` |
| `ANTHROPIC_MAX_TOKENS` | å¦ | è½¬ Anthropic æ—¶çš„é»˜è®¤ max_tokens | `4096` |

### é˜ˆå€¼é…ç½®è¯´æ˜

```
thinkingBudget â‰¤ LOW_THRESHOLD  â†’  reasoning_effort = "low"
LOW_THRESHOLD < thinkingBudget â‰¤ HIGH_THRESHOLD  â†’  reasoning_effort = "medium"
thinkingBudget > HIGH_THRESHOLD  â†’  reasoning_effort = "high"
thinkingBudget = -1 (åŠ¨æ€)  â†’  reasoning_effort = "high"
```

### æ¸ é“é…ç½®

æ¸ é“é…ç½®ä¸­å¯ä»¥è®¾ç½®æ¨¡å‹æ˜ å°„ï¼š

```python
if channel.models_mapping and isinstance(request_data, dict):
    original_model = request_data.get("model")
    if original_model:
        mapped_model = channel.models_mapping.get(original_model)
        if mapped_model:
            logger.info(f"Applying model mapping: {original_model} -> {mapped_model}")
```

---

## å®Œæ•´ç¤ºä¾‹

### ç¤ºä¾‹ 1ï¼šåŸºç¡€å¯¹è¯è¯·æ±‚

**Gemini è¯·æ±‚**ï¼š
```json
{
    "systemInstruction": {
        "parts": [{"text": "You are a helpful assistant."}]
    },
    "contents": [
        {
            "role": "user",
            "parts": [{"text": "What is the capital of France?"}]
        }
    ],
    "generationConfig": {
        "temperature": 0.7,
        "maxOutputTokens": 1000
    }
}
```

**è½¬æ¢åçš„ OpenAI è¯·æ±‚**ï¼š
```json
{
    "model": "gpt-4",
    "messages": [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "What is the capital of France?"}
    ],
    "temperature": 0.7,
    "max_tokens": 1000
}
```

### ç¤ºä¾‹ 2ï¼šå·¥å…·è°ƒç”¨è¯·æ±‚

**Gemini è¯·æ±‚**ï¼š
```json
{
    "contents": [
        {
            "role": "user",
            "parts": [{"text": "What's the weather in Beijing?"}]
        }
    ],
    "tools": [{
        "function_declarations": [{
            "name": "get_weather",
            "description": "Get current weather",
            "parameters": {
                "type": "OBJECT",
                "properties": {
                    "location": {"type": "STRING", "description": "City name"}
                },
                "required": ["location"]
            }
        }]
    }],
    "generationConfig": {
        "temperature": 0.7
    }
}
```

**è½¬æ¢åçš„ OpenAI è¯·æ±‚**ï¼š
```json
{
    "model": "gpt-4",
    "messages": [
        {"role": "user", "content": "What's the weather in Beijing?"}
    ],
    "tools": [{
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "Get current weather",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {"type": "string", "description": "City name"}
                },
                "required": ["location"]
            }
        }
    }],
    "tool_choice": "auto",
    "temperature": 0.7
}
```

### ç¤ºä¾‹ 3ï¼šå·¥å…·å“åº”æµç¨‹

**Gemini è¯·æ±‚ï¼ˆåŒ…å«å·¥å…·è°ƒç”¨ç»“æœï¼‰**ï¼š
```json
{
    "contents": [
        {
            "role": "user",
            "parts": [{"text": "What's the weather in Beijing?"}]
        },
        {
            "role": "model",
            "parts": [{
                "functionCall": {
                    "name": "get_weather",
                    "args": {"location": "Beijing"}
                }
            }]
        },
        {
            "role": "user",
            "parts": [{
                "functionResponse": {
                    "name": "get_weather",
                    "response": {"content": "Sunny, 25Â°C"}
                }
            }]
        }
    ]
}
```

**è½¬æ¢åçš„ OpenAI è¯·æ±‚**ï¼š
```json
{
    "model": "gpt-4",
    "messages": [
        {"role": "user", "content": "What's the weather in Beijing?"},
        {
            "role": "assistant",
            "content": null,
            "tool_calls": [{
                "id": "call_get_weather_0001",
                "type": "function",
                "function": {
                    "name": "get_weather",
                    "arguments": "{\"location\": \"Beijing\"}"
                }
            }]
        },
        {
            "role": "tool",
            "tool_call_id": "call_get_weather_0001",
            "content": "Sunny, 25Â°C"
        }
    ]
}
```

### ç¤ºä¾‹ 4ï¼šæ€è€ƒæ¨¡å¼è¯·æ±‚

**Gemini è¯·æ±‚**ï¼š
```json
{
    "contents": [
        {
            "role": "user",
            "parts": [{"text": "Solve this complex math problem..."}]
        }
    ],
    "generationConfig": {
        "thinkingConfig": {
            "thinkingBudget": 10000
        },
        "maxOutputTokens": 4096
    }
}
```

**è½¬æ¢åçš„ OpenAI è¯·æ±‚**ï¼ˆå‡è®¾ LOW=4096, HIGH=16384ï¼‰ï¼š
```json
{
    "model": "o1",
    "messages": [
        {"role": "user", "content": "Solve this complex math problem..."}
    ],
    "reasoning_effort": "medium",
    "max_completion_tokens": 4096
}
```

### ç¤ºä¾‹ 5ï¼šOpenAI å“åº”è½¬ Gemini

**OpenAI å“åº”**ï¼š
```json
{
    "id": "chatcmpl-abc123",
    "object": "chat.completion",
    "created": 1234567890,
    "model": "gpt-4",
    "choices": [{
        "index": 0,
        "message": {
            "role": "assistant",
            "content": null,
            "tool_calls": [{
                "id": "call_xyz",
                "type": "function",
                "function": {
                    "name": "get_weather",
                    "arguments": "{\"location\": \"Beijing\"}"
                }
            }]
        },
        "finish_reason": "tool_calls"
    }],
    "usage": {
        "prompt_tokens": 50,
        "completion_tokens": 20,
        "total_tokens": 70
    }
}
```

**è½¬æ¢åçš„ Gemini å“åº”**ï¼š
```json
{
    "candidates": [{
        "content": {
            "parts": [{
                "functionCall": {
                    "name": "get_weather",
                    "args": {"location": "Beijing"}
                }
            }],
            "role": "model"
        },
        "finishReason": "MODEL_REQUESTED_TOOL",
        "index": 0
    }],
    "usageMetadata": {
        "promptTokenCount": 50,
        "candidatesTokenCount": 20,
        "totalTokenCount": 70
    }
}
```

---

## å¤šè½®å·¥å…·è°ƒç”¨çŠ¶æ€ä¿ç•™

### æ¦‚è¿°

å½“ä½¿ç”¨ OpenRouter ç­‰ç¬¬ä¸‰æ–¹ API ä»£ç†è°ƒç”¨ Gemini 3 Pro ç­‰æ¨ç†æ¨¡å‹è¿›è¡Œå¤šè½®å·¥å…·è°ƒç”¨æ—¶ï¼Œéœ€è¦ä¿ç•™ä¸¤ä¸ªå…³é”®å­—æ®µï¼š

1. **`thoughtSignature`**ï¼šGemini åŸç”Ÿçš„æ€è€ƒç­¾åï¼Œç”¨äºéªŒè¯å·¥å…·è°ƒç”¨çš„æ¨ç†è¿‡ç¨‹
2. **`reasoning_details`**ï¼šOpenRouter ç‰¹æœ‰çš„æ¨ç†è¯¦æƒ…ï¼Œç”¨äºåœ¨å¤šè½®å¯¹è¯ä¸­ä¿æŒæ¨ç†ä¸Šä¸‹æ–‡

å¦‚æœä¸æ­£ç¡®ä¿ç•™è¿™äº›å­—æ®µï¼Œå¤šè½®å·¥å…·è°ƒç”¨ä¼šè¿”å› 400 é”™è¯¯ï¼š
- `"Function call is missing a thought_signature in functionCall parts"`
- `"Gemini models require OpenRouter reasoning details to be preserved in each request"`

### thoughtSignature å¤„ç†é€»è¾‘

#### èƒŒæ™¯

`thoughtSignature` æ˜¯ Google Gemini API åœ¨ä½¿ç”¨æ¨ç†æ¨¡å‹ï¼ˆå¦‚ Gemini 2.5 Flash/Proï¼‰è¿›è¡Œå·¥å…·è°ƒç”¨æ—¶è¿”å›çš„ç­¾åå­—æ®µã€‚å®ƒä½äºå“åº”çš„ `functionCall` åŒçº§ä½ç½®ï¼Œç”¨äºéªŒè¯æ¨ç†è¿‡ç¨‹çš„å®Œæ•´æ€§ã€‚

**å‚è€ƒæ–‡æ¡£**: https://ai.google.dev/gemini-api/docs/thought-signatures

#### æ•°æ®ç»“æ„

```python
# GeminiConverter.__init__
self._thought_signatures_by_tool_call_id: Dict[str, str] = {}
# key: tool_call_id (å¦‚ "call_get_weather_0001")
# value: thoughtSignature å­—ç¬¦ä¸²
```

#### æ•è·æµç¨‹

åœ¨ Gemini â†’ OpenAI è¯·æ±‚è½¬æ¢æ—¶ï¼Œä» `functionCall` åŒçº§æå– `thoughtSignature`ï¼š

```python
# _convert_content_from_gemini æ–¹æ³•
elif "functionCall" in part:
    fc = part["functionCall"]
    tool_call_id = fc.get("id") or f"call_{func_name}_{sequence:04d}"

    # æå–å¹¶ä¿å­˜ thoughtSignature
    thought_signature = part.get("thoughtSignature")
    if thought_signature:
        self._thought_signatures_by_tool_call_id[tool_call_id] = thought_signature
        print(f"ğŸ§  [THOUGHT_SIGNATURE] Captured: tool_call_id={tool_call_id}")
```

**Gemini å“åº”æ ¼å¼**ï¼ˆåŒ…å« thoughtSignatureï¼‰ï¼š
```json
{
    "candidates": [{
        "content": {
            "parts": [{
                "functionCall": {
                    "name": "get_weather",
                    "args": {"location": "Beijing"},
                    "id": "call_xyz"
                },
                "thoughtSignature": "eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9..."
            }],
            "role": "model"
        }
    }]
}
```

#### å›å¡«æµç¨‹

åœ¨ OpenAI â†’ Gemini å“åº”è½¬æ¢æ—¶ï¼Œå°†ç¼“å­˜çš„ `thoughtSignature` å›å¡«åˆ° `functionCall` åŒçº§ï¼š

```python
# _convert_from_openai_response æ–¹æ³•
part: Dict[str, Any] = {"functionCall": function_call}

# å›å¡« thoughtSignature
if tool_call_id and tool_call_id in self._thought_signatures_by_tool_call_id:
    thought_signature = self._thought_signatures_by_tool_call_id[tool_call_id]
    part["thoughtSignature"] = thought_signature
    print(f"ğŸ§  [THOUGHT_SIGNATURE] Restored: tool_call_id={tool_call_id}")

parts.append(part)
```

#### å›å¡«ä½ç½®

| è½¬æ¢æ–¹æ³• | è¯´æ˜ |
|---------|------|
| `_convert_from_openai_response` | éæµå¼ OpenAI å“åº” â†’ Gemini |
| `_convert_from_openai_streaming_chunk` | æµå¼ OpenAI å“åº” â†’ Gemini |
| `_convert_from_anthropic_response` | éæµå¼ Anthropic å“åº” â†’ Gemini |
| `_convert_from_anthropic_streaming_chunk` | æµå¼ Anthropic å“åº” â†’ Gemini |

#### ç”Ÿå‘½å‘¨æœŸ

- **åˆ›å»º**: æ¯æ¬¡è¯·æ±‚è½¬æ¢å¼€å§‹æ—¶æ¸…ç©º (`_convert_to_openai_request`)
- **ä¿ç•™**: åœ¨åŒä¸€è¯·æ±‚çš„å“åº”è½¬æ¢ä¸­ä½¿ç”¨
- **æ¸…ç†**: ä¸‹ä¸€æ¬¡è¯·æ±‚è½¬æ¢å¼€å§‹æ—¶æ¸…ç©º

---

### reasoning_details å¤„ç†é€»è¾‘

#### èƒŒæ™¯

`reasoning_details` æ˜¯ OpenRouter åœ¨è°ƒç”¨æ¨ç†æ¨¡å‹æ—¶è¿”å›çš„æ¨ç†è¯¦æƒ…æ•°ç»„ã€‚å®ƒåŒ…å«æ¨¡å‹çš„æ€è€ƒè¿‡ç¨‹ã€åŠ å¯†çš„æ¨ç†é“¾ç­‰ä¿¡æ¯ã€‚**åœ¨å¤šè½®å·¥å…·è°ƒç”¨ä¸­ï¼Œå¿…é¡»å°† `reasoning_details` åŸæ ·å›ä¼ åˆ°åç»­è¯·æ±‚çš„ assistant æ¶ˆæ¯ä¸­**ã€‚

**å‚è€ƒæ–‡æ¡£**: https://openrouter.ai/docs/guides/best-practices/reasoning-tokens

#### æ•°æ®ç»“æ„

```python
# GeminiConverter ç±»å±æ€§ï¼ˆç¼“å­˜é…ç½®ï¼‰
_CACHE_TTL_SECONDS = 3600  # 1 å°æ—¶ TTL
_CACHE_MAX_SIZE = 1000     # æœ€å¤§æ¡ç›®æ•°

# GeminiConverter.__init__
self._reasoning_details_cache: OrderedDict[str, Dict[str, Any]] = OrderedDict()
# key: é¦–ä¸ª tool_call_id (å¦‚ "call_get_weather_0001")
# value: {"data": reasoning_details æ•°ç»„, "ts": monotonic æ—¶é—´æˆ³}
self._cache_lock = Lock()  # çº¿ç¨‹å®‰å…¨é”
```

**å…³é”®è®¾è®¡**:
- ä½¿ç”¨é¦–ä¸ª `tool_call_id` ä½œä¸ºç´¢å¼•ï¼Œå› ä¸ºæ¯ä¸ªåŒ…å«å·¥å…·è°ƒç”¨çš„ assistant æ¶ˆæ¯éƒ½æœ‰å”¯ä¸€çš„é¦–ä¸ª tool_call_id
- ä½¿ç”¨ `OrderedDict` å®ç° LRU æ·˜æ±°ç­–ç•¥
- ä½¿ç”¨ `monotonic()` æ—¶é—´æˆ³é¿å…ç³»ç»Ÿæ—¶é’Ÿå›æ‹¨å½±å“

#### reasoning_details ç»“æ„

OpenRouter è¿”å›çš„ `reasoning_details` æ˜¯ä¸€ä¸ªæ•°ç»„ï¼ŒåŒ…å«å¤šç§ç±»å‹çš„æ¨ç†å—ï¼š

```json
{
    "choices": [{
        "message": {
            "role": "assistant",
            "content": null,
            "tool_calls": [...],
            "reasoning_details": [
                {
                    "type": "reasoning.summary",
                    "summary": "åˆ†æé—®é¢˜å¹¶ç¡®å®šéœ€è¦è°ƒç”¨å¤©æ°”å·¥å…·",
                    "format": "anthropic-claude-v1",
                    "index": 0
                },
                {
                    "type": "reasoning.encrypted",
                    "data": "eyJlbmNyeXB0ZWQiOiJ0cnVlIn0=",
                    "format": "anthropic-claude-v1",
                    "index": 1
                },
                {
                    "type": "reasoning.text",
                    "text": "è®©æˆ‘ä¸€æ­¥æ­¥æ€è€ƒè¿™ä¸ªé—®é¢˜...",
                    "signature": "sha256:abc123...",
                    "format": "anthropic-claude-v1",
                    "index": 2
                }
            ]
        }
    }]
}
```

#### ç¼“å­˜æµç¨‹

åœ¨ OpenAI â†’ Gemini å“åº”è½¬æ¢æ—¶ï¼Œæ•è· `reasoning_details` å¹¶å­˜å…¥ç¼“å­˜ï¼š

**éæµå¼å“åº”** (`_convert_from_openai_response`):
```python
reasoning_details = message.get("reasoning_details")
if reasoning_details and tool_calls:
    first_tool_call_id = tool_calls[0].get("id")
    if first_tool_call_id:
        self._cache_reasoning_details(first_tool_call_id, reasoning_details)  # ä½¿ç”¨ç¼“å­˜æ–¹æ³•
        print(f"ğŸ§  [REASONING_DETAILS] Cached {len(reasoning_details)} items (key={first_tool_call_id})")
```

**æµå¼å“åº”** (`_convert_from_openai_streaming_chunk`):
```python
# ä¸´æ—¶å­˜å‚¨
reasoning_details = delta.get("reasoning_details")
if reasoning_details:
    self._streaming_reasoning_details = reasoning_details

# æµå¼ç»“æŸæ—¶å­˜å…¥ cache
if self._streaming_tool_calls and self._streaming_reasoning_details:
    first_index = min(self._streaming_tool_calls.keys())
    first_tool_call_id = self._streaming_tool_calls[first_index].get("id")
    if first_tool_call_id:
        self._cache_reasoning_details(first_tool_call_id, self._streaming_reasoning_details)  # ä½¿ç”¨ç¼“å­˜æ–¹æ³•
```

#### å›ä¼ æµç¨‹

åœ¨ Gemini â†’ OpenAI è¯·æ±‚è½¬æ¢æ—¶ï¼Œä¸ºæ¯ä¸ªåŒ…å«å·¥å…·è°ƒç”¨çš„ assistant æ¶ˆæ¯é™„åŠ å¯¹åº”çš„ `reasoning_details`ï¼š

```python
# _convert_to_openai_request æ–¹æ³•ï¼Œå¤„ç† model è§’è‰²æ¶ˆæ¯
elif gemini_role == "model":
    message_content = self._convert_content_from_gemini(parts)

    if isinstance(message_content, dict) and message_content.get("type") == "tool_calls":
        tool_calls = message_content["tool_calls"]
        message = {
            "role": "assistant",
            "content": tool_call_content,
            "tool_calls": tool_calls
        }

        # ä» cache ä¸­æŸ¥æ‰¾å¯¹åº”çš„ reasoning_detailsï¼ˆä½¿ç”¨ TTL éªŒè¯ï¼‰
        if tool_calls:
            first_tool_call_id = tool_calls[0].get("id")
            if first_tool_call_id:
                cached_details = self._get_cached_reasoning_details(first_tool_call_id)  # TTL + LRU
                if cached_details:
                    message["reasoning_details"] = cached_details
                    print(f"ğŸ§  [REASONING_DETAILS] Attached to assistant message (key={first_tool_call_id})")

        messages.append(message)
```

#### ç”Ÿå‘½å‘¨æœŸä¸æ¸…ç†æœºåˆ¶

- **åˆ›å»º**: å“åº”è½¬æ¢æ—¶é€šè¿‡ `_cache_reasoning_details()` å­˜å…¥ç¼“å­˜
- **ä¿ç•™**: **è·¨è¯·æ±‚ä¿ç•™**ï¼ˆä¸åœ¨ `reset_streaming_state` ä¸­æ¸…ç©ºï¼‰
- **ä½¿ç”¨**: è¯·æ±‚è½¬æ¢æ—¶é€šè¿‡ `_get_cached_reasoning_details()` é™„åŠ åˆ°å¯¹åº”çš„ assistant æ¶ˆæ¯
- **è‡ªåŠ¨æ¸…ç†**: TTL + LRU æœºåˆ¶è‡ªåŠ¨æ¸…ç†è¿‡æœŸå’Œè¶…å®¹é‡æ¡ç›®

#### ç¼“å­˜æ¸…ç†æœºåˆ¶ï¼ˆTTL + LRUï¼‰

ä¸ºé˜²æ­¢é•¿ç”Ÿå‘½å‘¨æœŸå®ä¾‹çš„å†…å­˜æ³„æ¼ï¼Œ`_reasoning_details_cache` å®ç°äº†è‡ªåŠ¨æ¸…ç†æœºåˆ¶ï¼š

| æœºåˆ¶ | è¯´æ˜ |
|------|------|
| **TTL è¿‡æœŸ** | æ¡ç›®è¶…è¿‡ 1 å°æ—¶ï¼ˆ`_CACHE_TTL_SECONDS`ï¼‰è‡ªåŠ¨å¤±æ•ˆ |
| **LRU æ·˜æ±°** | è¶…è¿‡ 1000 æ¡ï¼ˆ`_CACHE_MAX_SIZE`ï¼‰æ—¶æ·˜æ±°æœ€æ—§æ¡ç›® |
| **çº¿ç¨‹å®‰å…¨** | æ‰€æœ‰ç¼“å­˜æ“ä½œä½¿ç”¨ `threading.Lock` ä¿æŠ¤ |
| **æ—¶é’Ÿå®‰å…¨** | ä½¿ç”¨ `time.monotonic()` é¿å…ç³»ç»Ÿæ—¶é’Ÿå›æ‹¨ |

**ç¼“å­˜è¾…åŠ©æ–¹æ³•**:

```python
def _cache_reasoning_details(self, tool_call_id: str, details: List[Dict[str, Any]]):
    """å­˜å‚¨ reasoning_detailsï¼Œå¸¦ TTL å’Œ LRU æ·˜æ±°"""
    with self._cache_lock:
        self._cleanup_stale_cache_locked()  # æ¸…ç†è¿‡æœŸæ¡ç›®
        # å®¹é‡æ·˜æ±°
        if self._CACHE_MAX_SIZE > 0:
            while len(self._reasoning_details_cache) >= self._CACHE_MAX_SIZE:
                evicted_key, _ = self._reasoning_details_cache.popitem(last=False)
                self.logger.debug(f"[CACHE] LRU evicted: {evicted_key}")
        # å­˜å‚¨
        self._reasoning_details_cache[tool_call_id] = {"data": details, "ts": monotonic()}
        self._reasoning_details_cache.move_to_end(tool_call_id)

def _get_cached_reasoning_details(self, tool_call_id: str) -> Optional[List[Dict[str, Any]]]:
    """è·å–ç¼“å­˜çš„ reasoning_detailsï¼Œè¿‡æœŸè¿”å› None"""
    with self._cache_lock:
        entry = self._reasoning_details_cache.get(tool_call_id)
        if not entry:
            return None
        if monotonic() - entry["ts"] > self._CACHE_TTL_SECONDS:
            self._reasoning_details_cache.pop(tool_call_id, None)
            self.logger.debug(f"[CACHE] TTL expired: {tool_call_id}")
            return None
        self._reasoning_details_cache.move_to_end(tool_call_id)  # LRU
        return entry["data"]
```

**è®¾è®¡å†³ç­–**:
- `reset_streaming_state()` **ä¸æ¸…ç©º**ç¼“å­˜ï¼Œå› ä¸ºéœ€è¦è·¨è¯·æ±‚ä¿ç•™ä»¥æ”¯æŒå¤šè½®å·¥å…·è°ƒç”¨
- ä¾èµ– TTL + LRU è‡ªåŠ¨æ¸…ç†ï¼Œé¿å…å†…å­˜æ³„æ¼åŒæ—¶ä¿è¯åŠŸèƒ½æ­£ç¡®æ€§

#### å¤šè½®å·¥å…·è°ƒç”¨ç¤ºä¾‹

**ç¬¬ä¸€è½®**:
```
è¯·æ±‚: ç”¨æˆ·è¯¢é—®å¤©æ°”
å“åº”: assistant è°ƒç”¨ get_weather å·¥å…· (tool_call_id: call_001)
      è¿”å› reasoning_details_1
ç¼“å­˜: {"call_001": reasoning_details_1}
```

**ç¬¬äºŒè½®**:
```
è¯·æ±‚: åŒ…å« functionResponse
      assistant æ¶ˆæ¯éœ€è¦é™„åŠ  reasoning_details_1 (é€šè¿‡ call_001 æŸ¥æ‰¾)
å“åº”: assistant å›å¤ç»“æœæˆ–è°ƒç”¨å¦ä¸€ä¸ªå·¥å…· (tool_call_id: call_002)
      è¿”å› reasoning_details_2
ç¼“å­˜: {"call_001": reasoning_details_1, "call_002": reasoning_details_2}
```

**ç¬¬ä¸‰è½®**:
```
è¯·æ±‚: åŒ…å«å¤šä¸ªå†å² assistant æ¶ˆæ¯
      ç¬¬ä¸€ä¸ª assistant æ¶ˆæ¯é™„åŠ  reasoning_details_1 (key: call_001)
      ç¬¬äºŒä¸ª assistant æ¶ˆæ¯é™„åŠ  reasoning_details_2 (key: call_002)
å“åº”: ...
```

### è°ƒè¯•æ—¥å¿—

| æ—¥å¿—å‰ç¼€ | è¯´æ˜ |
|---------|------|
| `ğŸ§  [THOUGHT_SIGNATURE] Captured` | æ•è· thoughtSignature |
| `ğŸ§  [THOUGHT_SIGNATURE] Restored` | å›å¡« thoughtSignature |
| `ğŸ§  [REASONING_DETAILS] Cached` | ç¼“å­˜ reasoning_details |
| `ğŸ§  [REASONING_DETAILS] Attached` | é™„åŠ  reasoning_details åˆ°è¯·æ±‚ |
| `[CACHE] LRU evicted` | ç¼“å­˜å®¹é‡æ·˜æ±°ï¼ˆDEBUG çº§åˆ«ï¼‰|
| `[CACHE] TTL expired` | ç¼“å­˜ TTL è¿‡æœŸï¼ˆDEBUG çº§åˆ«ï¼‰|

### å¸¸è§é”™è¯¯åŠè§£å†³æ–¹æ¡ˆ

| é”™è¯¯ä¿¡æ¯ | åŸå›  | è§£å†³æ–¹æ¡ˆ |
|---------|------|---------|
| `Function call is missing a thought_signature` | thoughtSignature æœªæ­£ç¡®å›ä¼  | æ£€æŸ¥ Gemini è¯·æ±‚ä¸­ functionCall åŒçº§æ˜¯å¦æœ‰ thoughtSignature |
| `Gemini models require OpenRouter reasoning details to be preserved` | reasoning_details æœªæ­£ç¡®å›ä¼  | æ£€æŸ¥ assistant æ¶ˆæ¯æ˜¯å¦åŒ…å« reasoning_details å­—æ®µ |
| `400 INVALID_ARGUMENT` | å¤šç§åŸå›  | æ£€æŸ¥è°ƒè¯•æ—¥å¿—ï¼Œç¡®è®¤æ•è·å’Œå›ä¼ æ—¥å¿—éƒ½æœ‰è¾“å‡º |

### å‚è€ƒèµ„æ–™

- [Google Gemini Thought Signatures](https://ai.google.dev/gemini-api/docs/thought-signatures)
- [OpenRouter Reasoning Tokens](https://openrouter.ai/docs/guides/best-practices/reasoning-tokens)
- [Vercel Community: Gemini 3 Pro 400 Error](https://community.vercel.com/t/gemini-3-pro-returns-400-invalid-argument-in-vercel-ai-sdk/28040)
- [Open WebUI Issue #19328](https://github.com/open-webui/open-webui/issues/19328)

---

## æ€»ç»“

Gemini åˆ° OpenAI çš„æ ¼å¼è½¬æ¢æ¶‰åŠä»¥ä¸‹æ ¸å¿ƒç‚¹ï¼š

1. **æ¶ˆæ¯ç»“æ„å·®å¼‚**ï¼šGemini ä½¿ç”¨ `contents` æ•°ç»„å’Œå•ç‹¬çš„ `systemInstruction`ï¼ŒOpenAI ä½¿ç”¨ç»Ÿä¸€çš„ `messages` æ•°ç»„
2. **è§’è‰²æ˜ å°„**ï¼š`model` â†’ `assistant`ï¼Œç³»ç»Ÿæ¶ˆæ¯éœ€è¦åˆå¹¶åˆ° messages æ•°ç»„
3. **å·¥å…·è°ƒç”¨æ ¼å¼**ï¼šGemini çš„ `functionDeclarations/functionCall` è½¬æ¢ä¸º OpenAI çš„ `tools/tool_calls`
4. **å‚æ•°æ ¼å¼**ï¼šå‡½æ•°å‚æ•°åœ¨ Gemini ä¸­æ˜¯å¯¹è±¡ï¼Œåœ¨ OpenAI ä¸­æ˜¯ JSON å­—ç¬¦ä¸²
5. **ç±»å‹åç§°**ï¼šGemini ä½¿ç”¨å¤§å†™ç±»å‹åï¼ˆSTRINGï¼‰ï¼ŒOpenAI ä½¿ç”¨å°å†™ï¼ˆstringï¼‰
6. **å·¥å…·è°ƒç”¨ ID**ï¼šéœ€è¦é¢„æ‰«æç”Ÿæˆä¸€è‡´çš„ ID æ˜ å°„
7. **æ€è€ƒæ¨¡å¼**ï¼š`thinkingBudget` æ ¹æ®é˜ˆå€¼æ˜ å°„ä¸º `reasoning_effort` ç­‰çº§
8. **æµå¼å“åº”**ï¼šéœ€è¦ç´¯ç§¯å·¥å…·è°ƒç”¨å‚æ•°ï¼Œåœ¨ç»“æŸæ—¶ä¸€æ¬¡æ€§è¾“å‡ºå®Œæ•´çš„ functionCall
9. **å¤šè½®å·¥å…·è°ƒç”¨çŠ¶æ€ä¿ç•™**ï¼š
   - **thoughtSignature**ï¼šGemini æ¨ç†æ¨¡å‹çš„æ€è€ƒç­¾åï¼Œéœ€è¦åœ¨è¯·æ±‚-å“åº”å¾€è¿”ä¸­ä¿ç•™
   - **reasoning_details**ï¼šOpenRouter æ¨ç†è¯¦æƒ…ï¼Œéœ€è¦è·¨è¯·æ±‚ä¿ç•™å¹¶é™„åŠ åˆ°å¯¹åº”çš„ assistant æ¶ˆæ¯
