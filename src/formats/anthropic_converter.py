"""
Anthropicæ ¼å¼è½¬æ¢å™¨
å¤„ç†Anthropic APIæ ¼å¼ä¸å…¶ä»–æ ¼å¼ä¹‹é—´çš„è½¬æ¢
"""
from typing import Dict, Any, Optional, List, Tuple, Pattern
import json
import copy
import os
import re

from .base_converter import BaseConverter, ConversionResult, ConversionError
from .unified import UnifiedChatRequest, UnifiedChatResponse, UnifiedContent, UnifiedContentType
from .unified.stream_state import StreamState, StreamPhase
from src.utils.logger import log_structured_error, ERROR_TYPE_CONVERSION

# å…¨å±€å·¥å…·çŠ¶æ€ç®¡ç†å™¨
class ToolStateManager:
    _instance = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._tool_mappings = {}
        return cls._instance
    
    def store_tool_mapping(self, func_name: str, tool_id: str):
        """å­˜å‚¨å·¥å…·ååˆ°IDçš„æ˜ å°„"""
        self._tool_mappings[func_name] = tool_id
    
    def get_tool_id(self, func_name: str) -> Optional[str]:
        """æ ¹æ®å·¥å…·åè·å–ID"""
        return self._tool_mappings.get(func_name)
    
    def clear_mappings(self):
        """æ¸…é™¤æ‰€æœ‰æ˜ å°„"""
        self._tool_mappings.clear()

# å…¨å±€å·¥å…·çŠ¶æ€ç®¡ç†å™¨å®ä¾‹
tool_state_manager = ToolStateManager()


class AnthropicConverter(BaseConverter):
    """Anthropicæ ¼å¼è½¬æ¢å™¨"""
    
    def __init__(self):
        super().__init__()
        self.original_model = None
        self._tool_id_mapping = {}  # å­˜å‚¨tool_use_idåˆ°function_nameçš„æ˜ å°„
        
        # ä½¿ç”¨ç»Ÿä¸€çš„æ—¥å¿—è®¾ç½®ï¼ˆç»§æ‰¿è‡ªBaseConverterï¼‰
        # self.logger å·²ç»åœ¨ BaseConverter.__init__() ä¸­æ­£ç¡®è®¾ç½®
    
    def set_original_model(self, model: str):
        """è®¾ç½®åŸå§‹æ¨¡å‹åç§°"""
        self.original_model = model
    
    def _determine_reasoning_effort_from_budget(self, budget_tokens: Optional[int]) -> str:
        """æ ¹æ®budget_tokensæ™ºèƒ½åˆ¤æ–­OpenAI reasoning_effortç­‰çº§
        
        Args:
            budget_tokens: Anthropic thinkingçš„budget_tokenså€¼
            
        Returns:
            str: OpenAI reasoning_effortç­‰çº§ ("low", "medium", "high")
        """
        import os
        
        # å¦‚æœæ²¡æœ‰æä¾›budget_tokensï¼Œé»˜è®¤ä¸ºhigh
        if budget_tokens is None:
            self.logger.info("No budget_tokens provided, defaulting to reasoning_effort='high'")
            return "high"
        
        # ä»ç¯å¢ƒå˜é‡è·å–é˜ˆå€¼é…ç½®ï¼ˆå¸¦é»˜è®¤å€¼ï¼‰
        low_threshold_str = os.environ.get("ANTHROPIC_TO_OPENAI_LOW_REASONING_THRESHOLD", "2048")
        high_threshold_str = os.environ.get("ANTHROPIC_TO_OPENAI_HIGH_REASONING_THRESHOLD", "16384")
        
        try:
            low_threshold = int(low_threshold_str)
            high_threshold = int(high_threshold_str)
            
            self.logger.debug(f"Threshold configuration: low <= {low_threshold}, medium <= {high_threshold}, high > {high_threshold}")
            
            if budget_tokens <= low_threshold:
                effort = "low"
            elif budget_tokens <= high_threshold:
                effort = "medium"
            else:
                effort = "high"
            
            self.logger.info(f"ğŸ¯ Budget tokens {budget_tokens} -> reasoning_effort '{effort}' (thresholds: low<={low_threshold}, high<={high_threshold})")
            return effort
            
        except ValueError as e:
            raise ConversionError(f"Invalid threshold values in environment variables: {e}. ANTHROPIC_TO_OPENAI_LOW_REASONING_THRESHOLD and ANTHROPIC_TO_OPENAI_HIGH_REASONING_THRESHOLD must be integers.")
    
    def reset_streaming_state(self):
        """é‡ç½®æ‰€æœ‰æµå¼ç›¸å…³çš„çŠ¶æ€å˜é‡ï¼Œé¿å…çŠ¶æ€æ±¡æŸ“"""
        self.logger.debug("reset_streaming_state() called - cleaning up streaming state")
        streaming_attrs = [
            '_streaming_state', '_gemini_sent_start', '_gemini_stream_id', 
            '_gemini_text_started'
        ]
        cleaned_attrs = []
        for attr in streaming_attrs:
            if hasattr(self, attr):
                cleaned_attrs.append(attr)
                delattr(self, attr)
        
        self.logger.debug(f"Cleaned streaming attributes: {cleaned_attrs}")
        
        # å¼ºåˆ¶é‡ç½®ï¼Œç¡®ä¿ä¸‹æ¬¡è®¿é—®æ—¶é‡æ–°åˆå§‹åŒ–
        self._force_reset = True
    
    def get_supported_formats(self) -> List[str]:
        """è·å–æ”¯æŒçš„æ ¼å¼åˆ—è¡¨"""
        return ["openai", "anthropic", "gemini"]
    
    def convert_request(
        self,
        data: Dict[str, Any],
        target_format: str,
        headers: Optional[Dict[str, str]] = None
    ) -> ConversionResult:
        """è½¬æ¢Anthropicè¯·æ±‚åˆ°ç›®æ ‡æ ¼å¼"""
        try:
            if target_format == "anthropic":
                # Anthropicåˆ°Anthropicï¼Œæ ¼å¼ä¸æ¸ é“ç›¸åŒï¼Œä¸éœ€è¦è½¬æ¢æ€è€ƒå‚æ•°
                return ConversionResult(success=True, data=data)
            elif target_format == "openai":
                return self._convert_to_openai_request(data)
            elif target_format == "gemini":
                return self._convert_to_gemini_request(data)
            else:
                return ConversionResult(
                    success=False,
                    error=f"Unsupported target format: {target_format}"
                )
        except Exception as e:
            log_structured_error(
                self.logger,
                error_type=ERROR_TYPE_CONVERSION,
                exc=e,
                request_body=data,
                extra={
                    "converter": self.__class__.__name__,
                    "source_format": "anthropic",
                    "target_format": target_format,
                    "stage": "convert_request",
                },
            )
            return ConversionResult(success=False, error=str(e))
    
    def convert_response(
        self,
        data: Dict[str, Any],
        source_format: str,
        target_format: str
    ) -> ConversionResult:
        """è½¬æ¢å“åº”åˆ°Anthropicæ ¼å¼"""
        try:
            if source_format == "anthropic":
                return ConversionResult(success=True, data=data)
            elif source_format == "openai":
                return self._convert_from_openai_response(data)
            elif source_format == "gemini":
                return self._convert_from_gemini_response(data)
            else:
                return ConversionResult(
                    success=False,
                    error=f"Unsupported source format: {source_format}"
                )
        except Exception as e:
            log_structured_error(
                self.logger,
                error_type=ERROR_TYPE_CONVERSION,
                exc=e,
                response_body=data,
                extra={
                    "converter": self.__class__.__name__,
                    "source_format": source_format,
                    "target_format": target_format,
                    "stage": "convert_response",
                },
            )
            return ConversionResult(success=False, error=str(e))
    
    def _convert_to_openai_request(self, data: Dict[str, Any]) -> ConversionResult:
        """è½¬æ¢Anthropicè¯·æ±‚åˆ°OpenAIæ ¼å¼ï¼ˆåŸºäºç»Ÿä¸€ç±»å‹å±‚ï¼‰"""
        # 1) ä½¿ç”¨ UnifiedChatRequest è§£æ Anthropic è¯·æ±‚
        unified_request = UnifiedChatRequest.from_anthropic(data)

        # 1.1) åè®®æ ‡ç­¾æ¸…ç†ï¼šå¯¹ç”¨æˆ·/ç³»ç»Ÿè¾“å…¥åšå®‰å…¨è¿‡æ»¤ï¼Œé˜²æ­¢ä¼ªé€ åè®®æ ‡ç­¾æ³¨å…¥
        if unified_request.system:
            unified_request.system = self._sanitize_protocol_tags(unified_request.system)

        for msg in unified_request.messages:
            # åªæ¸…ç† user/system ä¾§å†…å®¹ï¼Œé¿å…è¯¯æ”¹æ¨¡å‹è¾“å‡º
            if msg.role in ("user", "system"):
                for content_block in msg.content:
                    if content_block.type in (
                        UnifiedContentType.TEXT,
                        UnifiedContentType.THINKING,
                    ):
                        if isinstance(content_block.text, str):
                            content_block.text = self._sanitize_protocol_tags(
                                content_block.text
                            )

        # 2) å·¥å…·å®šä¹‰æ¸…ç†ï¼ˆä¿æŒä¸ OpenAI çš„ JSON Schema å…¼å®¹ï¼‰
        if "tools" in data:
            cleaned_tools: List[Dict[str, Any]] = []
            for tool in data.get("tools", []):
                cleaned_tools.append({
                    "name": tool.get("name", ""),
                    "description": tool.get("description", ""),
                    "input_schema": self._clean_json_schema_properties(tool.get("input_schema", {})),
                })
            unified_request.tools = cleaned_tools

        # 3) æ€è€ƒæ¨¡å¼åˆ° OpenAI reasoning æ¨¡å¼çš„æ˜ å°„
        if unified_request.thinking_enabled:
            budget_tokens = unified_request.thinking_budget_tokens

            # æ ¹æ® budget_tokens æ™ºèƒ½åˆ¤æ–­ reasoning_effort ç­‰çº§
            reasoning_effort = self._determine_reasoning_effort_from_budget(budget_tokens)
            unified_request.reasoning_effort = reasoning_effort

            # å¤„ç† max_completion_tokens çš„ä¼˜å…ˆçº§é€»è¾‘
            max_completion_tokens: Optional[int] = None

            # ä¼˜å…ˆçº§ 1ï¼šå®¢æˆ·ç«¯åœ¨ Anthropic è¯·æ±‚ä¸­ä¼ å…¥çš„ max_tokens
            if "max_tokens" in data:
                max_completion_tokens = unified_request.max_tokens
                unified_request.max_tokens = None  # é¿å…åŒæ—¶ä¸‹å‘ max_tokens ä¸ max_completion_tokens
                self.logger.info(
                    f"Using client max_tokens as max_completion_tokens: {max_completion_tokens}"
                )
            else:
                # ä¼˜å…ˆçº§ 2ï¼šç¯å¢ƒå˜é‡ OPENAI_REASONING_MAX_TOKENS
                env_max_tokens = os.environ.get("OPENAI_REASONING_MAX_TOKENS")
                if env_max_tokens:
                    try:
                        max_completion_tokens = int(env_max_tokens)
                        self.logger.info(
                            f"Using OPENAI_REASONING_MAX_TOKENS from environment: {max_completion_tokens}"
                        )
                    except ValueError:
                        self.logger.warning(
                            f"Invalid OPENAI_REASONING_MAX_TOKENS value '{env_max_tokens}', must be integer"
                        )
                        env_max_tokens = None

                if not env_max_tokens:
                    # ä¼˜å…ˆçº§ 3ï¼šä¸¤è€…éƒ½ç¼ºå¤±æ—¶æŠ›å‡ºé”™è¯¯
                    raise ConversionError(
                        "For OpenAI reasoning models, max_completion_tokens is required. "
                        "Please specify max_tokens in the request or set OPENAI_REASONING_MAX_TOKENS environment variable."
                    )

            unified_request.max_completion_tokens = max_completion_tokens
            self.logger.info(
                f"Anthropic thinking enabled -> OpenAI reasoning_effort='{reasoning_effort}', "
                f"max_completion_tokens={max_completion_tokens}"
            )
            if budget_tokens:
                self.logger.info(
                    f"Budget tokens: {budget_tokens} -> reasoning_effort: '{reasoning_effort}'"
                )

        # 4) ç»Ÿä¸€è¯·æ±‚å¯¹è±¡è½¬æ¢ä¸º OpenAI è¯·æ±‚
        result_data = unified_request.to_openai()

        # 5) tool_choice å…¼å®¹æ€§å¤„ç†
        if "tool_choice" in data:
            result_data["tool_choice"] = data["tool_choice"]
        elif "tools" in data and result_data.get("tools") and "tool_choice" not in result_data:
            result_data["tool_choice"] = "auto"

        # 6) OpenAI å…¼å®¹æ€§æ ¡éªŒï¼šç¡®ä¿æ‰€æœ‰ assistant.tool_calls å‡æœ‰åŒ¹é…çš„ tool å“åº”
        messages = result_data.get("messages", [])
        validated_messages: List[Dict[str, Any]] = []
        for idx, msg in enumerate(messages):
            if msg.get("role") == "assistant" and msg.get("tool_calls"):
                call_ids = [tc.get("id") for tc in msg.get("tool_calls", []) if tc.get("id")]
                unmatched = set(call_ids)

                # åœ¨åç»­æ¶ˆæ¯ä¸­æŸ¥æ‰¾å¯¹åº”çš„ tool å“åº”
                for later in messages[idx + 1:]:
                    if later.get("role") == "tool" and later.get("tool_call_id") in unmatched:
                        unmatched.discard(later["tool_call_id"])
                    if not unmatched:
                        break

                if unmatched:
                    self.logger.warning(
                        f"Unmatched tool_call IDs without tool responses, cleaning: {list(unmatched)}"
                    )
                    msg["tool_calls"] = [
                        tc for tc in msg.get("tool_calls", []) if tc.get("id") not in unmatched
                    ]
                    # å¦‚æœå…¨éƒ¨è¢«ç§»é™¤ï¼Œåˆ™é™çº§ä¸ºæ™®é€š assistant æ–‡æœ¬æ¶ˆæ¯
                    if not msg["tool_calls"]:
                        msg.pop("tool_calls", None)
                        if msg.get("content") is None:
                            msg["content"] = ""

            validated_messages.append(msg)

        result_data["messages"] = validated_messages

        # 7) ä¸æ—§å®ç°ä¿æŒä¸€è‡´ï¼šå¦‚æœåŸè¯·æ±‚æœªæä¾› model å­—æ®µï¼Œåˆ™ä¸åœ¨è¾“å‡ºä¸­å¼ºè¡Œæ·»åŠ 
        if "model" not in data:
            result_data.pop("model", None)

        return ConversionResult(success=True, data=result_data)
    
    def _convert_to_gemini_request(self, data: Dict[str, Any]) -> ConversionResult:
        """è½¬æ¢Anthropicè¯·æ±‚åˆ°Geminiæ ¼å¼"""
        result_data = {}
        
        # å¤„ç†æ¨¡å‹åç§°
        if "model" in data:
            # ç›´æ¥ä½¿ç”¨åŸå§‹æ¨¡å‹åç§°ï¼Œä¸è¿›è¡Œæ˜ å°„
            result_data["model"] = data["model"]
        
        # å¤„ç†ç³»ç»Ÿæ¶ˆæ¯ - åŸºäº2025å¹´Gemini APIæ–‡æ¡£æ ¼å¼
        if "system" in data:
            # ç¡®ä¿ç³»ç»ŸæŒ‡ä»¤å†…å®¹ä¸ä¸ºç©ºä¸”ä¸ºå­—ç¬¦ä¸²
            system_content = str(data["system"]).strip() if data["system"] else ""
            if system_content:
                # å¯¹ç³»ç»ŸæŒ‡ä»¤åšåè®®æ ‡ç­¾è¿‡æ»¤ï¼Œé˜²æ­¢ä¼ªé€  <invoke>/<tool_result>/<thinking> ç­‰æ ‡ç­¾
                system_content = self._sanitize_protocol_tags(system_content)
                result_data["system_instruction"] = {
                    "parts": [{"text": system_content}]
                }
        
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        if "messages" in data:
            # æ„å»ºå·¥å…·è°ƒç”¨IDåˆ°å‡½æ•°åçš„æ˜ å°„
            tool_use_to_name = {}
            for msg in data["messages"]:
                if msg.get("role") == "assistant" and isinstance(msg.get("content"), list):
                    for item in msg["content"]:
                        if item.get("type") == "tool_use":
                            tool_use_to_name[item.get("id")] = item.get("name")
            
            # è®¾ç½®æ˜ å°„ä¾›_build_function_responseä½¿ç”¨
            self._tool_use_mapping = tool_use_to_name
            
            gemini_contents = []
            for msg in data["messages"]:
                anthropic_role = msg.get("role", "assistant")
                # ä¼ é€’ role å‚æ•°ï¼Œä»¥ä¾¿å¯¹ user æ¶ˆæ¯åšåè®®æ ‡ç­¾è¿‡æ»¤
                parts_converted = self._convert_content_to_gemini(
                    msg.get("content", ""),
                    role=anthropic_role
                )

                # -------- ä¿®æ­£è§’è‰²æ˜ å°„ï¼štool æ¶ˆæ¯ä¿æŒä¸º tool --------------
                if anthropic_role == "user":
                    role = "user"
                elif anthropic_role == "assistant":
                    role = "model"
                elif anthropic_role == "tool":
                    role = "tool"
                else:
                    role = "model"

                # å¦‚æœå†…å®¹åŒ…å« functionResponseï¼Œå¼ºåˆ¶è®¾ä¸º tool è§’è‰²ï¼Œé¿å…ç”¨æˆ·ç«¯ role å†™é”™
                if any("functionResponse" in p for p in parts_converted):
                    role = "tool"
                
                # ç¡®ä¿ tool è§’è‰²çš„æ¶ˆæ¯è‡³å°‘æœ‰ä¸€ä¸ªæœ‰æ•ˆçš„ partï¼Œé¿å… Gemini 500 é”™è¯¯
                if role == "tool" and not parts_converted:
                    parts_converted = [{"text": ""}]
                elif role == "tool" and all(not p for p in parts_converted):
                    parts_converted = [{"text": ""}]
                
                gemini_contents.append({
                    "role": role,
                    "parts": parts_converted
                })
            result_data["contents"] = gemini_contents
        
        # å¤„ç†ç”Ÿæˆé…ç½®
        generation_config = {}
        if "temperature" in data:
            generation_config["temperature"] = data["temperature"]
        if "top_p" in data:
            generation_config["topP"] = data["top_p"]
        if "top_k" in data:
            generation_config["topK"] = data["top_k"]
        if "max_tokens" in data:
            generation_config["maxOutputTokens"] = data["max_tokens"]
        if "stop_sequences" in data:
            generation_config["stopSequences"] = data["stop_sequences"]
        
        # å¤„ç†æ€è€ƒé¢„ç®—è½¬æ¢ (Anthropic thinkingBudget -> Gemini thinkingBudget)
        if "thinking" in data and data["thinking"].get("type") == "enabled":
            budget_tokens = data["thinking"].get("budget_tokens")
            if budget_tokens:
                generation_config["thinkingConfig"] = {
                    "thinkingBudget": budget_tokens
                }
                self.logger.info(f"Anthropic thinkingBudget {budget_tokens} -> Gemini thinkingBudget {budget_tokens}")
            elif "thinking" in data:
                # å¦‚æœæ²¡æœ‰è®¾ç½®budget_tokensï¼Œå¯¹åº”Geminiçš„-1ï¼ˆåŠ¨æ€æ€è€ƒï¼‰
                generation_config["thinkingConfig"] = {
                    "thinkingBudget": -1
                }
                self.logger.info("Anthropic thinking enabled without budget -> Gemini thinkingBudget -1 (dynamic)")
        
        # ç¡®ä¿ generationConfig æ°¸è¿œå­˜åœ¨ï¼Œé¿å… Gemini 2.0+ çš„ 500 é”™è¯¯
        result_data["generationConfig"] = generation_config or {}
        
        # å¤„ç†å·¥å…·è°ƒç”¨
        if "tools" in data:
            function_declarations = []
            for tool in data["tools"]:
                function_declarations.append({
                    "name": tool.get("name", ""),
                    "description": tool.get("description", ""),
                    "parameters": self._clean_json_schema_properties(tool.get("input_schema", {}))
                })
            
            if function_declarations:
                # Geminiå®˜æ–¹è§„èŒƒä½¿ç”¨ camelCase: functionDeclarations
                result_data["tools"] = [{"functionDeclarations": function_declarations}]
        
        # åº”ç”¨æ·±åº¦æ¸…ç†ï¼Œç§»é™¤å¯èƒ½å¯¼è‡´åè®®é”™è¯¯çš„å­—æ®µ
        cleaned_result_data = self._deep_clean_for_gemini(result_data)
        
        return ConversionResult(success=True, data=cleaned_result_data)
    
    def _convert_from_openai_response(self, data: Dict[str, Any]) -> ConversionResult:
        """è½¬æ¢OpenAIå“åº”åˆ°Anthropicæ ¼å¼ï¼ˆåŸºäºç»Ÿä¸€ä¸­é—´å±‚ï¼‰"""
        # å¿…é¡»æœ‰åŸå§‹æ¨¡å‹åç§°
        if not self.original_model:
            raise ValueError("Original model name is required for response conversion")

        # 1) é€šè¿‡ç»Ÿä¸€ä¸­é—´å±‚è§£æ OpenAI å“åº”
        unified_response = UnifiedChatResponse.from_openai(data, self.original_model)

        # 2) å…¼å®¹æ—§å®ç°ï¼šä»æ–‡æœ¬å†…å®¹ä¸­è§£æ <thinking> æ ‡ç­¾
        normalized_content: List[UnifiedContent] = []
        for c in unified_response.content:
            if c.type == UnifiedContentType.TEXT and isinstance(c.text, str):
                extracted = self._extract_thinking_from_openai_text(c.text)
                if isinstance(extracted, list):
                    for block in extracted:
                        block_type = block.get("type")
                        if block_type == "thinking":
                            normalized_content.append(
                                UnifiedContent(
                                    type=UnifiedContentType.THINKING,
                                    text=block.get("thinking", ""),
                                )
                            )
                        else:
                            normalized_content.append(
                                UnifiedContent(
                                    type=UnifiedContentType.TEXT,
                                    text=block.get("text", ""),
                                )
                            )
                else:
                    c.text = extracted
                    normalized_content.append(c)
            else:
                normalized_content.append(c)
        unified_response.content = normalized_content

        # 3) æ”¯æŒ annotations é€ä¼ 
        annotations = self._extract_annotations_from_openai(data)
        if annotations:
            for c in unified_response.content:
                if c.type == UnifiedContentType.TEXT:
                    c.annotations = annotations

        # 4) è½¬æ¢ä¸º Anthropic æ ¼å¼
        result_data = unified_response.to_anthropic()

        # 5) ä¿æŒå‘åå…¼å®¹ï¼šid / stop_reason / usage
        result_data["id"] = data.get("id") or result_data.get("id") or "msg_openai"

        finish_reason = "stop"
        if data.get("choices") and data["choices"][0]:
            finish_reason = data["choices"][0].get("finish_reason", "stop")
        result_data["stop_reason"] = self._map_finish_reason(finish_reason, "openai", "anthropic")

        if "usage" not in result_data or not result_data["usage"]:
            if data.get("usage"):
                usage = data["usage"]
                result_data["usage"] = {
                    "input_tokens": usage.get("prompt_tokens", 0),
                    "output_tokens": usage.get("completion_tokens", 0),
                }
            else:
                result_data["usage"] = {}

        return ConversionResult(success=True, data=result_data)

    def _extract_annotations_from_openai(self, data: Dict[str, Any]) -> Optional[List[Dict[str, Any]]]:
        """ä» OpenAI å“åº”ä¸­æå– annotations æ•°ç»„"""
        try:
            choices = data.get("choices") or []
            if not choices:
                return data.get("annotations")
            first_choice = choices[0]
            if not isinstance(first_choice, dict):
                return None
            message = first_choice.get("message", {})
            return (
                message.get("annotations")
                or first_choice.get("annotations")
                or data.get("annotations")
            )
        except Exception:
            return None
    
    def _extract_thinking_from_openai_text(self, text: str) -> Any:
        """ä»OpenAIæ–‡æœ¬ä¸­æå–thinkingå†…å®¹ï¼Œè¿”å›Anthropicæ ¼å¼çš„content blocks"""
        import re
        
        # åŒ¹é… <thinking>...</thinking> æ ‡ç­¾
        thinking_pattern = r'<thinking>\s*(.*?)\s*</thinking>'
        matches = re.finditer(thinking_pattern, text, re.DOTALL)
        
        content_blocks = []
        last_end = 0
        
        for match in matches:
            # æ·»åŠ thinkingæ ‡ç­¾ä¹‹å‰çš„æ–‡æœ¬ï¼ˆå¦‚æœæœ‰ï¼‰
            before_text = text[last_end:match.start()].strip()
            if before_text:
                content_blocks.append({
                    "type": "text",
                    "text": before_text
                })
            
            # æ·»åŠ thinkingå†…å®¹
            thinking_text = match.group(1).strip()
            if thinking_text:
                content_blocks.append({
                    "type": "thinking",
                    "thinking": thinking_text
                })
            
            last_end = match.end()
        
        # æ·»åŠ æœ€åä¸€ä¸ªthinkingæ ‡ç­¾ä¹‹åçš„æ–‡æœ¬ï¼ˆå¦‚æœæœ‰ï¼‰
        after_text = text[last_end:].strip()
        if after_text:
            content_blocks.append({
                "type": "text",
                "text": after_text
            })
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°thinkingæ ‡ç­¾ï¼Œè¿”å›åŸæ–‡æœ¬
        if not content_blocks:
            return text
        
        # å¦‚æœåªæœ‰ä¸€ä¸ªæ–‡æœ¬å—ï¼Œè¿”å›å­—ç¬¦ä¸²
        if len(content_blocks) == 1 and content_blocks[0].get("type") == "text":
            return content_blocks[0]["text"]
        
        return content_blocks
    
    def _convert_from_gemini_response(self, data: Dict[str, Any]) -> ConversionResult:
        """è½¬æ¢Geminiå“åº”åˆ°Anthropicæ ¼å¼"""
        # å¿…é¡»æœ‰åŸå§‹æ¨¡å‹åç§°ï¼Œå¦åˆ™æŠ¥é”™
        if not self.original_model:
            raise ValueError("Original model name is required for response conversion")
            
        result_data = {
            "id": f"msg_gemini_{hash(str(data))}",
            "type": "message",
            "role": "assistant",
            "content": [],
            "model": self.original_model,  # ä½¿ç”¨åŸå§‹æ¨¡å‹åç§°
            "stop_reason": "end_turn",
            "usage": {}
        }
        
        # å¤„ç†å€™é€‰ç»“æœ
        if "candidates" in data and data["candidates"] and data["candidates"][0]:
            candidate = data["candidates"][0]
            content_list = []
            if "content" in candidate and "parts" in candidate["content"]:
                for part in candidate["content"]["parts"]:
                    # æ™®é€šæ–‡æœ¬
                    if "text" in part:
                        content_list.append({
                            "type": "text",
                            "text": part["text"]
                        })
                    # å‡½æ•°è°ƒç”¨
                    elif "functionCall" in part:
                        fc = part["functionCall"]
                        content_list.append({
                            "type": "tool_use",
                            "id": f"call_{fc.get('name','tool')}_{abs(hash(str(fc)))}",
                            "name": fc.get("name", ""),
                            "input": fc.get("args", {})
                        })
            if content_list:
                result_data["content"] = content_list

            # æ ¹æ®æ˜¯å¦å­˜åœ¨ functionCall åˆ¤æ–­ stop_reason
            finish_reason = candidate.get("finishReason", "STOP")
            if content_list and any(c.get("type") == "tool_use" for c in content_list):
                result_data["stop_reason"] = "tool_use"
            else:
                result_data["stop_reason"] = self._map_finish_reason(finish_reason, "gemini", "anthropic")
        
        # å¤„ç†ä½¿ç”¨æƒ…å†µ
        if "usageMetadata" in data:
            usage = data["usageMetadata"]
            result_data["usage"] = {
                "input_tokens": usage.get("promptTokenCount", 0),
                "output_tokens": usage.get("candidatesTokenCount", 0)
            }
        
        return ConversionResult(success=True, data=result_data)
    
    def _convert_from_openai_streaming_chunk(self, data: Dict[str, Any]) -> ConversionResult:
        """è½¬æ¢OpenAIæµå¼å“åº”chunkåˆ°Anthropic SSEæ ¼å¼ï¼ˆåŸºäºStreamStateï¼‰"""
        import json
        import time

        # 1) æ ¡éªŒåŸå§‹æ¨¡å‹å
        if not self.original_model:
            raise ValueError("Original model name is required for streaming response conversion")

        # 2) åˆå§‹åŒ– StreamState
        if not hasattr(self, "_openai_stream_state") or getattr(self, "_force_reset", False):
            for attr in ["_gemini_sent_start", "_gemini_stream_id", "_gemini_text_started", "_force_reset"]:
                if hasattr(self, attr):
                    delattr(self, attr)
            self._openai_stream_state = StreamState(
                model=self.original_model,
                original_model=self.original_model,
            )

        state: StreamState = self._openai_stream_state

        # 3) è§£æ OpenAI chunk
        choices = data.get("choices") or []
        choice = choices[0] if choices else None
        if not choice:
            return ConversionResult(success=True, data="")

        delta = choice.get("delta", {}) or {}
        content = delta.get("content") or ""
        tool_calls = delta.get("tool_calls") or []
        reasoning_content = delta.get("reasoning_content") or ""
        finish_reason = choice.get("finish_reason")

        events: List[str] = []

        # 4) å‘é€ message_startï¼ˆä»…ä¸€æ¬¡ï¼‰
        if not state.sent_message_start and not state.sent_message_stop:
            has_meaningful_data = (
                delta.get("role")
                or content
                or reasoning_content
                or tool_calls
                or choice.get("index") is not None
            )
            if has_meaningful_data:
                state.sent_message_start = True
                state.phase = StreamPhase.MESSAGE_STARTED
                model_name = state.model or self.original_model or "unknown"
                message_start = {
                    "type": "message_start",
                    "message": {
                        "id": state.stream_id,
                        "type": "message",
                        "role": "assistant",
                        "content": [],
                        "model": model_name,
                        "stop_reason": None,
                        "stop_sequence": None,
                        "usage": {"input_tokens": 0, "output_tokens": 0},
                    },
                }
                events.append(f"event: message_start\ndata: {json.dumps(message_start, ensure_ascii=False)}\n\n")

        # 5) å¤„ç† reasoning_content â†’ thinking å—ï¼ˆOpenAI o1/o3ï¼‰
        if reasoning_content and not state.sent_message_stop:
            is_new_thinking = not state.thinking_block_started
            thinking_index = state.start_thinking_block()

            if is_new_thinking:
                content_block_start = {
                    "type": "content_block_start",
                    "index": thinking_index,
                    "content_block": {"type": "thinking", "thinking": ""},
                }
                events.append(f"event: content_block_start\ndata: {json.dumps(content_block_start, ensure_ascii=False)}\n\n")

            state.thinking_buffer += reasoning_content
            thinking_delta = {
                "type": "content_block_delta",
                "index": thinking_index,
                "delta": {"type": "thinking_delta", "thinking": reasoning_content},
            }
            events.append(f"event: content_block_delta\ndata: {json.dumps(thinking_delta, ensure_ascii=False)}\n\n")

            if state.phase in (StreamPhase.NOT_STARTED, StreamPhase.MESSAGE_STARTED):
                state.phase = StreamPhase.CONTENT_STREAMING

        # 6) å¤„ç†æ™®é€šæ–‡æœ¬ content
        if content and not state.sent_message_stop:
            is_new_text = not state.text_block_started
            text_index = state.start_text_block()

            if is_new_text:
                content_block_start = {
                    "type": "content_block_start",
                    "index": text_index,
                    "content_block": {"type": "text", "text": ""},
                }
                events.append(f"event: content_block_start\ndata: {json.dumps(content_block_start, ensure_ascii=False)}\n\n")

            content_delta = {
                "type": "content_block_delta",
                "index": text_index,
                "delta": {"type": "text_delta", "text": content},
            }
            events.append(f"event: content_block_delta\ndata: {json.dumps(content_delta, ensure_ascii=False)}\n\n")

            if state.phase in (StreamPhase.NOT_STARTED, StreamPhase.MESSAGE_STARTED):
                state.phase = StreamPhase.CONTENT_STREAMING

        # 7) å¤„ç†å·¥å…·è°ƒç”¨ tool_calls â†’ tool_use
        if tool_calls and not state.sent_message_stop:
            state.phase = StreamPhase.TOOL_STREAMING
            processed_in_this_chunk: set = set()

            for tool_call in tool_calls:
                if not tool_call:
                    continue

                tool_call_index = tool_call.get("index", 0)
                if tool_call_index in processed_in_this_chunk:
                    continue
                processed_in_this_chunk.add(tool_call_index)

                func = tool_call.get("function", {}) or {}
                tool_state = state.get_tool_call(tool_call_index)

                if not tool_state:
                    tool_call_id = tool_call.get("id", f"call_{int(time.time())}_{tool_call_index}")
                    tool_call_name = func.get("name", f"tool_{tool_call_index}")
                    tool_content_index = state.start_tool_call(tool_call_index, tool_call_id, tool_call_name)

                    content_block_start = {
                        "type": "content_block_start",
                        "index": tool_content_index,
                        "content_block": {
                            "type": "tool_use",
                            "id": tool_call_id,
                            "name": tool_call_name,
                            "input": {},
                        },
                    }
                    events.append(f"event: content_block_start\ndata: {json.dumps(content_block_start, ensure_ascii=False)}\n\n")
                else:
                    tool_content_index = tool_state.content_block_index

                arguments_fragment = func.get("arguments")
                if arguments_fragment:
                    state.append_tool_arguments(tool_call_index, arguments_fragment)
                    cleaned_fragment = self._clean_json_fragment(arguments_fragment)

                    if cleaned_fragment:
                        input_json_delta = {
                            "type": "content_block_delta",
                            "index": tool_content_index,
                            "delta": {"type": "input_json_delta", "partial_json": cleaned_fragment},
                        }
                        events.append(f"event: content_block_delta\ndata: {json.dumps(input_json_delta, ensure_ascii=False)}\n\n")

        # 8) å¤„ç†æµç»“æŸ
        if finish_reason and not state.sent_message_stop and state.sent_message_start:
            state.phase = StreamPhase.FINISHED

            # æŒ‰é¡ºåºå…³é—­æ‰€æœ‰å·²å¼€å¯çš„ content blocks
            for index in state.get_all_content_block_indices():
                content_block_stop = {"type": "content_block_stop", "index": index}
                events.append(f"event: content_block_stop\ndata: {json.dumps(content_block_stop, ensure_ascii=False)}\n\n")

            anthropic_stop_reason = self._map_finish_reason(finish_reason, "openai", "anthropic")

            message_delta: Dict[str, Any] = {
                "type": "message_delta",
                "delta": {"stop_reason": anthropic_stop_reason, "stop_sequence": None},
            }

            if data.get("usage") is not None:
                usage = data["usage"]
                state.accumulate_usage(usage.get("prompt_tokens", 0), usage.get("completion_tokens", 0))
            message_delta["usage"] = {
                "input_tokens": state.input_tokens or 0,
                "output_tokens": state.output_tokens or 0,
            }

            events.append(f"event: message_delta\ndata: {json.dumps(message_delta, ensure_ascii=False)}\n\n")
            events.append(f"event: message_stop\ndata: {{\"type\": \"message_stop\"}}\n\n")

            state.sent_message_stop = True

        # 9) æµç»“æŸåæ¸…ç†çŠ¶æ€
        if finish_reason and hasattr(self, "_openai_stream_state"):
            delattr(self, "_openai_stream_state")

        # 10) è¿”å› SSE ä¸²
        if not events:
            self.logger.debug(
                f"No events generated for chunk - content: {bool(content)}, "
                f"reasoning: {bool(reasoning_content)}, tool_calls: {bool(tool_calls)}, "
                f"sent_message_start: {state.sent_message_start}"
            )
            return ConversionResult(success=True, data="")

        result_data = "".join(events)
        self.logger.debug(f"Generated {len(events)} events, total data length: {len(result_data)}")
        return ConversionResult(success=True, data=result_data)
    
    def _clean_json_fragment(self, fragment: str) -> str:
        """æ¸…ç†JSONç‰‡æ®µï¼Œé¿å…ä¸å®Œæ•´çš„Unicodeå­—ç¬¦æˆ–è½¬ä¹‰åºåˆ—"""
        if not fragment:
            return fragment
        
        try:
            # ç§»é™¤å¼€å¤´å’Œç»“å°¾å¯èƒ½ä¸å®Œæ•´çš„Unicodeå­—ç¬¦
            # æ£€æŸ¥æœ€åå‡ ä¸ªå­—ç¬¦æ˜¯å¦æ˜¯ä¸å®Œæ•´çš„è½¬ä¹‰åºåˆ—
            cleaned = fragment
            
            # å¤„ç†å¯èƒ½è¢«æˆªæ–­çš„è½¬ä¹‰åºåˆ—
            if cleaned.endswith('\\') and not cleaned.endswith('\\\\'):
                cleaned = cleaned[:-1]  # ç§»é™¤æ‚¬æŒ‚çš„åæ–œæ 
            elif cleaned.endswith('\\u') or cleaned.endswith('\\u0') or cleaned.endswith('\\u00'):
                # ä¸å®Œæ•´çš„Unicodeè½¬ä¹‰åºåˆ—
                idx = cleaned.rfind('\\u')
                cleaned = cleaned[:idx]
            
            # éªŒè¯æ¸…ç†åçš„ç‰‡æ®µä¸ä¼šå¯¼è‡´JSONè§£æé”™è¯¯
            if cleaned:
                # ç®€å•æµ‹è¯•ï¼šå¦‚æœç‰‡æ®µåŒ…å«å¼•å·ï¼Œç¡®ä¿å®ƒä»¬æ˜¯å¹³è¡¡çš„
                quote_count = cleaned.count('"') - cleaned.count('\\"')
                if quote_count % 2 == 1:
                    # å¦‚æœå¼•å·æ•°é‡ä¸ºå¥‡æ•°ï¼Œå¯èƒ½åœ¨å­—ç¬¦ä¸²ä¸­é—´è¢«æˆªæ–­
                    pass  # ä»ç„¶å‘é€ï¼Œè®©æ¥æ”¶ç«¯å¤„ç†
            
            return cleaned
            
        except Exception as e:
            self.logger.warning(f"Error cleaning JSON fragment: {e}, returning original")
            return fragment
    
    
    def _convert_from_gemini_streaming_chunk(self, data: Dict[str, Any]) -> ConversionResult:
        """å°† Gemini æµå¼ chunk è½¬ä¸º Anthropic SSE æ ¼å¼ - ç®€åŒ–ç‰ˆæœ¬"""
        import json, random, time
        
        self.logger.debug(f"Converting Gemini chunk: {str(data)[:200]}...")
        
        # æ£€æŸ¥å½“å‰çŠ¶æ€
        current_state = {
            '_gemini_stream_id': hasattr(self, '_gemini_stream_id'),
            '_gemini_sent_start': hasattr(self, '_gemini_sent_start'),
            '_gemini_text_started': hasattr(self, '_gemini_text_started'),
            '_streaming_state': hasattr(self, '_streaming_state'),
            '_force_reset': getattr(self, '_force_reset', False)
        }
        self.logger.debug(f"Current state before processing: {current_state}")
        
        # æ¯æ¬¡å¼€å§‹æ–°çš„æµå¼è½¬æ¢æ—¶ï¼Œé‡ç½®æ‰€æœ‰ç›¸å…³çŠ¶æ€å˜é‡ï¼Œé¿å…çŠ¶æ€æ±¡æŸ“
        if not hasattr(self, '_gemini_stream_id') or getattr(self, '_force_reset', False):
            self.logger.debug("Initializing new Gemini stream")
            # æ¸…ç†å¯èƒ½æ®‹ç•™çš„çŠ¶æ€
            for attr in ['_gemini_sent_start', '_gemini_text_started', '_streaming_state', '_force_reset']:
                if hasattr(self, attr):
                    delattr(self, attr)
            # ç”Ÿæˆæ–°çš„æµID
            self._gemini_stream_id = f"msg_{random.randint(100000, 999999)}"
            self.logger.debug(f"Generated stream ID: {self._gemini_stream_id}")

        # ä¿å­˜æ¨¡å‹åï¼ˆå¿…é¡»å·²åœ¨ set_original_model è®¾ç½®ï¼‰
        if not self.original_model:
            raise ValueError("Original model name is required for streaming response conversion")

        # æå–æœ¬æ¬¡ chunk çš„ candidateã€å†…å®¹ã€ç»“æŸæ ‡è®°
        candidate = None
        if data.get("candidates") and data["candidates"][0]:
            candidate = data["candidates"][0]

        content = ""
        function_calls = []
        if candidate and candidate.get("content") and candidate["content"].get("parts"):
            for part in candidate["content"]["parts"]:
                if "text" in part:
                    content += part["text"]
                elif "functionCall" in part:
                    # å¤„ç†å‡½æ•°è°ƒç”¨
                    func_call = part["functionCall"]
                    function_calls.append({
                        "name": func_call.get("name", ""),
                        "args": func_call.get("args", {})
                    })

        is_end = bool(candidate and candidate.get("finishReason"))

        events: list[str] = []  # ä¿å­˜ SSE è¡Œ

        # ç¬¬ä¸€æ¬¡è¿›å…¥ï¼šå‘é€ message_start
        if not hasattr(self, '_gemini_sent_start'):
            import logging
            unified_logger = logging.getLogger("unified_api")
            unified_logger.debug("ANTHROPIC_CONVERTER: Sending message_start for new Gemini stream")
            self.logger.debug("Sending message_start for new Gemini stream")
            self._gemini_sent_start = True

            # é˜²å¾¡æ€§æ£€æŸ¥ï¼šç¡®ä¿modelå’Œroleå±æ€§å§‹ç»ˆæœ‰æ•ˆ
            model_name = self.original_model or 'unknown'
            message_start = {
                "type": "message_start",
                "message": {
                    "id": self._gemini_stream_id,
                    "type": "message",
                    "role": "assistant",  # å§‹ç»ˆç¡®ä¿roleå±æ€§å­˜åœ¨
                    "content": [],
                    "model": model_name,  # ä½¿ç”¨é˜²å¾¡æ€§æ£€æŸ¥åçš„æ¨¡å‹åç§°
                    "stop_reason": None,
                    "stop_sequence": None,
                    "usage": {"input_tokens": 0, "output_tokens": 0},
                },
            }
            events += [
                "event: message_start",
                f"data: {json.dumps(message_start, ensure_ascii=False)}",
                "",
            ]

        # å¤„ç†æ–‡æœ¬å†…å®¹
        if content:
            # å¦‚æœè¿˜æ²¡æœ‰å‘é€è¿‡æ–‡æœ¬ content_block_startï¼Œå…ˆå‘é€
            if not hasattr(self, '_gemini_text_started'):
                self._gemini_text_started = True
                content_block_start = {
                    "type": "content_block_start",
                    "index": 0,
                    "content_block": {"type": "text", "text": ""},
                }
                events += [
                    "event: content_block_start",
                    f"data: {json.dumps(content_block_start, ensure_ascii=False)}",
                    "",
                ]

            # å‘é€æ–‡æœ¬å¢é‡
            content_block_delta = {
                "type": "content_block_delta",
                "index": 0,
                "delta": {"type": "text_delta", "text": content},
            }
            events += [
                "event: content_block_delta",
                f"data: {json.dumps(content_block_delta, ensure_ascii=False)}",
                "",
            ]

        # å¤„ç†å‡½æ•°è°ƒç”¨
        if function_calls:
            for i, func_call in enumerate(function_calls):
                # ç¡®å®šç´¢å¼•ï¼šå¦‚æœæœ‰æ–‡æœ¬å†…å®¹ï¼Œå·¥å…·è°ƒç”¨ä»ç´¢å¼•1å¼€å§‹ï¼›å¦åˆ™ä»ç´¢å¼•0å¼€å§‹
                # ç”±äºæˆ‘ä»¬ä¸Šé¢å¯èƒ½æ·»åŠ äº†è§£é‡Šæ–‡æœ¬ï¼Œæ‰€ä»¥_gemini_text_startedåº”è¯¥å·²ç»è®¾ç½®
                tool_index = 1 if hasattr(self, '_gemini_text_started') else 0
                # å¦‚æœæœ‰å¤šä¸ªå·¥å…·è°ƒç”¨ï¼Œåç»­å·¥å…·çš„ç´¢å¼•éœ€è¦é€’å¢
                tool_index += i
                
                # å‘é€ tool_use content_block_start
                tool_block_start = {
                    "type": "content_block_start",
                    "index": tool_index,
                    "content_block": {
                        "type": "tool_use",
                        "id": f"toolu_{random.randint(100000, 999999)}",
                        "name": func_call["name"],
                        "input": {}
                    }
                }
                events += [
                    "event: content_block_start",
                    f"data: {json.dumps(tool_block_start, ensure_ascii=False)}",
                    "",
                ]

                # å‘é€å·¥å…·è°ƒç”¨å‚æ•°
                if func_call["args"]:
                    tool_delta = {
                        "type": "content_block_delta",
                        "index": tool_index,
                        "delta": {
                            "type": "input_json_delta",
                            "partial_json": json.dumps(func_call["args"], ensure_ascii=False)
                        }
                    }
                    events += [
                        "event: content_block_delta",
                        f"data: {json.dumps(tool_delta, ensure_ascii=False)}",
                        "",
                    ]

                # å‘é€ content_block_stop
                tool_block_stop = {"type": "content_block_stop", "index": tool_index}
                events += [
                    "event: content_block_stop",
                    f"data: {json.dumps(tool_block_stop, ensure_ascii=False)}",
                    "",
                ]

        # å¦‚æœæœ¬ chunk æºå¸¦ finishReasonï¼Œè¯´æ˜å¯¹è¯ç»“æŸï¼Œè¡¥å……æ”¶å°¾äº‹ä»¶
        if is_end:
            self.logger.debug(f"Stream ending with finishReason: {candidate.get('finishReason') if candidate else 'None'}")
            # å¦‚æœæœ‰æ–‡æœ¬å†…å®¹å—è¿˜æœªç»“æŸï¼Œå‘é€ content_block_stop
            if hasattr(self, '_gemini_text_started'):
                content_block_stop = {"type": "content_block_stop", "index": 0}
                events += [
                    "event: content_block_stop",
                    f"data: {json.dumps(content_block_stop, ensure_ascii=False)}",
                    "",
                ]

            # message_deltaï¼ˆåŒ…å« stop_reason ä¸ usageï¼‰
            # å¯¹äºGeminiå·¥å…·è°ƒç”¨çš„ç‰¹æ®Šå¤„ç†ï¼š
            # - å¦‚æœæ£€æµ‹åˆ°å‡½æ•°è°ƒç”¨ï¼Œstop_reasonåº”è¯¥æ˜¯tool_useï¼ˆæ— è®ºGeminiçš„finishReasonæ˜¯ä»€ä¹ˆï¼‰
            # - å¦‚æœæ²¡æœ‰å‡½æ•°è°ƒç”¨ï¼Œä½¿ç”¨æ­£å¸¸çš„finish_reasonæ˜ å°„
            if function_calls:
                stop_reason = "tool_use"
                self.logger.info(f"Setting stop_reason to 'tool_use' due to detected function calls: {[fc.get('name') for fc in function_calls]}")
            else:
                stop_reason = self._map_finish_reason(candidate.get("finishReason", ""), "gemini", "anthropic")
                self.logger.debug(f"Mapped finish_reason '{candidate.get('finishReason', '')}' to '{stop_reason}'")
            
            message_delta = {
                "type": "message_delta",
                "delta": {
                    "stop_reason": stop_reason,
                    "stop_sequence": None,
                },
            }
            
            # æ€»æ˜¯æä¾› usage ä¿¡æ¯ï¼Œå³ä½¿ Gemini æ²¡æœ‰ usageMetadata
            if data.get("usageMetadata"):
                usage = data["usageMetadata"]
                message_delta["usage"] = {
                    "input_tokens": usage.get("promptTokenCount", 0),
                    "output_tokens": usage.get("candidatesTokenCount", 0)
                }
            else:
                # å¦‚æœæ²¡æœ‰ usage ä¿¡æ¯ï¼Œæä¾›é»˜è®¤å€¼ä»¥é¿å…å‰ç«¯é”™è¯¯
                message_delta["usage"] = {
                    "input_tokens": 0,
                    "output_tokens": 0
                }

            events += [
                "event: message_delta",
                f"data: {json.dumps(message_delta, ensure_ascii=False)}",
                "",
                "event: message_stop",
                "data: {\"type\": \"message_stop\"}",
                "",
            ]
            import logging
            unified_logger = logging.getLogger("unified_api")
            unified_logger.debug("ANTHROPIC_CONVERTER: Sent message_stop for Gemini stream")
            self.logger.debug("Sent message_stop for Gemini stream")

            # ç»“æŸå½“å‰æµåæ¸…ç†çŠ¶æ€ï¼Œé¿å…å½±å“ä¸‹ä¸€æ¬¡è¯·æ±‚
            self.logger.debug("Cleaning up Gemini streaming state after stream end")
            cleaned_attrs = []
            if hasattr(self, '_gemini_sent_start'):
                cleaned_attrs.append('_gemini_sent_start')
                delattr(self, '_gemini_sent_start')
            if hasattr(self, '_gemini_stream_id'):
                cleaned_attrs.append('_gemini_stream_id')
                delattr(self, '_gemini_stream_id')
            if hasattr(self, '_gemini_text_started'):
                cleaned_attrs.append('_gemini_text_started')
                delattr(self, '_gemini_text_started')
            self.logger.debug(f"Cleaned up attributes after stream end: {cleaned_attrs}")

        # è‹¥æ²¡æœ‰ä»»ä½•äº‹ä»¶éœ€è¦å‘é€ï¼Œåˆ™è¿”å›ç©ºå­—ç¬¦ä¸²ï¼ˆä¸Šå±‚ä¼šå¿½ç•¥ï¼‰
        if not events:
            return ConversionResult(success=True, data="")

        # å°†äº‹ä»¶æŒ‰ "\n\n" åˆ†ç»„ï¼Œæ¯ä¸ªå®Œæ•´äº‹ä»¶ä½œä¸ºåˆ—è¡¨çš„ä¸€ä¸ªå…ƒç´ 
        complete_events = []
        i = 0
        while i < len(events):
            if events[i].startswith("event:") or events[i].startswith("data:"):
                # æ‰¾åˆ°ä¸€ä¸ªå®Œæ•´äº‹ä»¶çš„ç»“æŸï¼ˆä¸‹ä¸€ä¸ªç©ºè¡Œï¼‰
                event_lines = []
                while i < len(events) and events[i] != "":
                    event_lines.append(events[i])
                    i += 1
                # æ·»åŠ ç»“æŸçš„ç©ºè¡Œ
                if i < len(events) and events[i] == "":
                    event_lines.append("")
                    i += 1
                # å°†å®Œæ•´äº‹ä»¶æ‹¼æ¥æˆå­—ç¬¦ä¸²
                complete_events.append("\n".join(event_lines) + "\n")
            else:
                i += 1

        self.logger.debug(f"Successfully converted Gemini chunk to {len(complete_events)} events")
        return ConversionResult(success=True, data=complete_events)
        
    
    def _parse_anthropic_sse_event(self, sse_data: str) -> ConversionResult:
        """è§£æAnthropic SSEäº‹ä»¶æ•°æ®ï¼Œæå–äº‹ä»¶ç±»å‹å’Œæ•°æ®
        
        """
        import re
        import json
        
        # ä½¿ç”¨ä¸claude-to-chatgpté¡¹ç›®ç›¸åŒçš„æ­£åˆ™è¡¨è¾¾å¼
        # /event:\s*.*?\s*\ndata:\s*(.*?)(?=\n\n|\s*$)/gs
        pattern = r'event:\s*([^\n]*)\s*\ndata:\s*([^\n]*)'
        matches = re.findall(pattern, sse_data)
        
        parsed_events = []
        for event_type, data_content in matches:
            event_type = event_type.strip()
            data_content = data_content.strip()
            
            # å°è¯•è§£æJSONæ•°æ®
            try:
                if data_content:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ç»“æŸæ ‡è®°
                    if data_content.strip() == "[DONE]":
                        break
                    parsed_data = json.loads(data_content)
                    parsed_events.append({
                        'event': event_type,
                        'data': parsed_data
                    })
            except json.JSONDecodeError:
                # å¦‚æœä¸æ˜¯JSONï¼Œè·³è¿‡æˆ–è®°å½•è­¦å‘Š
                self.logger.warning(f"Failed to parse SSE data as JSON: {data_content}")
                continue
        
        return ConversionResult(success=True, data=parsed_events)
    
    def _convert_content_from_anthropic(self, content: Any) -> Any:
        """è½¬æ¢Anthropicå†…å®¹åˆ°é€šç”¨æ ¼å¼"""
        if isinstance(content, str):
            return content
        elif isinstance(content, list):
            # å¤„ç†å¤šæ¨¡æ€å†…å®¹
            converted_content = []
            for item in content:
                if item.get("type") == "text":
                    converted_content.append({
                        "type": "text",
                        "text": item.get("text", "")
                    })
                elif item.get("type") == "image":
                    # è½¬æ¢å›¾åƒæ ¼å¼
                    source = item.get("source", {})
                    if source.get("type") == "base64":
                        media_type = source.get("media_type", "image/jpeg")
                        data_part = source.get("data", "")
                        converted_content.append({
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:{media_type};base64,{data_part}"
                            }
                        })
            return converted_content if len(converted_content) > 1 else converted_content[0].get("text", "") if converted_content else ""
        return content
    
    def _convert_content_to_gemini(
        self, content: Any, role: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """å°† Anthropic çš„ content è½¬ä¸º Gemini parts ç»“æ„ã€‚

        Args:
            content: Anthropic æ ¼å¼çš„æ¶ˆæ¯å†…å®¹
            role: æ¶ˆæ¯è§’è‰²ï¼Œå½“ä¸º "user" æ—¶ä¼šå¯¹æ–‡æœ¬æ‰§è¡Œåè®®æ ‡ç­¾è¿‡æ»¤

        Returns:
            Gemini parts åˆ—è¡¨
        """
        is_user = role == "user"

        # 1. çº¯æ–‡æœ¬
        if isinstance(content, str):
            text_val = self._sanitize_protocol_tags(content) if is_user else content
            return [{"text": text_val}]

        # 2. åˆ—è¡¨ï¼ˆå¯èƒ½æ··æ‚å¤šæ¨¡æ€ / tool æ¶ˆæ¯ï¼‰
        if isinstance(content, list):
            gemini_parts: List[Dict[str, Any]] = []
            for item in content:
                if not isinstance(item, dict):
                    continue

                item_type = item.get("type")

                # 2.1 æ™®é€šæ–‡æœ¬
                if item_type == "text":
                    text_content = item.get("text", "")
                    if is_user and isinstance(text_content, str):
                        text_content = self._sanitize_protocol_tags(text_content)
                    if text_content:  # åªæ·»åŠ éç©ºæ–‡æœ¬
                        gemini_parts.append({"text": text_content})

                # 2.2 å›¾åƒï¼ˆbase64ï¼‰
                elif item_type == "image":
                    source = item.get("source", {})
                    if source.get("type") == "base64":
                        gemini_parts.append({
                            "inlineData": {
                                "mimeType": source.get("media_type", "image/jpeg"),
                                "data": source.get("data", "")
                            }
                        })

                # 2.3 tool_use â†’ functionCall
                elif item_type == "tool_use":
                    tool_name = item.get("name", "")
                    tool_id = item.get("id", "")
                    
                    # å­˜å‚¨tool_idåˆ°function_nameçš„æ˜ å°„ï¼Œç”¨äºåç»­tool_resultè½¬æ¢
                    if tool_id and tool_name:
                        self._tool_id_mapping[tool_name] = tool_id
                        # åŒæ—¶å­˜å‚¨åˆ°å…¨å±€å·¥å…·çŠ¶æ€ç®¡ç†å™¨ä¸­
                        tool_state_manager.store_tool_mapping(tool_name, tool_id)
                        
                    gemini_parts.append({
                        "functionCall": {
                            "name": tool_name,
                            "args": item.get("input", {})
                        }
                    })

                # 2.4 tool_result â†’ functionResponse
                elif item_type == "tool_result":
                    fr = self._build_function_response(item)
                    if fr:
                        gemini_parts.append(fr)

            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„ partsï¼Œè¿”å›ç©ºæ–‡æœ¬è€Œä¸æ˜¯ç©ºæ•°ç»„
            if not gemini_parts:
                return [{"text": ""}]
            
            return gemini_parts

        # 3. å•ä¸ª dictï¼ˆå¯èƒ½å°±æ˜¯ tool_resultï¼‰
        if isinstance(content, dict):
            fr = self._build_function_response(content)
            if fr:
                return [fr]
            # å¦‚æœä¸æ˜¯å·¥å…·ç»“æœï¼Œè½¬ä¸ºæ–‡æœ¬
            content_text = content.get("text") or json.dumps(content, ensure_ascii=False)
            if is_user and isinstance(content_text, str):
                content_text = self._sanitize_protocol_tags(content_text)
            return [{"text": content_text}]

        # 4. å…¶å®ƒç±»å‹ç»Ÿä¸€è½¬å­—ç¬¦ä¸²
        text_val = str(content) if content else ""
        if is_user and text_val:
            text_val = self._sanitize_protocol_tags(text_val)
        return [{"text": text_val}]

    # --------- è¾…åŠ©ï¼šæ„é€  functionResponse part ---------
    def _build_function_response(self, item: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """æ ¹æ® tool_result å­—æ®µæ„é€  Gemini functionResponse"""
        if not isinstance(item, dict):
            return None

        # åˆ¤å®šæ˜¯å¦ä¸ºå·¥å…·ç»“æœ
        is_result = (
            item.get("type") == "tool_result"
            or "tool_use_id" in item
            or "tool_output" in item
            or "result" in item
            or "content" in item
        )
        if not is_result:
            return None

        # æå–å‡½æ•°å
        func_name = None
        
        # æ–¹æ³•1ï¼šä»æ˜ å°„è¡¨ä¸­è·å–ï¼ˆAnthropicæ ¼å¼ï¼‰
        tool_use_id = item.get("tool_use_id") or item.get("id")
        if tool_use_id and hasattr(self, '_tool_use_mapping'):
            func_name = self._tool_use_mapping.get(tool_use_id)
        
        # æ–¹æ³•1.5ï¼šä½¿ç”¨å…¨å±€å·¥å…·çŠ¶æ€ç®¡ç†å™¨
        if not func_name and tool_use_id:
            # å…ˆå°è¯•ä»IDä¸­æå–å¯èƒ½çš„å‡½æ•°å
            potential_func_name = None
            if str(tool_use_id).startswith("call_"):
                name_and_hash = tool_use_id[len("call_"):]
                potential_func_name = name_and_hash.rsplit("_", 1)[0]
            
            # æ£€æŸ¥å…¨å±€ç®¡ç†å™¨ä¸­æ˜¯å¦æœ‰å¯¹åº”çš„æ˜ å°„
            if potential_func_name:
                stored_id = tool_state_manager.get_tool_id(potential_func_name)
                if stored_id == tool_use_id:
                    func_name = potential_func_name
        
        # æ–¹æ³•2ï¼šä» tool_use_id ä¸­æå–ï¼ˆOpenAIæ ¼å¼ï¼‰
        if not func_name and tool_use_id and str(tool_use_id).startswith("call_"):
            # æ ¼å¼: call_<function_name>_<hash> ï¼Œå‡½æ•°åå¯èƒ½åŒ…å«å¤šä¸ªä¸‹åˆ’çº¿
            name_and_hash = tool_use_id[len("call_"):]
            func_name = name_and_hash.rsplit("_", 1)[0]  # å»æ‰æœ€åä¸€ä¸ª hash æ®µ
        
        # æ–¹æ³•3ï¼šç›´æ¥ä»å­—æ®µè·å–
        if not func_name:
            func_name = (
                item.get("tool_name")
                or item.get("name")
                or item.get("function_name")
            )

        if not func_name:
            return None

        # æå–ç»“æœå†…å®¹
        func_response = None
        
        # å°è¯•å¤šä¸ªå¯èƒ½çš„ç»“æœå­—æ®µ
        for key in ["content", "tool_output", "output", "response", "result"]:
            if key in item:
                func_response = item[key]
                break
        
        # å¦‚æœ content æ˜¯åˆ—è¡¨ï¼Œå°è¯•æå–æ–‡æœ¬
        if isinstance(func_response, list) and func_response:
            text_parts = [p.get("text", "") for p in func_response if isinstance(p, dict) and p.get("type") == "text"]
            if text_parts:
                func_response = "".join(text_parts)
        
        # ç¡®ä¿æœ‰å“åº”å†…å®¹
        if func_response is None:
            func_response = ""

        # Gemini è¦æ±‚ response ä¸º JSON å¯¹è±¡ï¼Œè‹¥ä¸ºåŸå§‹å­—ç¬¦ä¸²åˆ™åŒ…è£…
        if not isinstance(func_response, (dict, list)):
            func_response = {"content": str(func_response)}

        return {
            "functionResponse": {
                "name": func_name,
                "response": func_response
            }
        }
    
    def _map_finish_reason(self, reason: str, source_format: str, target_format: str) -> str:
        """æ˜ å°„ç»“æŸåŸå› """
        reason_mappings = {
            "openai": {
                "anthropic": {
                    "stop": "end_turn",
                    "length": "max_tokens",
                    "content_filter": "stop_sequence",
                    "tool_calls": "tool_use"
                }
            },
            "gemini": {
                "anthropic": {
                    # æ—§ç‰ˆæœ¬å¤§å†™æ ¼å¼
                    "STOP": "end_turn",
                    "MAX_TOKENS": "max_tokens",
                    "SAFETY": "stop_sequence",
                    "RECITATION": "stop_sequence",
                    # æ–°ç‰ˆæœ¬å°å†™æ ¼å¼ï¼ˆv1beta/v1 APIï¼‰
                    "stop": "end_turn",
                    "length": "max_tokens",
                    "safety": "stop_sequence",
                    "recitation": "stop_sequence",
                    "other": "end_turn"
                }
            }
        }
        
        try:
            return reason_mappings[source_format][target_format].get(reason, "end_turn")
        except KeyError:
            return "end_turn"

    # ==================== åè®®æ ‡ç­¾å®‰å…¨è¿‡æ»¤ ====================

    # é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼ï¼Œé¿å…è¿è¡Œæ—¶é‡å¤ç¼–è¯‘
    _DANGEROUS_BLOCK_PATTERN = None
    _THINKING_TAG_PATTERN = None
    _GENERIC_XML_TAG_PATTERN = None

    @classmethod
    def _get_sanitize_patterns(
        cls,
    ) -> Tuple[Pattern[str], Pattern[str], Pattern[str]]:
        """å»¶è¿Ÿåˆå§‹åŒ–æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ï¼ˆçº¿ç¨‹å®‰å…¨çš„å•ä¾‹æ¨¡å¼ï¼‰"""
        if cls._DANGEROUS_BLOCK_PATTERN is None:
            # é«˜é£é™©åè®®å—ï¼šå®Œæ•´åˆ é™¤ï¼ˆåŒ…æ‹¬å†…éƒ¨å†…å®¹ï¼‰
            cls._DANGEROUS_BLOCK_PATTERN = re.compile(
                r'<\s*(invoke|tool_result)\b[^>]*>.*?</\s*\1\s*>',
                re.IGNORECASE | re.DOTALL
            )
            # thinking æ ‡ç­¾ï¼šä¿ç•™å†…éƒ¨å†…å®¹ï¼Œä»…å»é™¤æ ‡ç­¾å£³
            cls._THINKING_TAG_PATTERN = re.compile(
                r'<\s*thinking\b[^>]*>(.*?)</\s*thinking\s*>',
                re.IGNORECASE | re.DOTALL
            )
            # é€šç”¨ XML æ ‡ç­¾ï¼šå»é™¤æ ‡ç­¾å£³
            cls._GENERIC_XML_TAG_PATTERN = re.compile(
                r'</?[a-zA-Z][a-zA-Z0-9_\-:]*[^>]*>'
            )
        return (
            cls._DANGEROUS_BLOCK_PATTERN,
            cls._THINKING_TAG_PATTERN,
            cls._GENERIC_XML_TAG_PATTERN,
        )

    def _sanitize_protocol_tags(self, text: str) -> str:
        """è¿‡æ»¤æ½œåœ¨åè®®æ ‡ç­¾ï¼Œé˜²æ­¢ç”¨æˆ·é€šè¿‡ä¼ªé€  XML æ ‡ç­¾è¿›è¡Œæç¤ºæ³¨å…¥æ”»å‡»ã€‚

        å¤„ç†ç­–ç•¥ï¼š
        1. <invoke>...</invoke>ã€<tool_result>...</tool_result>ï¼šæ•´å—åˆ é™¤
        2. <thinking>...</thinking>ï¼šä¿ç•™å†…éƒ¨è‡ªç„¶è¯­è¨€å†…å®¹ï¼Œä»…ç§»é™¤æ ‡ç­¾
        3. å…¶ä»– XML æ ·å¼æ ‡ç­¾ï¼šç§»é™¤æ ‡ç­¾å£³ï¼Œä¿ç•™å†…éƒ¨æ–‡æœ¬

        æ³¨æ„ï¼šæ­¤æ–¹æ³•ä»…ç”¨äºç”¨æˆ·/ç³»ç»Ÿä¾§è¾“å…¥æ–‡æœ¬ï¼Œä¸åº”ç”¨äºæ¨¡å‹è¾“å‡ºå†…å®¹ã€‚
        """
        if not isinstance(text, str) or not text:
            return text

        dangerous_pattern, thinking_pattern, generic_pattern = self._get_sanitize_patterns()

        # 1) ç§»é™¤é«˜é£é™©åè®®å—ï¼ˆåŒ…æ‹¬å†…éƒ¨å†…å®¹ï¼‰
        cleaned = dangerous_pattern.sub('', text)

        # 2) å»æ‰ <thinking> æ ‡ç­¾å¤–å£³ï¼Œä¿ç•™å†…éƒ¨è‡ªç„¶è¯­è¨€å†…å®¹
        cleaned = thinking_pattern.sub(r'\1', cleaned)

        # 3) ç§»é™¤å‰©ä½™çš„ XML æ ·å¼æ ‡ç­¾ï¼ˆä»…æ ‡ç­¾æœ¬èº«ï¼Œä¿ç•™å†…éƒ¨æ–‡æœ¬ï¼‰
        cleaned = generic_pattern.sub('', cleaned)

        return cleaned

    # ==================== Schema æ¸…ç†æ–¹æ³• ====================

    def _sanitize_schema(self, schema: Dict[str, Any]) -> Dict[str, Any]:
        """é€’å½’ç§»é™¤Geminiä¸æ”¯æŒçš„JSON Schemaå…³é”®å­—"""
        if not isinstance(schema, dict):
            return schema

        allowed_keys = {"type", "description", "properties", "required", "enum", "items"}
        sanitized = {k: v for k, v in schema.items() if k in allowed_keys}

        if "properties" in sanitized and isinstance(sanitized["properties"], dict):
            sanitized["properties"] = {
                prop_name: self._sanitize_schema(prop_schema)
                for prop_name, prop_schema in sanitized["properties"].items()
            }

        if "items" in sanitized:
            sanitized["items"] = self._sanitize_schema(sanitized["items"])

        return sanitized

    def _clean_json_schema_properties(self, schema: Dict[str, Any]) -> Dict[str, Any]:
        """é€’å½’æ¸…ç†Geminiä¸æ”¯æŒçš„JSON Schemaå±æ€§"""
        if not isinstance(schema, dict):
            return schema

        # ç§»é™¤æ‰€æœ‰éæ ‡å‡†å±æ€§
        sanitized = {k: v for k, v in schema.items() if k in {"type", "description", "properties", "required", "enum", "items"}}

        if "properties" in sanitized and isinstance(sanitized["properties"], dict):
            sanitized["properties"] = {
                prop_name: self._clean_json_schema_properties(prop_schema)
                for prop_name, prop_schema in sanitized["properties"].items()
            }

        if "items" in sanitized:
            sanitized["items"] = self._clean_json_schema_properties(sanitized["items"])

        return sanitized

    def _deep_clean_for_gemini(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """æ·±åº¦æ¸…ç†Geminiè¯·æ±‚æ•°æ®ï¼Œç§»é™¤å¯èƒ½å¼•èµ·åè®®é”™è¯¯çš„å­—æ®µå’Œæ ¼å¼é—®é¢˜"""
        if not isinstance(data, dict):
            return data
        
        cleaned = {}
        
        for key, value in data.items():
            # å¤„ç† system_instruction
            if key == "system_instruction" and isinstance(value, dict):
                cleaned_si = {}
                if "parts" in value and isinstance(value["parts"], list):
                    clean_parts = []
                    for part in value["parts"]:
                        if isinstance(part, dict) and "text" in part:
                            # ç¡®ä¿textå­—æ®µæ˜¯çº¯å­—ç¬¦ä¸²ï¼Œæ— ç‰¹æ®Šå­—ç¬¦æˆ–ç¼–ç é—®é¢˜
                            text_content = str(part["text"]).strip()
                            if text_content:  # åªæ·»åŠ éç©ºæ–‡æœ¬
                                clean_parts.append({"text": text_content})
                    if clean_parts:
                        cleaned_si["parts"] = clean_parts
                        cleaned[key] = cleaned_si
            
            # å¤„ç† contents
            elif key == "contents" and isinstance(value, list):
                clean_contents = []
                for content in value:
                    if isinstance(content, dict):
                        clean_content = {}
                        # ç¡®ä¿roleå­—æ®µæ­£ç¡®
                        if "role" in content:
                            clean_content["role"] = str(content["role"])
                        # æ¸…ç†parts
                        if "parts" in content and isinstance(content["parts"], list):
                            clean_parts = []
                            for part in content["parts"]:
                                if isinstance(part, dict):
                                    clean_part = {}
                                    # åªä¿ç•™æ”¯æŒçš„å­—æ®µ
                                    if "text" in part:
                                        text_val = str(part["text"]).strip() if part["text"] else ""
                                        if text_val:
                                            clean_part["text"] = text_val
                                    elif "functionCall" in part:
                                        clean_part["functionCall"] = part["functionCall"]
                                    elif "functionResponse" in part:
                                        clean_part["functionResponse"] = part["functionResponse"]
                                    elif "inlineData" in part:
                                        clean_part["inlineData"] = part["inlineData"]
                                    
                                    if clean_part:  # åªæ·»åŠ éç©ºpart
                                        clean_parts.append(clean_part)
                            
                            if clean_parts:
                                clean_content["parts"] = clean_parts
                                clean_contents.append(clean_content)
                
                if clean_contents:
                    cleaned[key] = clean_contents
            
            # å¤„ç† generationConfig
            elif key == "generationConfig" and isinstance(value, dict):
                clean_gen_config = {}
                # åªä¿ç•™Geminiæ”¯æŒçš„ç”Ÿæˆé…ç½®å­—æ®µ
                allowed_gen_keys = {"temperature", "topP", "topK", "maxOutputTokens", "stopSequences", "thinkingConfig"}
                for gen_key, gen_value in value.items():
                    if gen_key in allowed_gen_keys and gen_value is not None:
                        clean_gen_config[gen_key] = gen_value
                cleaned[key] = clean_gen_config
            
            # å¤„ç† tools
            elif key == "tools" and isinstance(value, list):
                clean_tools = []
                for tool in value:
                    if isinstance(tool, dict) and "functionDeclarations" in tool:
                        clean_func_decls = []
                        for func_decl in tool["functionDeclarations"]:
                            if isinstance(func_decl, dict):
                                clean_decl = {}
                                if "name" in func_decl:
                                    clean_decl["name"] = str(func_decl["name"])
                                if "description" in func_decl:
                                    clean_decl["description"] = str(func_decl["description"])
                                if "parameters" in func_decl:
                                    # åº”ç”¨ç°æœ‰çš„schemaæ¸…ç†
                                    clean_decl["parameters"] = self._clean_json_schema_properties(func_decl["parameters"])
                                clean_func_decls.append(clean_decl)
                        
                        if clean_func_decls:
                            clean_tools.append({"functionDeclarations": clean_func_decls})
                
                if clean_tools:
                    cleaned[key] = clean_tools
            
            # å…¶ä»–å­—æ®µç›´æ¥ä¿ç•™
            else:
                cleaned[key] = value
        
        return cleaned