"""
Geminiæ ¼å¼è½¬æ¢å™¨
å¤„ç†Google Gemini APIæ ¼å¼ä¸å…¶ä»–æ ¼å¼ä¹‹é—´çš„è½¬æ¢
"""
from typing import Dict, Any, Optional, List
import json
import copy

from .base_converter import BaseConverter, ConversionResult, ConversionError


class GeminiConverter(BaseConverter):
    """Geminiæ ¼å¼è½¬æ¢å™¨"""

    def __init__(self):
        super().__init__()
        self.original_model = None
        # ç”¨äºè·¨ OpenAI å¾€è¿”ä¿ç•™ Gemini thoughtSignature çš„æ˜ å°„
        # key: tool_call_id, value: thoughtSignature
        self._thought_signatures_by_tool_call_id: Dict[str, str] = {}
        # ç”¨äºä¿ç•™ OpenRouter reasoning_detailsï¼ˆæŒ‰ assistant æ¶ˆæ¯çš„é¦–ä¸ª tool_call_id ç´¢å¼•ï¼‰
        # key: first_tool_call_id, value: reasoning_details array
        # å¤šè½®å·¥å…·è°ƒç”¨æ—¶ï¼Œæ¯ä¸ª assistant æ¶ˆæ¯çš„ reasoning_details éƒ½éœ€è¦ä¿ç•™
        self._reasoning_details_cache: Dict[str, List[Dict[str, Any]]] = {}
    
    def set_original_model(self, model: str):
        """è®¾ç½®åŸå§‹æ¨¡å‹åç§°"""
        self.original_model = model
    
    def _determine_reasoning_effort_from_budget(self, thinking_budget: Optional[int]) -> str:
        """æ ¹æ®thinkingBudgetåˆ¤æ–­OpenAI reasoning_effortç­‰çº§
        
        Args:
            thinking_budget: Gemini thinkingçš„thinkingBudgetå€¼
            
        Returns:
            str: OpenAI reasoning_effortç­‰çº§ ("low", "medium", "high")
        """
        import os
        
        # å¦‚æœæ²¡æœ‰æä¾›thinking_budgetæˆ–ä¸º-1ï¼ˆåŠ¨æ€æ€è€ƒï¼‰ï¼Œé»˜è®¤ä¸ºhigh
        if thinking_budget is None or thinking_budget == -1:
            reason = "dynamic thinking (-1)" if thinking_budget == -1 else "no budget provided"
            self.logger.info(f"No valid thinkingBudget ({reason}), defaulting to reasoning_effort='high'")
            return "high"
        
        # ä»ç¯å¢ƒå˜é‡è·å–é˜ˆå€¼é…ç½®
        low_threshold_str = os.environ.get("GEMINI_TO_OPENAI_LOW_REASONING_THRESHOLD")
        high_threshold_str = os.environ.get("GEMINI_TO_OPENAI_HIGH_REASONING_THRESHOLD")
        
        # æ£€æŸ¥å¿…éœ€çš„ç¯å¢ƒå˜é‡
        if low_threshold_str is None:
            raise ConversionError("GEMINI_TO_OPENAI_LOW_REASONING_THRESHOLD environment variable is required for intelligent reasoning_effort determination")
        
        if high_threshold_str is None:
            raise ConversionError("GEMINI_TO_OPENAI_HIGH_REASONING_THRESHOLD environment variable is required for intelligent reasoning_effort determination")
        
        try:
            low_threshold = int(low_threshold_str)
            high_threshold = int(high_threshold_str)
            
            self.logger.debug(f"Threshold configuration: low <= {low_threshold}, medium <= {high_threshold}, high > {high_threshold}")
            
            if thinking_budget <= low_threshold:
                effort = "low"
            elif thinking_budget <= high_threshold:
                effort = "medium"
            else:
                effort = "high"
            
            self.logger.info(f"ğŸ¯ Thinking budget {thinking_budget} -> reasoning_effort '{effort}' (thresholds: low<={low_threshold}, high<={high_threshold})")
            return effort
            
        except ValueError as e:
            raise ConversionError(f"Invalid threshold values in environment variables: {e}. GEMINI_TO_OPENAI_LOW_REASONING_THRESHOLD and GEMINI_TO_OPENAI_HIGH_REASONING_THRESHOLD must be integers.")
    
    def reset_streaming_state(self):
        """é‡ç½®æ‰€æœ‰æµå¼ç›¸å…³çš„çŠ¶æ€å˜é‡ï¼Œé¿å…çŠ¶æ€æ±¡æŸ“"""
        streaming_attrs = [
            '_anthropic_stream_id', '_openai_sent_start', '_gemini_text_started',
            '_anthropic_to_gemini_state', '_streaming_tool_calls'
        ]
        for attr in streaming_attrs:
            if hasattr(self, attr):
                delattr(self, attr)
        # æ¸…ç©º thoughtSignature æ˜ å°„
        self._thought_signatures_by_tool_call_id.clear()
        # æ³¨æ„ï¼šä¸æ¸…ç©º _reasoning_details_cacheï¼Œå› ä¸ºå®ƒéœ€è¦è·¨è¯·æ±‚ä¿ç•™ä»¥æ”¯æŒå¤šè½®å·¥å…·è°ƒç”¨
    
    def get_supported_formats(self) -> List[str]:
        """è·å–æ”¯æŒçš„æ ¼å¼åˆ—è¡¨"""
        return ["openai", "anthropic", "gemini"]
    
    def convert_request(
        self,
        data: Dict[str, Any],
        target_format: str,
        headers: Optional[Dict[str, str]] = None
    ) -> ConversionResult:
        """è½¬æ¢Geminiè¯·æ±‚åˆ°ç›®æ ‡æ ¼å¼"""
        try:
            if target_format == "gemini":
                # Geminiåˆ°Geminiï¼Œæ ¼å¼ä¸æ¸ é“ç›¸åŒï¼Œä¸éœ€è¦è½¬æ¢æ€è€ƒå‚æ•°
                # ä½†éœ€è¦ç§»é™¤å†…éƒ¨å¤„ç†ç”¨çš„streamå­—æ®µï¼Œå› ä¸ºGemini APIä¸æ¥å—æ­¤å­—æ®µ
                cleaned_data = data.copy()
                if "stream" in cleaned_data:
                    cleaned_data.pop("stream")
                    self.logger.debug("Removed internal stream flag for Gemini API")
                return ConversionResult(success=True, data=cleaned_data)
            elif target_format == "openai":
                return self._convert_to_openai_request(data)
            elif target_format == "anthropic":
                return self._convert_to_anthropic_request(data)
            else:
                return ConversionResult(
                    success=False,
                    error=f"Unsupported target format: {target_format}"
                )
        except Exception as e:
            self.logger.error(f"Failed to convert Gemini request to {target_format}: {e}")
            return ConversionResult(success=False, error=str(e))
    
    def convert_response(
        self,
        data: Dict[str, Any],
        source_format: str,
        target_format: str
    ) -> ConversionResult:
        """è½¬æ¢å“åº”åˆ°Geminiæ ¼å¼"""
        try:
            if source_format == "gemini":
                return ConversionResult(success=True, data=data)
            elif source_format == "openai":
                return self._convert_from_openai_response(data)
            elif source_format == "anthropic":
                return self._convert_from_anthropic_response(data)
            else:
                return ConversionResult(
                    success=False,
                    error=f"Unsupported source format: {source_format}"
                )
        except Exception as e:
            self.logger.error(f"Failed to convert {source_format} response to Gemini: {e}")
            return ConversionResult(success=False, error=str(e))
    
    def _convert_to_openai_request(self, data: Dict[str, Any]) -> ConversionResult:
        """è½¬æ¢Geminiè¯·æ±‚åˆ°OpenAIæ ¼å¼"""
        result_data = {}

        # æ¸…ç©º thoughtSignature æ˜ å°„ï¼Œé¿å…è·¨è¯·æ±‚æ±¡æŸ“
        self._thought_signatures_by_tool_call_id.clear()
        # æ³¨æ„ï¼šä¸æ¸…ç©º _reasoning_details_cacheï¼Œå› ä¸ºå®ƒéœ€è¦è·¨è¯·æ±‚ä¿ç•™

        # å¿…é¡»æœ‰åŸå§‹æ¨¡å‹åç§°ï¼Œå¦åˆ™æŠ¥é”™
        if not self.original_model:
            raise ValueError("Original model name is required for request conversion")

        result_data["model"] = self.original_model  # ä½¿ç”¨åŸå§‹æ¨¡å‹åç§°

        # åˆå§‹åŒ–å‡½æ•°è°ƒç”¨IDæ˜ å°„è¡¨ï¼Œç”¨äºä¿æŒå·¥å…·è°ƒç”¨å’Œå·¥å…·ç»“æœçš„IDä¸€è‡´æ€§
        # å…ˆæ‰«ææ•´ä¸ªå¯¹è¯å†å²ï¼Œä¸ºæ¯ä¸ªfunctionCallå’ŒfunctionResponseå»ºç«‹æ˜ å°„å…³ç³»
        self._function_call_mapping = self._build_function_call_mapping(data.get("contents", []))
        
        # å¤„ç†æ¶ˆæ¯å’Œç³»ç»Ÿæ¶ˆæ¯
        messages = []
        # ç”¨äºå»é‡ï¼šè®°å½•å·²å¤„ç†çš„ functionResponse IDï¼Œé¿å… gemini-cli å‘é€é‡å¤æ¶ˆæ¯å¯¼è‡´ 400 é”™è¯¯
        _processed_function_response_ids: set = set()

        # ğŸ” è°ƒè¯•æ—¥å¿—ï¼šè®°å½•æ”¶åˆ°çš„ contents ç»“æ„
        contents = data.get("contents", [])
        print(f"ğŸ“¥ [GEMINI->OPENAI] Received {len(contents)} content items")
        for idx, content in enumerate(contents):
            role = content.get("role", "user")
            parts = content.get("parts", [])
            # æ£€æŸ¥æ˜¯å¦æœ‰ functionResponse
            fr_ids = []
            for part in parts:
                if "functionResponse" in part:
                    fr_id = part["functionResponse"].get("id", "NO_ID")
                    fr_ids.append(fr_id)
            if fr_ids:
                print(f"ğŸ“¥ [GEMINI->OPENAI] Content[{idx}] role={role}, functionResponse IDs: {fr_ids}")

        # æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯ - æ”¯æŒä¸¤ç§æ ¼å¼
        system_instruction_data = data.get("systemInstruction") or data.get("system_instruction")
        if system_instruction_data:
            system_parts = system_instruction_data.get("parts", [])
            system_text = ""
            for part in system_parts:
                if "text" in part:
                    system_text += part["text"]
            if system_text:
                messages.append(self._create_system_message(system_text))
        
        # è½¬æ¢å†…å®¹
        if "contents" in data:
            for content in data["contents"]:
                gemini_role = content.get("role", "user")
                parts = content.get("parts", [])
                
                # å¤„ç†ä¸åŒè§’è‰²çš„æ¶ˆæ¯
                if gemini_role == "user":
                    # æ£€æŸ¥æ˜¯å¦åŒ…å« functionResponseï¼ˆå·¥å…·ç»“æœï¼‰
                    has_function_response = any("functionResponse" in part for part in parts)
                    if has_function_response:
                        # è½¬æ¢ä¸º OpenAI çš„ tool æ¶ˆæ¯
                        for part in parts:
                            if "functionResponse" in part:
                                fr = part["functionResponse"]
                                func_name = fr.get("name", "")
                                response_content = fr.get("response", {})

                                # ä»å“åº”å†…å®¹ä¸­æå–æ–‡æœ¬
                                if isinstance(response_content, dict):
                                    tool_result = response_content.get("content") or response_content.get("output") or json.dumps(response_content, ensure_ascii=False)
                                else:
                                    tool_result = str(response_content)

                                # ä¼˜å…ˆä½¿ç”¨ functionResponse è‡ªå¸¦çš„ id å­—æ®µ
                                tool_call_id = fr.get("id")
                                if not tool_call_id:
                                    # å¤‡é€‰ï¼šä½¿ç”¨æ˜ å°„
                                    if not hasattr(self, '_current_response_sequence'):
                                        self._current_response_sequence = {}
                                    sequence = self._current_response_sequence.get(func_name, 0) + 1
                                    self._current_response_sequence[func_name] = sequence
                                    tool_call_id = self._function_call_mapping.get(f"response_{func_name}_{sequence}")
                                    if not tool_call_id:
                                        tool_call_id = self._function_call_mapping.get(f"{func_name}_{sequence}")
                                        if not tool_call_id:
                                            tool_call_id = f"call_{func_name}_{sequence:04d}"

                                # å»é‡ï¼šè·³è¿‡å·²å¤„ç†çš„ functionResponseï¼ˆåŸºäº tool_call_idï¼‰
                                if tool_call_id in _processed_function_response_ids:
                                    print(f"ğŸ”„ [GEMINI->OPENAI] Skipping duplicate functionResponse: {tool_call_id}")
                                    continue
                                _processed_function_response_ids.add(tool_call_id)
                                print(f"âœ… [GEMINI->OPENAI] Processing functionResponse: {tool_call_id}")

                                # æŸäº› OpenAI å…¼å®¹ API è¦æ±‚ tool æ¶ˆæ¯åŒ…å« name å­—æ®µ
                                tool_msg = {
                                    "role": "tool",
                                    "tool_call_id": tool_call_id,
                                    "content": tool_result
                                }
                                if func_name:
                                    tool_msg["name"] = func_name
                                messages.append(tool_msg)
                    else:
                        # æ™®é€šç”¨æˆ·æ¶ˆæ¯
                        message_content = self._convert_content_from_gemini(parts)
                        messages.append({
                            "role": "user",
                            "content": message_content
                        })
                        
                elif gemini_role == "model":
                    # åŠ©æ‰‹æ¶ˆæ¯ï¼Œå¯èƒ½åŒ…å«å·¥å…·è°ƒç”¨
                    message_content = self._convert_content_from_gemini(parts)

                    if isinstance(message_content, dict) and message_content.get("type") == "tool_calls":
                        # æœ‰å·¥å…·è°ƒç”¨çš„åŠ©æ‰‹æ¶ˆæ¯
                        # æŸäº› OpenAI å…¼å®¹ API è¦æ±‚ content å­—æ®µå¿…é¡»å­˜åœ¨ï¼ˆå³ä½¿ä¸ºç©ºå­—ç¬¦ä¸²ï¼‰
                        tool_call_content = message_content.get("content") or ""
                        tool_calls = message_content["tool_calls"]
                        message: Dict[str, Any] = {
                            "role": "assistant",
                            "content": tool_call_content,
                            "tool_calls": tool_calls
                        }
                        # ä» cache ä¸­æŸ¥æ‰¾å¯¹åº”çš„ reasoning_detailsï¼ˆæŒ‰é¦–ä¸ª tool_call_id ç´¢å¼•ï¼‰
                        if tool_calls:
                            first_tool_call_id = tool_calls[0].get("id")
                            if first_tool_call_id and first_tool_call_id in self._reasoning_details_cache:
                                message["reasoning_details"] = self._reasoning_details_cache[first_tool_call_id]
                                print(f"ğŸ§  [REASONING_DETAILS] Attached {len(message['reasoning_details'])} reasoning_details to assistant message (key={first_tool_call_id})")
                        messages.append(message)
                    else:
                        # æ™®é€šåŠ©æ‰‹æ¶ˆæ¯ï¼ˆæ— å·¥å…·è°ƒç”¨ï¼‰
                        msg: Dict[str, Any] = {
                            "role": "assistant",
                            "content": message_content
                        }
                        messages.append(msg)
                        
                elif gemini_role == "tool":
                    # å·¥å…·è§’è‰²çš„æ¶ˆæ¯ï¼Œå¤„ç†functionResponse
                    for part in parts:
                        if "functionResponse" in part:
                            fr = part["functionResponse"]
                            func_name = fr.get("name", "")
                            response_content = fr.get("response", {})

                            # ä»å“åº”å†…å®¹ä¸­æå–æ–‡æœ¬
                            if isinstance(response_content, dict):
                                tool_result = response_content.get("content") or response_content.get("output") or json.dumps(response_content, ensure_ascii=False)
                            else:
                                tool_result = str(response_content)

                            # ä¼˜å…ˆä½¿ç”¨ functionResponse è‡ªå¸¦çš„ id å­—æ®µ
                            tool_call_id = fr.get("id")
                            if not tool_call_id:
                                # å¤‡é€‰ï¼šä½¿ç”¨æ˜ å°„
                                if not hasattr(self, '_current_response_sequence'):
                                    self._current_response_sequence = {}
                                sequence = self._current_response_sequence.get(func_name, 0) + 1
                                self._current_response_sequence[func_name] = sequence
                                tool_call_id = self._function_call_mapping.get(f"response_{func_name}_{sequence}")
                                if not tool_call_id:
                                    tool_call_id = self._function_call_mapping.get(f"{func_name}_{sequence}")
                                    if not tool_call_id:
                                        tool_call_id = f"call_{func_name}_{sequence:04d}"

                            # å»é‡ï¼šè·³è¿‡å·²å¤„ç†çš„ functionResponseï¼ˆåŸºäº tool_call_idï¼‰
                            if tool_call_id in _processed_function_response_ids:
                                print(f"ğŸ”„ [GEMINI->OPENAI] Skipping duplicate functionResponse (tool role): {tool_call_id}")
                                continue
                            _processed_function_response_ids.add(tool_call_id)
                            print(f"âœ… [GEMINI->OPENAI] Processing functionResponse (tool role): {tool_call_id}")

                            # æŸäº› OpenAI å…¼å®¹ API è¦æ±‚ tool æ¶ˆæ¯åŒ…å« name å­—æ®µ
                            tool_msg = {
                                "role": "tool",
                                "tool_call_id": tool_call_id,
                                "content": tool_result
                            }
                            if func_name:
                                tool_msg["name"] = func_name
                            messages.append(tool_msg)
                else:
                    # å…¶ä»–è§’è‰²ï¼Œé»˜è®¤è½¬ä¸ºassistant
                    message_content = self._convert_content_from_gemini(parts)
                    messages.append({
                        "role": "assistant",
                        "content": message_content
                    })

        # åˆå¹¶è¿ç»­çš„åŒè§’è‰²æ¶ˆæ¯ï¼ˆGemini API ä¸å…è®¸è¿ç»­åŒè§’è‰²æ¶ˆæ¯ï¼‰
        # è¿™ä¸ªé—®é¢˜åœ¨ OpenAI â†’ Gemini è½¬æ¢æ—¶ä¼šå¯¼è‡´ 400 é”™è¯¯
        messages = self._merge_consecutive_messages(messages)

        # ğŸ” è°ƒè¯•æ—¥å¿—ï¼šè®°å½•è½¬æ¢åçš„ messages ç»“æ„
        tool_msgs = [m for m in messages if m.get("role") == "tool"]
        tool_call_ids = [m.get("tool_call_id") for m in tool_msgs]
        print(f"ğŸ“¤ [GEMINI->OPENAI] Generated {len(messages)} messages, {len(tool_msgs)} tool messages")
        if tool_call_ids:
            print(f"ğŸ“¤ [GEMINI->OPENAI] Tool message IDs: {tool_call_ids}")
        print(f"ğŸ“¤ [GEMINI->OPENAI] Dedup stats: processed {len(_processed_function_response_ids)} unique functionResponse IDs")

        result_data["messages"] = messages

        # P2-14: safetySettings é€ä¼ åˆ° metadata
        if "safetySettings" in data:
            metadata = result_data.get("metadata") or {}
            metadata["gemini_safety_settings"] = data["safetySettings"]
            result_data["metadata"] = metadata

        # å¤„ç†ç”Ÿæˆé…ç½®
        if "generationConfig" in data:
            config = data["generationConfig"]
            if "temperature" in config:
                result_data["temperature"] = config["temperature"]
            if "topP" in config:
                result_data["top_p"] = config["topP"]
            if "maxOutputTokens" in config:
                result_data["max_tokens"] = config["maxOutputTokens"]
            if "stopSequences" in config:
                result_data["stop"] = config["stopSequences"]
            # P1: presencePenalty/frequencyPenalty/candidateCount å‚æ•°è½¬æ¢
            if "presencePenalty" in config:
                result_data["presence_penalty"] = config["presencePenalty"]
            if "frequencyPenalty" in config:
                result_data["frequency_penalty"] = config["frequencyPenalty"]
            if "candidateCount" in config:
                result_data["n"] = config["candidateCount"]

            # å¤„ç†ç»“æ„åŒ–è¾“å‡º
            if config.get("response_mime_type") == "application/json":
                result_data["response_format"] = {"type": "json_object"}
                if "response_schema" in config:
                    result_data["response_format"] = {
                        "type": "json_schema",
                        "json_schema": {
                            "name": "response",
                            "strict": True,
                            "schema": config["response_schema"]
                        }
                    }

        # ç¡®ä¿æœ‰ max_tokensï¼ˆæŸäº› OpenAI å…¼å®¹ API è¦æ±‚å¿…é¡»æä¾›ï¼‰
        # ä¼˜å…ˆçº§ï¼š1. generationConfig.maxOutputTokens 2. ç¯å¢ƒå˜é‡ 3. é»˜è®¤å€¼
        if "max_tokens" not in result_data:
            import os
            env_max_tokens = os.environ.get("OPENAI_DEFAULT_MAX_TOKENS")
            if env_max_tokens:
                try:
                    result_data["max_tokens"] = int(env_max_tokens)
                    self.logger.debug(f"Using OPENAI_DEFAULT_MAX_TOKENS: {result_data['max_tokens']}")
                except ValueError:
                    self.logger.warning(f"Invalid OPENAI_DEFAULT_MAX_TOKENS: {env_max_tokens}")
            else:
                # ä½¿ç”¨åˆç†çš„é»˜è®¤å€¼
                result_data["max_tokens"] = 8192
                self.logger.debug("Using default max_tokens: 8192")
        
        # å¤„ç†å·¥å…·è°ƒç”¨
        if "tools" in data:
            openai_tools = []
            unsupported_tools = []
            has_code_execution = False
            has_google_search = False

            self.logger.info(f"ğŸ”§ [TOOLS] Processing {len(data['tools'])} tools")

            for tool in data["tools"]:
                self.logger.debug(f"ğŸ”§ [TOOLS] Tool structure keys: {list(tool.keys())}")

                # Geminiå®˜æ–¹ä½¿ç”¨ snake_case: function_declarations
                func_key = None
                if "function_declarations" in tool:
                    func_key = "function_declarations"
                elif "functionDeclarations" in tool:  # å…¼å®¹æ—§å†™æ³•
                    func_key = "functionDeclarations"

                if func_key:
                    self.logger.info(f"ğŸ”§ [TOOLS] Found {len(tool[func_key])} function declarations")
                    for func_decl in tool[func_key]:
                        func_name = func_decl.get("name", "")
                        function_def = {
                            "name": func_name,
                            "description": func_decl.get("description", "")
                        }

                        # å¤„ç† parameters - æŸäº› OpenAI å…¼å®¹ API è¦æ±‚å¿…é¡»æœ‰æ­¤å­—æ®µ
                        if "parameters" in func_decl:
                            raw_params = func_decl["parameters"]
                            self.logger.debug(f"ğŸ”§ [TOOLS] Function '{func_name}' raw parameters: {raw_params}")
                            function_def["parameters"] = self._sanitize_schema_for_openai(raw_params)
                        else:
                            # Gemini æ²¡æœ‰æä¾› parametersï¼Œæ·»åŠ ç©ºçš„ parameters ä»¥å…¼å®¹ç¬¬ä¸‰æ–¹ API
                            function_def["parameters"] = {"type": "object", "properties": {}}
                            self.logger.debug(f"ğŸ”§ [TOOLS] Function '{func_name}' has no parameters, using empty schema")

                        openai_tools.append({
                            "type": "function",
                            "function": function_def
                        })

                # æ£€æµ‹éå‡½æ•°å·¥å…·ç±»å‹ï¼ˆä¸ func_key ç‹¬ç«‹å¤„ç†ï¼‰
                if "code_execution" in tool:
                    has_code_execution = True
                if "google_search" in tool:
                    has_google_search = True
                if "google_search_retrieval" in tool:
                    unsupported_tools.append("google_search_retrieval")
                if "retrieval" in tool:
                    unsupported_tools.append("retrieval")

            # code_execution â†’ è™šæ‹Ÿå‡½æ•°å·¥å…·
            if has_code_execution:
                openai_tools.append({
                    "type": "function",
                    "function": {
                        "name": "code_execution",
                        "description": "Execute Python code. Caller must implement the execution handler.",
                        "parameters": {
                            "type": "object",
                            "properties": {
                                "code": {"type": "string", "description": "Python code to execute"}
                            },
                            "required": ["code"]
                        }
                    }
                })
                self.logger.warning(
                    "Gemini code_execution mapped to OpenAI function tool. "
                    "Caller must implement the execution handler."
                )

            # P2-15: google_search â†’ è™šæ‹Ÿå‡½æ•°å·¥å…·
            if has_google_search:
                openai_tools.append({
                    "type": "function",
                    "function": {
                        "name": "google_search",
                        "description": "Web search using Google. Caller must implement the search handler.",
                        "parameters": {
                            "type": "object",
                            "properties": {
                                "query": {"type": "string", "description": "Search query"}
                            },
                            "required": ["query"]
                        }
                    }
                })
                self.logger.warning(
                    "Gemini google_search mapped to OpenAI function tool. "
                    "Caller must implement the search handler."
                )

            # è®°å½•ä¸æ”¯æŒçš„å·¥å…·è­¦å‘Šï¼ˆå»é‡ï¼‰
            if unsupported_tools:
                self.logger.warning(
                    f"Gemini tools not supported for OpenAI conversion: {list(set(unsupported_tools))}. "
                    "These tools will be ignored in the converted request."
                )

            if openai_tools:
                result_data["tools"] = openai_tools
                result_data["tool_choice"] = "auto"
        
        # å¤„ç†æ€è€ƒé¢„ç®—è½¬æ¢ (Gemini thinkingConfig -> OpenAI reasoning_effort + max_completion_tokens)
        if "generationConfig" in data and "thinkingConfig" in data["generationConfig"]:
            thinking_config = data["generationConfig"]["thinkingConfig"]
            thinking_budget = thinking_config.get("thinkingBudget")
            
            if thinking_budget is not None and thinking_budget != 0:
                # æ£€æµ‹åˆ°æ€è€ƒå‚æ•°ï¼Œè®¾ç½®ä¸ºOpenAIæ€è€ƒæ¨¡å‹æ ¼å¼
                reasoning_effort = self._determine_reasoning_effort_from_budget(thinking_budget)
                result_data["reasoning_effort"] = reasoning_effort
                
                # å¤„ç†max_completion_tokensçš„ä¼˜å…ˆçº§é€»è¾‘
                max_completion_tokens = None
                
                # ä¼˜å…ˆçº§1ï¼šå®¢æˆ·ç«¯ä¼ å…¥çš„maxOutputTokens
                if "generationConfig" in data and "maxOutputTokens" in data["generationConfig"]:
                    max_completion_tokens = data["generationConfig"]["maxOutputTokens"]
                    # ç§»é™¤max_tokensï¼Œä½¿ç”¨max_completion_tokens
                    if "max_tokens" in result_data:
                        result_data.pop("max_tokens", None)
                    self.logger.info(f"Using client maxOutputTokens as max_completion_tokens: {max_completion_tokens}")
                else:
                    # ä¼˜å…ˆçº§2ï¼šç¯å¢ƒå˜é‡OPENAI_REASONING_MAX_TOKENS
                    import os
                    env_max_tokens = os.environ.get("OPENAI_REASONING_MAX_TOKENS")
                    if env_max_tokens:
                        try:
                            max_completion_tokens = int(env_max_tokens)
                            self.logger.info(f"Using OPENAI_REASONING_MAX_TOKENS from environment: {max_completion_tokens}")
                        except ValueError:
                            self.logger.warning(f"Invalid OPENAI_REASONING_MAX_TOKENS value '{env_max_tokens}', must be integer")
                            env_max_tokens = None
                    
                    if not env_max_tokens:
                        # ä¼˜å…ˆçº§3ï¼šéƒ½æ²¡æœ‰åˆ™æŠ¥é”™
                        raise ConversionError("For OpenAI reasoning models, max_completion_tokens is required. Please specify maxOutputTokens in generationConfig or set OPENAI_REASONING_MAX_TOKENS environment variable.")
                
                result_data["max_completion_tokens"] = max_completion_tokens
                self.logger.info(f"Gemini thinkingBudget {thinking_budget} -> OpenAI reasoning_effort='{reasoning_effort}', max_completion_tokens={max_completion_tokens}")

        # å¤„ç†æµå¼å‚æ•° - å…³é”®ä¿®å¤ï¼
        if "stream" in data:
            result_data["stream"] = data["stream"]

        # æ±‡æ€» thoughtSignature æ•è·æƒ…å†µ
        if self._thought_signatures_by_tool_call_id:
            print(f"ğŸ§  [THOUGHT_SIGNATURE] Request conversion complete. Captured {len(self._thought_signatures_by_tool_call_id)} signatures: {list(self._thought_signatures_by_tool_call_id.keys())}")

        return ConversionResult(success=True, data=result_data)
    
    def _convert_to_anthropic_request(self, data: Dict[str, Any]) -> ConversionResult:
        """è½¬æ¢Geminiè¯·æ±‚åˆ°Anthropicæ ¼å¼"""
        result_data = {}

        # æ¸…ç©º thoughtSignature æ˜ å°„ï¼Œé¿å…è·¨è¯·æ±‚æ±¡æŸ“
        self._thought_signatures_by_tool_call_id.clear()

        # å¿…é¡»æœ‰åŸå§‹æ¨¡å‹åç§°ï¼Œå¦åˆ™æŠ¥é”™
        if not self.original_model:
            raise ValueError("Original model name is required for request conversion")

        result_data["model"] = self.original_model  # ä½¿ç”¨åŸå§‹æ¨¡å‹åç§°
        
        # å¤„ç†ç³»ç»Ÿæ¶ˆæ¯ - æ”¯æŒä¸¤ç§æ ¼å¼
        system_instruction_data = data.get("systemInstruction") or data.get("system_instruction")
        if system_instruction_data:
            system_parts = system_instruction_data.get("parts", [])
            system_text = ""
            for part in system_parts:
                if "text" in part:
                    system_text += part["text"]
            if system_text:
                result_data["system"] = system_text
        
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        if "contents" in data:
            # å»ºç«‹å·¥å…·è°ƒç”¨IDæ˜ å°„è¡¨
            self._function_call_mapping = self._build_function_call_mapping(data["contents"])
            
            anthropic_messages = []
            for content in data["contents"]:
                role = content.get("role", "user")
                if role == "model":
                    role = "assistant"
                elif role == "tool":
                    # Geminiçš„toolè§’è‰²ï¼ˆfunctionResponseï¼‰å¯¹åº”Anthropicçš„userè§’è‰²
                    role = "user"
                
                message_content = self._convert_content_to_anthropic(content.get("parts", []))
                
                # è·³è¿‡ç©ºå†…å®¹çš„æ¶ˆæ¯ï¼ŒAnthropicä¸å…è®¸ç©ºå†…å®¹
                if not message_content or (isinstance(message_content, str) and not message_content.strip()):
                    self.logger.warning(f"Skipping message with empty content for role '{role}'")
                    continue
                
                anthropic_messages.append({
                    "role": role,
                    "content": message_content
                })
            result_data["messages"] = anthropic_messages
        
        # å¤„ç†ç”Ÿæˆé…ç½®
        if "generationConfig" in data:
            config = data["generationConfig"]
            if "temperature" in config:
                result_data["temperature"] = config["temperature"]
            if "topP" in config:
                result_data["top_p"] = config["topP"]
            if "topK" in config:
                result_data["top_k"] = config["topK"]
            if "maxOutputTokens" in config:
                result_data["max_tokens"] = config["maxOutputTokens"]
            if "stopSequences" in config:
                result_data["stop_sequences"] = config["stopSequences"]
        
        # Anthropic è¦æ±‚å¿…é¡»æœ‰ max_tokensï¼ŒæŒ‰ä¼˜å…ˆçº§å¤„ç†ï¼š
        # 1. Gemini generationConfigä¸­çš„maxOutputTokensï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
        # 2. ç¯å¢ƒå˜é‡ANTHROPIC_MAX_TOKENS
        # 3. éƒ½æ²¡æœ‰åˆ™æŠ¥é”™
        if "max_tokens" not in result_data:
            # ä¼˜å…ˆçº§2ï¼šæ£€æŸ¥ç¯å¢ƒå˜é‡ANTHROPIC_MAX_TOKENS
            import os
            env_max_tokens = os.environ.get("ANTHROPIC_MAX_TOKENS")
            if env_max_tokens:
                try:
                    max_tokens = int(env_max_tokens)
                    self.logger.info(f"Using ANTHROPIC_MAX_TOKENS from environment: {max_tokens}")
                    result_data["max_tokens"] = max_tokens
                except ValueError:
                    self.logger.warning(f"Invalid ANTHROPIC_MAX_TOKENS value '{env_max_tokens}', must be integer")
                    env_max_tokens = None
            
            if not env_max_tokens:
                # ä¼˜å…ˆçº§3ï¼šéƒ½æ²¡æœ‰åˆ™æŠ¥é”™ï¼Œè¦æ±‚ç”¨æˆ·æ˜ç¡®æŒ‡å®š
                raise ValueError(f"max_tokens is required for Anthropic API. Please specify max_tokens in generationConfig.maxOutputTokens or set ANTHROPIC_MAX_TOKENS environment variable.")
        
        # P2-14: safetySettings é€ä¼ åˆ° metadata
        if "safetySettings" in data:
            metadata = result_data.get("metadata") or {}
            metadata["gemini_safety_settings"] = data["safetySettings"]
            result_data["metadata"] = metadata

        # å¤„ç†å·¥å…·è°ƒç”¨
        if "tools" in data:
            anthropic_tools = []
            unsupported_tools = []
            has_code_execution = False
            has_google_search = False

            for tool in data["tools"]:
                func_key = None
                if "function_declarations" in tool:
                    func_key = "function_declarations"
                elif "functionDeclarations" in tool:  # å…¼å®¹æ—§å†™æ³•
                    func_key = "functionDeclarations"

                if func_key:
                    for func_decl in tool[func_key]:
                        anthropic_tools.append({
                            "name": func_decl.get("name", ""),
                            "description": func_decl.get("description", ""),
                            "input_schema": self._convert_schema_for_anthropic(func_decl.get("parameters", {}))
                        })

                # æ£€æµ‹éå‡½æ•°å·¥å…·ç±»å‹ï¼ˆä¸ func_key ç‹¬ç«‹å¤„ç†ï¼‰
                if "code_execution" in tool:
                    has_code_execution = True
                if "google_search" in tool:
                    has_google_search = True
                if "google_search_retrieval" in tool:
                    unsupported_tools.append("google_search_retrieval")
                if "retrieval" in tool:
                    unsupported_tools.append("retrieval")

            # code_execution â†’ è™šæ‹Ÿå·¥å…·
            if has_code_execution:
                anthropic_tools.append({
                    "name": "code_execution",
                    "description": "Execute Python code. Caller must implement the execution handler.",
                    "input_schema": {
                        "type": "object",
                        "properties": {
                            "code": {"type": "string", "description": "Python code to execute"}
                        },
                        "required": ["code"]
                    }
                })
                self.logger.warning(
                    "Gemini code_execution mapped to Anthropic tool. "
                    "Caller must implement the execution handler."
                )

            # P2-15: google_search â†’ è™šæ‹Ÿå·¥å…·
            if has_google_search:
                anthropic_tools.append({
                    "name": "google_search",
                    "description": "Web search using Google. Caller must implement the search handler.",
                    "input_schema": {
                        "type": "object",
                        "properties": {
                            "query": {"type": "string", "description": "Search query"}
                        },
                        "required": ["query"]
                    }
                })
                self.logger.warning(
                    "Gemini google_search mapped to Anthropic tool. "
                    "Caller must implement the search handler."
                )

            # è®°å½•ä¸æ”¯æŒçš„å·¥å…·è­¦å‘Šï¼ˆå»é‡ï¼‰
            if unsupported_tools:
                self.logger.warning(
                    f"Gemini tools not supported for Anthropic conversion: {list(set(unsupported_tools))}. "
                    "These tools will be ignored in the converted request."
                )

            if anthropic_tools:
                result_data["tools"] = anthropic_tools
        
        # å¤„ç†æ€è€ƒé¢„ç®—è½¬æ¢ (Gemini thinkingBudget -> Anthropic thinkingBudget)
        if "generationConfig" in data and "thinkingConfig" in data["generationConfig"]:
            thinking_config = data["generationConfig"]["thinkingConfig"]
            thinking_budget = thinking_config.get("thinkingBudget")
            
            if thinking_budget is not None:
                if thinking_budget == -1:
                    # åŠ¨æ€æ€è€ƒï¼Œå¯ç”¨ä½†ä¸è®¾ç½®å…·ä½“tokenæ•°
                    result_data["thinking"] = {
                        "type": "enabled"
                    }
                    self.logger.info("Gemini thinkingBudget -1 (dynamic) -> Anthropic thinking enabled without budget")
                elif thinking_budget == 0:
                    # ä¸å¯ç”¨æ€è€ƒ
                    pass
                else:
                    # æ•°å€¼å‹æ€è€ƒé¢„ç®—ï¼Œç›´æ¥è½¬æ¢
                    result_data["thinking"] = {
                        "type": "enabled",
                        "budget_tokens": thinking_budget
                    }
                    self.logger.info(f"Gemini thinkingBudget {thinking_budget} -> Anthropic thinkingBudget {thinking_budget}")
        
        # å¤„ç†æµå¼å‚æ•° - å…³é”®ä¿®å¤ï¼
        if "stream" in data:
            result_data["stream"] = data["stream"]
        
        return ConversionResult(success=True, data=result_data)
    
    def _convert_from_openai_response(self, data: Dict[str, Any]) -> ConversionResult:
        """è½¬æ¢OpenAIå“åº”åˆ°Geminiæ ¼å¼"""
        result_data = {
            "candidates": [],
            "usageMetadata": {}
        }

        # å¤„ç†é€‰æ‹©
        if "choices" in data and data["choices"] and data["choices"][0]:
            choice = data["choices"][0]
            message = choice.get("message", {})
            content = message.get("content", "")
            tool_calls = message.get("tool_calls", [])

            # æ•è· reasoning_detailsï¼ˆOpenRouter è¿”å›çš„æ¨ç†è¯¦æƒ…ï¼‰
            # æŒ‰é¦–ä¸ª tool_call_id å­˜å…¥ cacheï¼Œç”¨äºå¤šè½®å·¥å…·è°ƒç”¨æ—¶å›ä¼ 
            reasoning_details = message.get("reasoning_details")
            if reasoning_details and tool_calls:
                first_tool_call_id = tool_calls[0].get("id") if tool_calls else None
                if first_tool_call_id:
                    self._reasoning_details_cache[first_tool_call_id] = reasoning_details
                    print(f"ğŸ§  [REASONING_DETAILS] Cached {len(reasoning_details)} reasoning_details (key={first_tool_call_id})")

            # æ„å»º parts
            parts = []

            # æ·»åŠ æ–‡æœ¬å†…å®¹ï¼ˆå¦‚æœæœ‰ï¼‰
            if content:
                parts.append({"text": content})
            
            # æ·»åŠ å·¥å…·è°ƒç”¨ï¼ˆå¦‚æœæœ‰ï¼‰
            if tool_calls:
                for tool_call in tool_calls:
                    if tool_call and tool_call.get("type") == "function" and "function" in tool_call:
                        func = tool_call["function"]
                        func_name = func.get("name", "")
                        # OpenAI çš„ arguments æ˜¯ JSON å­—ç¬¦ä¸²ï¼Œéœ€è¦è§£æ
                        args_str = func.get("arguments", "{}")
                        try:
                            func_args = json.loads(args_str) if args_str else {}
                        except json.JSONDecodeError:
                            func_args = {}

                        func_args = self._adapt_tool_call_params(func_name, func_args)
                        function_call = {
                            "name": func_name,
                            "args": func_args
                        }
                        # ä¿ç•™åŸå§‹ tool_call IDï¼Œç¡®ä¿ functionResponse å¯ä»¥ä½¿ç”¨ç›¸åŒçš„ ID
                        tool_call_id = tool_call.get("id")
                        if tool_call_id:
                            function_call["id"] = tool_call_id

                        # æ„å»º partï¼ŒåŒ…å« functionCall
                        part: Dict[str, Any] = {"functionCall": function_call}

                        # å›å¡« thoughtSignatureï¼ˆå¦‚æœä¹‹å‰æ•è·è¿‡ï¼‰
                        if tool_call_id and tool_call_id in self._thought_signatures_by_tool_call_id:
                            thought_signature = self._thought_signatures_by_tool_call_id[tool_call_id]
                            part["thoughtSignature"] = thought_signature
                            print(f"ğŸ§  [THOUGHT_SIGNATURE] Restored: tool_call_id={tool_call_id}, signature={thought_signature[:50]}...")

                        parts.append(part)
            
            # å¦‚æœæ²¡æœ‰ä»»ä½•å†…å®¹ï¼Œæ·»åŠ ç©ºæ–‡æœ¬
            if not parts:
                parts = [{"text": ""}]
            
            candidate = {
                "content": {
                    "parts": parts,
                    "role": "model"
                },
                "finishReason": self._map_finish_reason(choice.get("finish_reason", "stop"), "openai", "gemini"),
                "index": 0
            }
            result_data["candidates"] = [candidate]
        
        # å¤„ç†ä½¿ç”¨æƒ…å†µ
        if "usage" in data and data["usage"] is not None:
            usage = data["usage"]
            result_data["usageMetadata"] = {
                "promptTokenCount": usage.get("prompt_tokens", 0),
                "candidatesTokenCount": usage.get("completion_tokens", 0),
                "totalTokenCount": usage.get("total_tokens", 0)
            }
        
        return ConversionResult(success=True, data=result_data)
    
    def _convert_from_anthropic_response(self, data: Dict[str, Any]) -> ConversionResult:
        """è½¬æ¢Anthropicå“åº”åˆ°Geminiæ ¼å¼"""
        result_data = {
            "candidates": [],
            "usageMetadata": {}
        }
        
        # å¤„ç†å†…å®¹ï¼ŒåŒ…æ‹¬æ–‡æœ¬ã€å·¥å…·è°ƒç”¨å’Œæ€è€ƒå†…å®¹
        parts = []
        if "content" in data and isinstance(data["content"], list):
            for item in data["content"]:
                item_type = item.get("type")
                
                # å¤„ç†æ–‡æœ¬å†…å®¹
                if item_type == "text":
                    text_content = item.get("text", "")
                    if text_content.strip():  # åªæ·»åŠ éç©ºæ–‡æœ¬
                        parts.append({"text": text_content})
                
                # å¤„ç†æ€è€ƒå†…å®¹ (thinking â†’ text with thought: true)
                elif item_type == "thinking":
                    thinking_content = item.get("thinking", "")
                    if thinking_content.strip():
                        parts.append({
                            "text": thinking_content,
                            "thought": True  # Gemini 2025æ ¼å¼çš„thinkingæ ‡è¯†
                        })
                
                # å¤„ç†å·¥å…·è°ƒç”¨ (tool_use â†’ functionCall)
                elif item_type == "tool_use":
                    func_name = item.get("name", "")
                    func_args = self._adapt_tool_call_params(func_name, item.get("input", {}))
                    function_call = {
                        "name": func_name,
                        "args": func_args
                    }
                    # ä¿ç•™ Anthropic tool_use çš„ ID
                    tool_use_id = item.get("id")
                    if tool_use_id:
                        function_call["id"] = tool_use_id

                    # æ„å»º partï¼ŒåŒ…å« functionCall
                    part: Dict[str, Any] = {"functionCall": function_call}

                    # å›å¡« thoughtSignatureï¼ˆå¦‚æœä¹‹å‰æ•è·è¿‡ï¼‰
                    if tool_use_id and tool_use_id in self._thought_signatures_by_tool_call_id:
                        thought_signature = self._thought_signatures_by_tool_call_id[tool_use_id]
                        part["thoughtSignature"] = thought_signature
                        print(f"ğŸ§  [THOUGHT_SIGNATURE] Restored (Anthropic): tool_use_id={tool_use_id}, signature={thought_signature[:50]}...")

                    parts.append(part)
        
        # å¦‚æœæ²¡æœ‰ä»»ä½•å†…å®¹ï¼Œæ·»åŠ ç©ºæ–‡æœ¬é¿å…ç©ºpartsæ•°ç»„
        if not parts:
            parts = [{"text": ""}]
        
        candidate = {
            "content": {
                "parts": parts,
                "role": "model"
            },
            "finishReason": self._map_finish_reason(data.get("stop_reason", "end_turn"), "anthropic", "gemini"),
            "index": 0
        }
        result_data["candidates"] = [candidate]
        
        # å¤„ç†ä½¿ç”¨æƒ…å†µ
        if "usage" in data and data["usage"] is not None:
            usage = data["usage"]
            input_tokens = usage.get("input_tokens", 0)
            output_tokens = usage.get("output_tokens", 0)
            result_data["usageMetadata"] = {
                "promptTokenCount": input_tokens,
                "candidatesTokenCount": output_tokens,
                "totalTokenCount": input_tokens + output_tokens
            }
        
        return ConversionResult(success=True, data=result_data)
    
    def _convert_from_openai_streaming_chunk(self, data: Dict[str, Any]) -> ConversionResult:
        """è½¬æ¢OpenAIæµå¼å“åº”chunkåˆ°Geminiæ ¼å¼"""
        self.logger.info(f"OPENAI->GEMINI CHUNK: {data}")  # è®°å½•è¾“å…¥æ•°æ®

        # ä¸ºæµå¼å·¥å…·è°ƒç”¨ç»´æŠ¤çŠ¶æ€
        if not hasattr(self, '_streaming_tool_calls'):
            self._streaming_tool_calls = {}

        # ä¸ºæµå¼ reasoning_details ç»´æŠ¤ä¸´æ—¶çŠ¶æ€ï¼ˆåœ¨ finish æ—¶å­˜å…¥ cacheï¼‰
        if not hasattr(self, '_streaming_reasoning_details'):
            self._streaming_reasoning_details = None

        # æ•è·æµå¼å“åº”ä¸­çš„ reasoning_details
        if "choices" in data and data["choices"] and data["choices"][0]:
            choice = data["choices"][0]
            delta = choice.get("delta", {})

            # reasoning_details å¯èƒ½åœ¨ delta æˆ–é¡¶å±‚ message ä¸­
            reasoning_details = delta.get("reasoning_details") or choice.get("message", {}).get("reasoning_details")
            if reasoning_details:
                self._streaming_reasoning_details = reasoning_details
                print(f"ğŸ§  [REASONING_DETAILS] Captured {len(reasoning_details)} reasoning_details from streaming chunk")

        # å…ˆå¤„ç†å¢é‡å†…å®¹å’Œå·¥å…·è°ƒç”¨ï¼ˆæ”¶é›†çŠ¶æ€ï¼‰
        if "choices" in data and data["choices"] and data["choices"][0]:
            choice = data["choices"][0]
            delta = choice.get("delta", {})
            
            # æ”¶é›†æµå¼å·¥å…·è°ƒç”¨ä¿¡æ¯
            if "tool_calls" in delta:
                for tool_call in delta["tool_calls"]:
                    call_index = tool_call.get("index", 0)
                    call_id = tool_call.get("id", "")
                    call_type = tool_call.get("type", "function")
                    
                    # åˆå§‹åŒ–å·¥å…·è°ƒç”¨çŠ¶æ€
                    if call_index not in self._streaming_tool_calls:
                        self._streaming_tool_calls[call_index] = {
                            "id": call_id,
                            "type": call_type,
                            "function": {
                                "name": "",
                                "arguments": ""
                            }
                        }
                    
                    # æ›´æ–°å·¥å…·è°ƒç”¨ä¿¡æ¯
                    if "function" in tool_call:
                        func = tool_call["function"]
                        if "name" in func:
                            self._streaming_tool_calls[call_index]["function"]["name"] = func["name"]
                        if "arguments" in func:
                            self._streaming_tool_calls[call_index]["function"]["arguments"] += func["arguments"]
                    
                    self.logger.debug(f"Updated tool call {call_index}: {self._streaming_tool_calls[call_index]}")
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯å®Œæ•´çš„æµå¼å“åº”ç»“æŸ
        if "choices" in data and data["choices"] and data["choices"][0] and data["choices"][0].get("finish_reason"):
            choice = data["choices"][0]
            delta = choice.get("delta", {})
            content = delta.get("content", "")
            
            # æ„å»ºpartsæ•°ç»„ï¼Œå¯èƒ½åŒ…å«å†…å®¹å’Œå·¥å…·è°ƒç”¨
            parts = []
            
            # å¤„ç†æ–‡æœ¬å†…å®¹
            if content:
                parts.append({"text": content})
            
            # å¤„ç†æ”¶é›†åˆ°çš„å·¥å…·è°ƒç”¨
            if self._streaming_tool_calls:
                self.logger.debug(f"FINISH: Processing collected tool calls: {self._streaming_tool_calls}")

                # åœ¨æµå¼ç»“æŸæ—¶ï¼Œå°† reasoning_details å­˜å…¥ cacheï¼ˆæŒ‰é¦–ä¸ª tool_call_id ç´¢å¼•ï¼‰
                if hasattr(self, '_streaming_reasoning_details') and self._streaming_reasoning_details:
                    # è·å–é¦–ä¸ª tool_call_id
                    first_index = min(self._streaming_tool_calls.keys())
                    first_tool_call_id = self._streaming_tool_calls[first_index].get("id")
                    if first_tool_call_id:
                        self._reasoning_details_cache[first_tool_call_id] = self._streaming_reasoning_details
                        print(f"ğŸ§  [REASONING_DETAILS] Cached {len(self._streaming_reasoning_details)} reasoning_details from streaming (key={first_tool_call_id})")
                    self._streaming_reasoning_details = None

                for call_index, tool_call in self._streaming_tool_calls.items():
                    func = tool_call.get("function", {})
                    func_name = func.get("name", "")
                    func_args = func.get("arguments", "{}")
                    self.logger.debug(f"FINISH TOOL CALL - name: {func_name}, args: '{func_args}'")

                    # OpenAI çš„ arguments å­—æ®µæ˜¯ JSON å­—ç¬¦ä¸²ï¼Œéœ€è¦è§£æ
                    if func_args.strip() == "[DONE]":
                        self.logger.warning(f"Found [DONE] in tool call arguments, skipping")
                        continue
                    try:
                        func_args_json = json.loads(func_args) if isinstance(func_args, str) else func_args
                    except json.JSONDecodeError as e:
                        self.logger.error(f"JSON decode error in tool args: {e}, func_args: '{func_args}'")
                        func_args_json = {}

                    func_args_json = self._adapt_tool_call_params(func_name, func_args_json)
                    function_call = {
                        "name": func_name,
                        "args": func_args_json
                    }
                    # ä¿ç•™åŸå§‹ tool_call ID
                    tool_call_id = tool_call.get("id")
                    if tool_call_id:
                        function_call["id"] = tool_call_id

                    # æ„å»º partï¼ŒåŒ…å« functionCall
                    part: Dict[str, Any] = {"functionCall": function_call}

                    # å›å¡« thoughtSignatureï¼ˆå¦‚æœä¹‹å‰æ•è·è¿‡ï¼‰
                    if tool_call_id and tool_call_id in self._thought_signatures_by_tool_call_id:
                        thought_signature = self._thought_signatures_by_tool_call_id[tool_call_id]
                        part["thoughtSignature"] = thought_signature
                        print(f"ğŸ§  [THOUGHT_SIGNATURE] Restored (streaming): tool_call_id={tool_call_id}, signature={thought_signature[:50]}...")

                    parts.append(part)
                
                # æ¸…ç†å·¥å…·è°ƒç”¨çŠ¶æ€ï¼Œä¸ºä¸‹æ¬¡è¯·æ±‚åšå‡†å¤‡
                self._streaming_tool_calls = {}
            
            # è¿™æ˜¯æœ€åçš„chunkï¼ŒåŒ…å«finish_reason
            result_data = {
                "candidates": [{
                    "content": {
                        "parts": parts,
                        "role": "model"
                    },
                    "finishReason": self._map_finish_reason(choice.get("finish_reason", "stop"), "openai", "gemini"),
                    "index": 0
                }]
            }
            
            # æ·»åŠ usageä¿¡æ¯ï¼ˆå¦‚æœæœ‰ä¸”ä¸ä¸ºNoneï¼‰
            if "usage" in data and data["usage"] is not None:
                usage = data["usage"]
                result_data["usageMetadata"] = {
                    "promptTokenCount": usage.get("prompt_tokens", 0),
                    "candidatesTokenCount": usage.get("completion_tokens", 0),
                    "totalTokenCount": usage.get("total_tokens", 0)
                }
            
            return ConversionResult(success=True, data=result_data)
            
        # æ£€æŸ¥æ˜¯å¦æœ‰å¢é‡å†…å®¹ï¼ˆéfinish chunkï¼‰
        elif "choices" in data and data["choices"] and data["choices"][0]:
            choice = data["choices"][0]
            delta = choice.get("delta", {})
            content = delta.get("content", "")
            tool_calls = delta.get("tool_calls", [])
            
            parts = []
            
            # å¤„ç†æ–‡æœ¬å†…å®¹
            if content:
                parts.append({"text": content})
            
            # å¯¹äºå·¥å…·è°ƒç”¨chunksï¼Œæˆ‘ä»¬å·²ç»åœ¨ä¸Šé¢æ”¶é›†äº†ï¼Œè¿™é‡Œä¸éœ€è¦å†å¤„ç†
            # åªæœ‰å½“æœ‰æ–‡æœ¬å†…å®¹æ—¶æ‰å‘é€chunkç»™å®¢æˆ·ç«¯
            if tool_calls:
                self.logger.debug(f"Skipping tool call chunk (already collected): {tool_calls}")
            
            # åªæœ‰åœ¨æœ‰æ–‡æœ¬å†…å®¹æ—¶æ‰åˆ›å»ºchunk
            if parts:
                result_data = {
                    "candidates": [{
                        "content": {
                            "parts": parts,
                            "role": "model"
                        },
                        "index": 0
                    }]
                }
                
                return ConversionResult(success=True, data=result_data)
        
        # å¦‚æœæ²¡æœ‰å†…å®¹ä¹Ÿæ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œåˆ™è¿”å›ç©º candidatesï¼Œä¿æŒ SSE è¿æ¥
        result_data = {
            "candidates": [{
                "content": {
                    "parts": [],
                    "role": "model"
                },
                "index": 0
            }]
        }
        
        return ConversionResult(success=True, data=result_data)
    
    def _convert_from_anthropic_streaming_chunk(self, data: Dict[str, Any]) -> ConversionResult:
        """è½¬æ¢Anthropicæµå¼å“åº”chunkåˆ°Geminiæ ¼å¼"""
        import json
        import random
        
        # åˆå§‹åŒ–æµçŠ¶æ€å˜é‡
        if not hasattr(self, '_anthropic_to_gemini_state'):
            self._anthropic_to_gemini_state = {
                'current_text': '',
                'current_tool_calls': {},  # index -> tool_call_info
                'has_started': False
            }
        
        state = self._anthropic_to_gemini_state
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯message_deltaç±»å‹çš„ç»“æŸ
        if data.get("type") == "message_delta" and "delta" in data and "stop_reason" in data["delta"]:
            # è¿™æ˜¯æœ€åçš„chunkï¼ŒåªåŒ…å«å·¥å…·è°ƒç”¨å’Œç»“æŸä¿¡æ¯ï¼Œä¸é‡å¤å‘é€æ–‡æœ¬
            parts = []
            
            # åªæ·»åŠ å·¥å…·è°ƒç”¨ï¼ˆæ–‡æœ¬å†…å®¹å·²ç»åœ¨ä¹‹å‰çš„text_deltaä¸­å‘é€è¿‡äº†ï¼‰
            for tool_info in state['current_tool_calls'].values():
                tool_name = tool_info.get('name')
                tool_use_id = tool_info.get('id')
                if tool_name and tool_info.get('complete_args'):
                    try:
                        args_obj = json.loads(tool_info['complete_args'])
                        args_obj = self._adapt_tool_call_params(tool_name, args_obj)
                        function_call = {
                            "name": tool_name,
                            "args": args_obj
                        }
                        # ä¿ç•™ Anthropic tool_use çš„ ID
                        if tool_use_id:
                            function_call["id"] = tool_use_id

                        # æ„å»º partï¼ŒåŒ…å« functionCall
                        part: Dict[str, Any] = {"functionCall": function_call}

                        # å›å¡« thoughtSignatureï¼ˆå¦‚æœä¹‹å‰æ•è·è¿‡ï¼‰
                        if tool_use_id and tool_use_id in self._thought_signatures_by_tool_call_id:
                            thought_signature = self._thought_signatures_by_tool_call_id[tool_use_id]
                            part["thoughtSignature"] = thought_signature
                            print(f"ğŸ§  [THOUGHT_SIGNATURE] Restored (Anthropic streaming): tool_use_id={tool_use_id}, signature={thought_signature[:50]}...")

                        parts.append(part)
                    except json.JSONDecodeError:
                        function_call = {
                            "name": tool_name,
                            "args": {}
                        }
                        if tool_use_id:
                            function_call["id"] = tool_use_id

                        # æ„å»º partï¼ŒåŒ…å« functionCall
                        part_err: Dict[str, Any] = {"functionCall": function_call}

                        # å›å¡« thoughtSignatureï¼ˆå¦‚æœä¹‹å‰æ•è·è¿‡ï¼‰
                        if tool_use_id and tool_use_id in self._thought_signatures_by_tool_call_id:
                            thought_signature = self._thought_signatures_by_tool_call_id[tool_use_id]
                            part_err["thoughtSignature"] = thought_signature
                            print(f"ğŸ§  [THOUGHT_SIGNATURE] Restored (Anthropic streaming, decode error): tool_use_id={tool_use_id}, signature={thought_signature[:50]}...")

                        parts.append(part_err)
            
            # ç¡®å®šfinish reason
            stop_reason = data["delta"]["stop_reason"]
            if stop_reason == "tool_use":
                finish_reason = "STOP"  # Geminiä¸­å·¥å…·è°ƒç”¨ä¹Ÿç”¨STOP
            else:
                finish_reason = self._map_finish_reason(stop_reason, "anthropic", "gemini")
            
            result_data = {
                "candidates": [{
                    "content": {
                        "parts": parts,
                        "role": "model"
                    },
                    "finishReason": finish_reason,
                    "index": 0
                }]
            }
            
            # æ·»åŠ usageä¿¡æ¯ï¼ˆå¦‚æœæœ‰ä¸”ä¸ä¸ºNoneï¼‰
            if "usage" in data and data["usage"] is not None:
                usage = data["usage"]
                result_data["usageMetadata"] = {
                    "promptTokenCount": usage.get("input_tokens", 0),
                    "candidatesTokenCount": usage.get("output_tokens", 0),
                    "totalTokenCount": usage.get("input_tokens", 0) + usage.get("output_tokens", 0)
                }
            
            # æ¸…ç†çŠ¶æ€
            if hasattr(self, '_anthropic_to_gemini_state'):
                delattr(self, '_anthropic_to_gemini_state')
            
            return ConversionResult(success=True, data=result_data)
        
        # å¤„ç†content_block_start - å·¥å…·è°ƒç”¨å¼€å§‹
        elif data.get("type") == "content_block_start" and "content_block" in data:
            content_block = data["content_block"]
            index = data.get("index", 0)
            
            if content_block.get("type") == "tool_use":
                # è®°å½•å·¥å…·è°ƒç”¨ä¿¡æ¯
                state['current_tool_calls'][index] = {
                    'id': content_block.get("id", ""),
                    'name': content_block.get("name", ""),
                    'complete_args': ''
                }
            
            # å¯¹äºcontent_block_startï¼Œä¸è¿”å›ä»»ä½•å†…å®¹
            return ConversionResult(success=True, data={})
        
        # å¤„ç†content_block_delta - å¢é‡å†…å®¹
        elif data.get("type") == "content_block_delta" and "delta" in data:
            delta = data["delta"]
            index = data.get("index", 0)
            
            # æ–‡æœ¬å¢é‡
            if delta.get("type") == "text_delta":
                text_content = delta.get("text", "")
                if text_content:
                    state['current_text'] += text_content
                    
                    # å®æ—¶è¿”å›æ–‡æœ¬å¢é‡
                    result_data = {
                        "candidates": [{
                            "content": {
                                "parts": [{"text": text_content}],
                                "role": "model"
                            },
                            "index": 0
                        }]
                    }
                    return ConversionResult(success=True, data=result_data)
            
            # å·¥å…·è°ƒç”¨å‚æ•°å¢é‡
            elif delta.get("type") == "input_json_delta":
                partial_json = delta.get("partial_json", "")
                if index in state['current_tool_calls']:
                    state['current_tool_calls'][index]['complete_args'] += partial_json
                
                # å¯¹äºå·¥å…·å‚æ•°å¢é‡ï¼Œä¸è¿”å›å®æ—¶å†…å®¹
                return ConversionResult(success=True, data={})
        
        # å¤„ç†content_block_stop
        elif data.get("type") == "content_block_stop":
            # å¯¹äºcontent_block_stopï¼Œä¸è¿”å›ä»»ä½•å†…å®¹
            return ConversionResult(success=True, data={})
        
        # å¤„ç†message_start
        elif data.get("type") == "message_start":
            # å¼ºåˆ¶é‡ç½®çŠ¶æ€ï¼Œç¡®ä¿æ–°æµå¼€å§‹æ—¶çŠ¶æ€å¹²å‡€
            self._anthropic_to_gemini_state = {
                'current_text': '',
                'current_tool_calls': {},  # index -> tool_call_info
                'has_started': True
            }
            # å¯¹äºmessage_startï¼Œä¸è¿”å›ä»»ä½•å†…å®¹
            return ConversionResult(success=True, data={})
        
        # å¤„ç†message_stop
        elif data.get("type") == "message_stop":
            # æ¸…ç†çŠ¶æ€
            if hasattr(self, '_anthropic_to_gemini_state'):
                delattr(self, '_anthropic_to_gemini_state')
            return ConversionResult(success=True, data={})
        
        # å…¶ä»–ç±»å‹çš„äº‹ä»¶ï¼Œè¿”å›ç©ºç»“æ„
        return ConversionResult(success=True, data={})
    
    def _convert_from_gemini_streaming_chunk(self, data: Dict[str, Any]) -> ConversionResult:
        """è½¬æ¢Geminiæµå¼å“åº”chunkåˆ°ç›®æ ‡æ ¼å¼"""
        # Geminiæµå¼å“åº”é€šå¸¸åŒ…å«candidatesæ•°ç»„
        if "candidates" in data and data["candidates"]:
            candidate = data["candidates"][0]
            content = candidate.get("content", {})
            parts = content.get("parts", [])
            finish_reason = candidate.get("finishReason")
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«å·¥å…·è°ƒç”¨
            has_function_call = any("functionCall" in part for part in parts)
            has_text = any("text" in part for part in parts)
            
            result_data = {
                "candidates": [{
                    "content": {
                        "parts": parts,
                        "role": "model"
                    },
                    "index": 0
                }]
            }
            
            # æ·»åŠ finishReasonï¼ˆå¦‚æœæœ‰ï¼‰
            if finish_reason:
                result_data["candidates"][0]["finishReason"] = finish_reason
            
            # å¤„ç†usageä¿¡æ¯
            if "usageMetadata" in data:
                result_data["usageMetadata"] = data["usageMetadata"]
            
            return ConversionResult(success=True, data=result_data)
        
        # å¦‚æœæ²¡æœ‰candidatesï¼Œè¿”å›ç©ºçš„ç»“æ„
        result_data = {
            "candidates": [{
                "content": {
                    "parts": [],
                    "role": "model"
                },
                "index": 0
            }]
        }
        
        return ConversionResult(success=True, data=result_data)
    
    def _convert_content_from_gemini(self, parts: List[Dict[str, Any]]) -> Any:
        """è½¬æ¢Geminiå†…å®¹åˆ°é€šç”¨æ ¼å¼"""
        if len(parts) == 1 and "text" in parts[0]:
            return parts[0]["text"]
        
        # å¤„ç†å¤šæ¨¡æ€å†…å®¹å’Œå·¥å…·è°ƒç”¨
        converted_content = []
        has_tool_calls = False
        tool_calls = []
        text_content = ""
        
        for part in parts:
            if "text" in part:
                text_content += part["text"]
                converted_content.append({
                    "type": "text",
                    "text": part["text"]
                })
            elif "inlineData" in part:
                inline_data = part["inlineData"]
                mime_type = inline_data.get("mimeType", "image/jpeg")
                data_part = inline_data.get("data", "")

                if isinstance(mime_type, str) and mime_type.startswith("image/"):
                    # å›¾åƒå†…å®¹ â†’ OpenAI image_url
                    converted_content.append({
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:{mime_type};base64,{data_part}"
                        }
                    })
                elif isinstance(mime_type, str) and mime_type.startswith("audio/"):
                    # éŸ³é¢‘å†…å®¹ â†’ OpenAI input_audio
                    audio_format = self._get_audio_format_from_mime(mime_type)
                    converted_content.append({
                        "type": "input_audio",
                        "input_audio": {
                            "data": data_part,
                            "format": audio_format
                        }
                    })
                elif isinstance(mime_type, str) and mime_type.startswith("video/"):
                    # è§†é¢‘å†…å®¹ï¼šOpenAIä¸ç›´æ¥æ”¯æŒï¼Œè®°å½•è­¦å‘Š
                    self.logger.warning(f"Video content (mimeType={mime_type}) not supported for OpenAI, skipping")
                else:
                    # æœªçŸ¥ç±»å‹æˆ–éå­—ç¬¦ä¸²ï¼Œè®°å½•è­¦å‘Šé¿å…é™é»˜å¤±è´¥
                    self.logger.warning(f"Unsupported inlineData mimeType: {mime_type}, skipping")
            elif "fileData" in part:
                # P2-16: fileData å†…å®¹ï¼ˆGCS æ–‡ä»¶å¼•ç”¨ï¼‰â†’ æ–‡æœ¬å ä½ç¬¦
                file_data = part.get("fileData") or {}
                mime_type = file_data.get("mimeType", "")
                file_uri = file_data.get("fileUri", "")
                placeholder = f"[fileData: mimeType={mime_type or 'unknown'}, uri={file_uri or 'unknown'}]"
                text_content += placeholder
                converted_content.append({"type": "text", "text": placeholder})
                self.logger.warning(f"Gemini fileData converted to text placeholder (uri={file_uri})")
            elif "executableCode" in part or "executable_code" in part:
                # Gemini ä»£ç æ‰§è¡Œå·¥å…·è¿”å›çš„å¯æ‰§è¡Œä»£ç ç‰‡æ®µ
                code_info = part.get("executableCode") or part.get("executable_code") or {}
                language = code_info.get("language", "python")
                code = code_info.get("code", "")

                # è½¬æ¢ä¸º Markdown fenced code block
                fence_lang = language if isinstance(language, str) else "python"
                code_block = f"```{fence_lang}\n{code}\n```"
                text_content += code_block
                converted_content.append({"type": "text", "text": code_block})
                self.logger.warning(
                    "Gemini executableCode converted to fenced code block. "
                    "OpenAI/Anthropic will not execute this code automatically."
                )
            elif "codeExecutionResult" in part or "code_execution_result" in part:
                # Gemini ä»£ç æ‰§è¡Œç»“æœ
                result_info = part.get("codeExecutionResult") or part.get("code_execution_result") or {}
                outcome = result_info.get("outcome")
                output = result_info.get("output", "")

                lines = ["[code_execution_result]"]
                if outcome:
                    lines.append(f"outcome: {outcome}")
                if output:
                    lines.append(f"output:\n{output}")

                result_text = "\n".join(lines)
                text_content += result_text
                converted_content.append({"type": "text", "text": result_text})
                self.logger.warning(
                    "Gemini codeExecutionResult converted to plain text."
                )
            elif "functionCall" in part:
                # è½¬æ¢å‡½æ•°è°ƒç”¨
                fc = part["functionCall"]
                func_name = fc.get("name", "")
                func_args = fc.get("args", {})

                # ä¼˜å…ˆä½¿ç”¨ functionCall è‡ªå¸¦çš„ id å­—æ®µï¼ˆä¸ functionResponse ä¸­çš„ id ä¿æŒä¸€è‡´ï¼‰
                tool_call_id = fc.get("id")
                if not tool_call_id:
                    # å¤‡é€‰ï¼šä½¿ç”¨é¢„å…ˆå»ºç«‹çš„IDæ˜ å°„
                    if not hasattr(self, '_current_call_sequence'):
                        self._current_call_sequence = {}

                    sequence = self._current_call_sequence.get(func_name, 0) + 1
                    self._current_call_sequence[func_name] = sequence

                    tool_call_id = self._function_call_mapping.get(f"{func_name}_{sequence}")
                    if not tool_call_id:
                        # å¦‚æœæ˜ å°„ä¸­æ²¡æœ‰æ‰¾åˆ°ï¼Œç”Ÿæˆæ–°çš„ID
                        tool_call_id = f"call_{func_name}_{sequence:04d}"

                # æå–å¹¶ä¿å­˜ thoughtSignatureï¼ˆç”¨äºåç»­å›ä¼ ï¼‰
                thought_signature = part.get("thoughtSignature")
                if thought_signature:
                    self._thought_signatures_by_tool_call_id[tool_call_id] = thought_signature
                    print(f"ğŸ§  [THOUGHT_SIGNATURE] Captured: tool_call_id={tool_call_id}, signature={thought_signature[:50]}...")

                tool_calls.append({
                    "id": tool_call_id,
                    "type": "function",
                    "function": {
                        "name": func_name,
                        "arguments": json.dumps(func_args, ensure_ascii=False)
                    }
                })
                has_tool_calls = True
            elif "functionResponse" in part:
                # å‡½æ•°å“åº”åœ¨è¿™é‡Œæ ‡è®°ï¼Œä½†å®é™…å¤„ç†éœ€è¦åœ¨æ¶ˆæ¯çº§åˆ«
                converted_content.append({
                    "type": "function_response",
                    "function_response": part["functionResponse"]
                })
        
        # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œè¿”å›ç‰¹æ®Šæ ¼å¼æ ‡è¯†
        if has_tool_calls:
            return {
                "type": "tool_calls",
                "content": text_content if text_content else None,
                "tool_calls": tool_calls
            }
        
        # å¦‚æœåªæœ‰ä¸€ä¸ªæ–‡æœ¬ä¸”æ²¡æœ‰å…¶ä»–å†…å®¹ï¼Œç›´æ¥è¿”å›æ–‡æœ¬
        if len(converted_content) == 1 and converted_content[0].get("type") == "text":
            return converted_content[0]["text"]
        
        return converted_content if converted_content else ""
    
    def _convert_content_to_anthropic(self, parts: List[Dict[str, Any]]) -> Any:
        """è½¬æ¢Geminiå†…å®¹åˆ°Anthropicæ ¼å¼"""
        # å¤„ç†å¤šæ¨¡æ€å’Œå·¥å…·è°ƒç”¨å†…å®¹
        anthropic_content = []
        
        for part in parts:
            if "text" in part:
                text_content = part["text"]
                if text_content.strip():  # åªæ·»åŠ éç©ºæ–‡æœ¬
                    # æ£€æŸ¥æ˜¯å¦æ˜¯thinkingå†…å®¹
                    if part.get("thought", False):
                        # Gemini thinking â†’ Anthropic thinking
                        anthropic_content.append({
                            "type": "thinking",
                            "thinking": text_content
                        })
                    else:
                        # æ™®é€šæ–‡æœ¬å†…å®¹
                        anthropic_content.append({
                            "type": "text",
                            "text": text_content
                        })
            elif "inlineData" in part:
                inline_data = part["inlineData"]
                mime_type = inline_data.get("mimeType", "image/jpeg")
                data_part = inline_data.get("data", "")

                if isinstance(mime_type, str) and mime_type.startswith("image/"):
                    # å›¾åƒå†…å®¹ â†’ Anthropic image
                    anthropic_content.append({
                        "type": "image",
                        "source": {
                            "type": "base64",
                            "media_type": mime_type,
                            "data": data_part
                        }
                    })
                elif isinstance(mime_type, str) and mime_type.startswith("audio/"):
                    # éŸ³é¢‘å†…å®¹ï¼šAnthropicä¸ç›´æ¥æ”¯æŒï¼Œè®°å½•è­¦å‘Š
                    self.logger.warning(f"Audio content (mimeType={mime_type}) not supported for Anthropic, skipping")
                elif isinstance(mime_type, str) and mime_type.startswith("video/"):
                    # è§†é¢‘å†…å®¹ï¼šAnthropicä¸ç›´æ¥æ”¯æŒï¼Œè®°å½•è­¦å‘Š
                    self.logger.warning(f"Video content (mimeType={mime_type}) not supported for Anthropic, skipping")
                else:
                    # æœªçŸ¥ç±»å‹ï¼Œè®°å½•è­¦å‘Š
                    self.logger.warning(f"Unsupported inlineData mimeType: {mime_type}, skipping")
            elif "fileData" in part:
                # P2-16: fileData å†…å®¹ï¼ˆGCS æ–‡ä»¶å¼•ç”¨ï¼‰â†’ æ–‡æœ¬å ä½ç¬¦
                file_data = part.get("fileData") or {}
                mime_type = file_data.get("mimeType", "")
                file_uri = file_data.get("fileUri", "")
                placeholder = f"[fileData: mimeType={mime_type or 'unknown'}, uri={file_uri or 'unknown'}]"
                anthropic_content.append({"type": "text", "text": placeholder})
                self.logger.warning(f"Gemini fileData converted to text placeholder for Anthropic (uri={file_uri})")
            elif "executableCode" in part or "executable_code" in part:
                # Gemini ä»£ç æ‰§è¡Œå·¥å…·è¿”å›çš„å¯æ‰§è¡Œä»£ç ç‰‡æ®µ â†’ è½¬ä¸ºæ–‡æœ¬
                code_info = part.get("executableCode") or part.get("executable_code") or {}
                language = code_info.get("language", "python")
                code = code_info.get("code", "")
                fence_lang = language if isinstance(language, str) else "python"
                code_block = f"```{fence_lang}\n{code}\n```"
                anthropic_content.append({"type": "text", "text": code_block})
                self.logger.warning(
                    "Gemini executableCode converted to text for Anthropic."
                )
            elif "codeExecutionResult" in part or "code_execution_result" in part:
                # Gemini ä»£ç æ‰§è¡Œç»“æœ â†’ è½¬ä¸ºæ–‡æœ¬
                result_info = part.get("codeExecutionResult") or part.get("code_execution_result") or {}
                outcome = result_info.get("outcome")
                output = result_info.get("output", "")
                lines = ["[code_execution_result]"]
                if outcome:
                    lines.append(f"outcome: {outcome}")
                if output:
                    lines.append(f"output:\n{output}")
                result_text = "\n".join(lines)
                anthropic_content.append({"type": "text", "text": result_text})
                self.logger.warning(
                    "Gemini codeExecutionResult converted to text for Anthropic."
                )
            elif "functionCall" in part:
                # è½¬æ¢å·¥å…·è°ƒç”¨ (functionCall â†’ tool_use)
                func_call = part["functionCall"]
                func_name = func_call.get("name", "")
                
                # ä½¿ç”¨æ˜ å°„è¡¨è·å–ä¸€è‡´çš„ID
                tool_id = None
                if hasattr(self, '_function_call_mapping') and self._function_call_mapping:
                    # æŸ¥æ‰¾å¯¹åº”çš„ID
                    for key, mapped_id in self._function_call_mapping.items():
                        if key.startswith(func_name) and not key.startswith("response_"):
                            tool_id = mapped_id
                            break
                
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜ å°„IDï¼Œç”Ÿæˆä¸€ä¸ª
                if not tool_id:
                    tool_id = f"call_{func_name}_{hash(str(func_call.get('args', {}))) % 1000000000}"
                
                anthropic_content.append({
                    "type": "tool_use",
                    "id": tool_id,
                    "name": func_name,
                    "input": func_call.get("args", {})
                })
            elif "functionResponse" in part:
                # è½¬æ¢å·¥å…·å“åº” (functionResponse â†’ tool_result)
                func_response = part["functionResponse"]
                func_name = func_response.get("name", "")
                
                # ä½¿ç”¨æ˜ å°„è¡¨è·å–å¯¹åº”çš„tool_use ID
                tool_id = None
                if hasattr(self, '_function_call_mapping') and self._function_call_mapping:
                    for key, mapped_id in self._function_call_mapping.items():
                        if key.startswith(f"response_{func_name}"):
                            tool_id = mapped_id
                            break
                
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜ å°„IDï¼Œå°è¯•ä»functionResponseä¸­è·å–æˆ–ç”Ÿæˆ
                if not tool_id:
                    # æ£€æŸ¥functionResponseæ˜¯å¦åŒ…å«åŸå§‹çš„tool_use_id
                    response_data = func_response.get("response", {})
                    if isinstance(response_data, dict) and "_tool_use_id" in response_data:
                        tool_id = response_data["_tool_use_id"]
                    else:
                        # ç”Ÿæˆä¸€ä¸ªåŸºäºå‡½æ•°åçš„ä¸€è‡´æ€§ID
                        # ä½¿ç”¨å‡½æ•°åä½œä¸ºç§å­æ¥ç”Ÿæˆä¸€è‡´çš„hash
                        import hashlib
                        seed = f"{func_name}_seed"
                        hash_value = int(hashlib.md5(seed.encode()).hexdigest()[:8], 16) % 1000000000
                        tool_id = f"call_{func_name}_{hash_value}"
                
                # æå–å®é™…çš„å·¥å…·ç»“æœå†…å®¹
                result_content = func_response.get("response", {})
                if isinstance(result_content, dict):
                    # å¦‚æœåŒ…å«_tool_use_idï¼Œç§»é™¤å®ƒ
                    result_content = {k: v for k, v in result_content.items() if k != "_tool_use_id"}
                    # æå–å®é™…å†…å®¹
                    actual_content = result_content.get("content", result_content)
                else:
                    actual_content = result_content
                    
                anthropic_content.append({
                    "type": "tool_result",
                    "tool_use_id": tool_id,
                    "content": str(actual_content)
                })
        
        # å¦‚æœæ²¡æœ‰ä»»ä½•æœ‰æ•ˆå†…å®¹ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²ï¼ˆä¼šåœ¨è°ƒç”¨å¤„å¤„ç†ï¼‰
        if not anthropic_content:
            return ""
        
        # å¦‚æœåªæœ‰ä¸€ä¸ªæ–‡æœ¬å†…å®¹ï¼Œç›´æ¥è¿”å›å­—ç¬¦ä¸²
        if len(anthropic_content) == 1 and anthropic_content[0].get("type") == "text":
            return anthropic_content[0]["text"]
        
        # è¿”å›å†…å®¹å—æ•°ç»„
        return anthropic_content

    def _get_audio_format_from_mime(self, mime_type: str) -> str:
        """ä»éŸ³é¢‘MIMEç±»å‹ä¸­æå–æ ¼å¼å­—ç¬¦ä¸²

        Args:
            mime_type: éŸ³é¢‘MIMEç±»å‹ï¼Œå¦‚ 'audio/wav', 'audio/mpeg'

        Returns:
            OpenAIæ”¯æŒçš„éŸ³é¢‘æ ¼å¼å­—ç¬¦ä¸²ï¼Œå¦‚ 'wav', 'mp3'
        """
        if not mime_type or "/" not in mime_type:
            return "wav"

        subtype = mime_type.split("/")[1].lower()

        # å»æ‰å¸¸è§å‰ç¼€ï¼ˆå¦‚ x-wavï¼‰
        if subtype.startswith("x-"):
            subtype = subtype[2:]

        # MIMEå­ç±»å‹åˆ°OpenAIæ ¼å¼çš„æ˜ å°„
        format_mapping = {
            "mpeg": "mp3",
            "mp3": "mp3",
            "wav": "wav",
            "wave": "wav",
            "webm": "webm",
            "ogg": "ogg",
            "flac": "flac",
        }

        return format_mapping.get(subtype, subtype or "wav")

    def _merge_consecutive_messages(self, messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """åˆå¹¶è¿ç»­çš„åŒè§’è‰²æ¶ˆæ¯

        Gemini API ä¸å…è®¸è¿ç»­çš„åŒè§’è‰²æ¶ˆæ¯ï¼ˆå¦‚è¿ç»­ä¸¤æ¡ user æ¶ˆæ¯ï¼‰ï¼Œä¼šè¿”å› 400 é”™è¯¯ã€‚
        æ­¤æ–¹æ³•å°†è¿ç»­çš„åŒè§’è‰²æ–‡æœ¬æ¶ˆæ¯åˆå¹¶ä¸ºä¸€æ¡ï¼Œä»¥é¿å…è¯¥é™åˆ¶ã€‚

        æ³¨æ„ï¼šåªåˆå¹¶çº¯æ–‡æœ¬æ¶ˆæ¯ï¼Œä¸åˆå¹¶åŒ…å« tool_calls æˆ– tool_call_id çš„æ¶ˆæ¯ã€‚
        """
        if not messages or len(messages) < 2:
            return messages

        merged = []
        i = 0
        while i < len(messages):
            current = messages[i]
            role = current.get("role")
            content = current.get("content")

            # åªå¯¹çº¯æ–‡æœ¬æ¶ˆæ¯è¿›è¡Œåˆå¹¶ï¼ˆä¸å« tool_calls / tool_call_idï¼‰
            if (role in ("user", "assistant")
                and "tool_calls" not in current
                and "tool_call_id" not in current
                and isinstance(content, str)):

                # æ”¶é›†æ‰€æœ‰è¿ç»­çš„åŒè§’è‰²çº¯æ–‡æœ¬æ¶ˆæ¯
                texts = [content]
                j = i + 1
                while j < len(messages):
                    next_msg = messages[j]
                    next_role = next_msg.get("role")
                    next_content = next_msg.get("content")
                    if (next_role == role
                        and "tool_calls" not in next_msg
                        and "tool_call_id" not in next_msg
                        and isinstance(next_content, str)):
                        texts.append(next_content)
                        j += 1
                    else:
                        break

                if len(texts) > 1:
                    # åˆå¹¶ä¸ºå•æ¡æ¶ˆæ¯
                    merged.append({
                        "role": role,
                        "content": "\n\n".join(texts)
                    })
                    self.logger.debug(f"Merged {len(texts)} consecutive {role} messages")
                else:
                    merged.append(current)
                i = j
            else:
                merged.append(current)
                i += 1

        return merged

    def _sanitize_schema_for_openai(self, schema: Dict[str, Any]) -> Dict[str, Any]:
        """å°†Geminiæ ¼å¼çš„JSON Schemaè½¬æ¢ä¸ºOpenAIå…¼å®¹çš„æ ¼å¼"""
        if not isinstance(schema, dict) or not schema:
            # OpenAI è¦æ±‚ parameters å¿…é¡»æ˜¯æœ‰æ•ˆçš„ JSON Schemaï¼Œè‡³å°‘éœ€è¦ type å­—æ®µ
            return {"type": "object", "properties": {}}

        # å¤åˆ¶schemaé¿å…ä¿®æ”¹åŸå§‹æ•°æ®
        sanitized = copy.deepcopy(schema)

        # ç¡®ä¿æœ‰ type å­—æ®µ
        if "type" not in sanitized:
            sanitized["type"] = "object"
        if sanitized.get("type") == "object" and "properties" not in sanitized:
            sanitized["properties"] = {}
        
        # Geminiåˆ°OpenAIçš„ç±»å‹æ˜ å°„
        type_mapping = {
            "STRING": "string",
            "NUMBER": "number", 
            "INTEGER": "integer",
            "BOOLEAN": "boolean",
            "ARRAY": "array",
            "OBJECT": "object"
        }
        
        def convert_types(obj):
            """é€’å½’è½¬æ¢schemaä¸­çš„ç±»å‹"""
            if isinstance(obj, dict):
                # è½¬æ¢typeå­—æ®µ
                if "type" in obj and isinstance(obj["type"], str):
                    obj["type"] = type_mapping.get(obj["type"].upper(), obj["type"].lower())
                
                # è½¬æ¢éœ€è¦æ•´æ•°å€¼çš„å­—æ®µï¼ˆå°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ•´æ•°ï¼‰
                integer_fields = ["minItems", "maxItems", "minimum", "maximum", "minLength", "maxLength"]
                for field in integer_fields:
                    if field in obj and isinstance(obj[field], str) and obj[field].isdigit():
                        obj[field] = int(obj[field])
                
                # é€’å½’å¤„ç†æ‰€æœ‰å­—æ®µï¼ˆè·³è¿‡å·²ç»å¤„ç†è¿‡çš„æ ‡é‡å­—æ®µï¼‰
                for key, value in obj.items():
                    if key not in ["type"] + integer_fields:  # é¿å…é‡å¤å¤„ç†å·²è½¬æ¢çš„å­—æ®µ
                        obj[key] = convert_types(value)
                    
            elif isinstance(obj, list):
                # å¤„ç†æ•°ç»„ä¸­çš„æ¯ä¸ªå…ƒç´ 
                return [convert_types(item) for item in obj]
            
            return obj
        
        return convert_types(sanitized)
    
    def _build_function_call_mapping(self, contents: List[Dict[str, Any]]) -> Dict[str, str]:
        """æ‰«ææ•´ä¸ªå¯¹è¯å†å²ï¼Œä¸ºfunctionCallå’ŒfunctionResponseå»ºç«‹IDæ˜ å°„"""
        mapping = {}
        function_call_sequence = {}  # {func_name: sequence_number}

        for content in contents:
            parts = content.get("parts", [])
            for part in parts:
                if "functionCall" in part:
                    fc = part["functionCall"]
                    func_name = fc.get("name", "")
                    if func_name:
                        # ä¸ºæ¯ä¸ªå‡½æ•°è°ƒç”¨ç”Ÿæˆå”¯ä¸€çš„sequence number
                        sequence = function_call_sequence.get(func_name, 0) + 1
                        function_call_sequence[func_name] = sequence

                        # ä¼˜å…ˆä½¿ç”¨ä¸Šæ¸¸æä¾›çš„ idï¼Œä¿è¯ä¸ functionResponse ä¸€è‡´
                        tool_call_id = fc.get("id") or f"call_{func_name}_{sequence:04d}"
                        mapping[f"{func_name}_{sequence}"] = tool_call_id
                        
                elif "functionResponse" in part:
                    func_name = part["functionResponse"].get("name", "")
                    if func_name:
                        # ä¸ºfunctionResponseåˆ†é…æœ€è¿‘çš„functionCallçš„ID
                        current_sequence = function_call_sequence.get(func_name, 0)
                        if current_sequence > 0:
                            mapping[f"response_{func_name}_{current_sequence}"] = mapping.get(f"{func_name}_{current_sequence}")
        
        return mapping
    
    def _convert_schema_for_anthropic(self, schema: Dict[str, Any]) -> Dict[str, Any]:
        """å°†Geminiæ ¼å¼çš„JSON Schemaè½¬æ¢ä¸ºAnthropicå…¼å®¹çš„æ ¼å¼"""
        if not isinstance(schema, dict):
            return schema
        
        # å¤åˆ¶schemaé¿å…ä¿®æ”¹åŸå§‹æ•°æ®
        converted = copy.deepcopy(schema)
        
        # Geminiåˆ°Anthropicçš„ç±»å‹æ˜ å°„
        type_mapping = {
            "STRING": "string",
            "NUMBER": "number", 
            "INTEGER": "integer",
            "BOOLEAN": "boolean",
            "ARRAY": "array",
            "OBJECT": "object"
        }
        
        def convert_types(obj):
            """é€’å½’è½¬æ¢schemaä¸­çš„ç±»å‹"""
            if isinstance(obj, dict):
                # è½¬æ¢typeå­—æ®µ
                if "type" in obj and isinstance(obj["type"], str):
                    obj["type"] = type_mapping.get(obj["type"].upper(), obj["type"].lower())
                
                # è½¬æ¢éœ€è¦æ•´æ•°å€¼çš„å­—æ®µï¼ˆå°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ•´æ•°ï¼‰
                integer_fields = ["minItems", "maxItems", "minimum", "maximum", "minLength", "maxLength"]
                for field in integer_fields:
                    if field in obj and isinstance(obj[field], str) and obj[field].isdigit():
                        obj[field] = int(obj[field])
                
                # é€’å½’å¤„ç†æ‰€æœ‰å­—æ®µï¼ˆè·³è¿‡å·²ç»å¤„ç†è¿‡çš„æ ‡é‡å­—æ®µï¼‰
                for key, value in obj.items():
                    if key not in ["type"] + integer_fields:  # é¿å…é‡å¤å¤„ç†å·²è½¬æ¢çš„å­—æ®µ
                        obj[key] = convert_types(value)
                    
            elif isinstance(obj, list):
                # å¤„ç†æ•°ç»„ä¸­çš„æ¯ä¸ªå…ƒç´ 
                return [convert_types(item) for item in obj]
            
            return obj
        
        return convert_types(converted)

    def _adapt_tool_call_params(self, tool_name: str, args: Any) -> Any:
        """é€‚é…å·¥å…·è°ƒç”¨å‚æ•°åˆ°é¢„æœŸçš„schema

        å¤„ç†å·²çŸ¥çš„ä¸å…¼å®¹æƒ…å†µï¼Œä¾‹å¦‚ï¼š
        - ReadFile: paths/path -> file_path
        """
        if not isinstance(args, dict):
            return args

        adapted = dict(args)

        # gemini-cli å†…ç½® read_file/ReadFile å·¥å…·ï¼špaths/path -> file_path
        if tool_name.lower() in ("readfile", "read_file") and "file_path" not in adapted:
            # ä¼˜å…ˆå¤„ç† pathsï¼ˆå¤æ•°ï¼‰
            paths = adapted.get("paths")
            if isinstance(paths, list) and paths:
                adapted["file_path"] = paths[0]
                del adapted["paths"]
                if len(paths) > 1:
                    self.logger.warning(f"ReadFile: only first path used, {len(paths)-1} paths ignored")
            elif isinstance(paths, str) and paths:
                adapted["file_path"] = paths
                del adapted["paths"]
            # å…¼å®¹ Gemini æ¨¡å‹ä½¿ç”¨çš„å•æ•° path å­—æ®µ
            elif "path" in adapted:
                single_path = adapted["path"]
                if isinstance(single_path, str) and single_path:
                    adapted["file_path"] = single_path
                    del adapted["path"]
                elif isinstance(single_path, list) and single_path:
                    adapted["file_path"] = single_path[0]
                    del adapted["path"]
                    if len(single_path) > 1:
                        self.logger.warning(f"ReadFile: only first path used from 'path', {len(single_path)-1} ignored")

        return adapted

    def _map_finish_reason(self, reason: str, source_format: str, target_format: str) -> str:
        """æ˜ å°„ç»“æŸåŸå› """
        reason_mappings = {
            "openai": {
                "gemini": {
                    "stop": "STOP",
                    "length": "MAX_TOKENS",
                    "content_filter": "SAFETY",
                    "tool_calls": "MODEL_REQUESTED_TOOL"
                }
            },
            "anthropic": {
                "gemini": {
                    "end_turn": "STOP",
                    "max_tokens": "MAX_TOKENS",
                    "stop_sequence": "STOP",
                    "tool_use": "STOP"
                }
            }
        }
        
        try:
            return reason_mappings[source_format][target_format].get(reason, "STOP")
        except KeyError:
            return "STOP"